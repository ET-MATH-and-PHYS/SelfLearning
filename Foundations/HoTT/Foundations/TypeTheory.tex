%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Type Theory}
\label{types} % Always give a unique label
% use \chaptermark{}
% to alter or adjust the chapter heading in the running head

\section{Type Theory versus Set Theory}

Homotopy type theory, among other things, is a foundational language for mathematics. We compare and contrast with the set-theoretic Zermelo-Fraenkel construction. The set-theoretic foundation has two ``layers": the deductive system of first-order logic, and, formulated inside this system, the axioms of a particular theory, such as ZFC. Thus, set theory is an interplay between sets (the objects of the second layer) and propositions (the objects of the first layer).

By constrast, type theory is its own deductive system: it need not be formulated inside any superstructure, such as first-order logic. Type theory has one basic notion: \textbf{types}. Propositions (i.e. statements which we can prove, disprove, assume, negate, and so on) are identified with particular types. The mathematical activity of proving atheorem is thus identified with a special case of the mathematical activity of constructing an object.

This leads to another difference. First, informally a deductive system is a collection of \textbf{rules} for deriving things called \textbf{judgments}. Thinking of a deductive system as a formal game, the judgments are ``position" in the game which we read by following the game rules. We can also think of a deductive system as a kind of algebraic theory, in which case the judgments are the elements and the deductive rules are the operations. From a logical point of view, the judgements can be considered to be the external statements, living in the metatheory, as opposed to the internal statements of the theory itself. 

For example, in first-order logic there is only one kind of judgment: that a given proposition has a proof. A rule of first-order logic is a rule of proof construction, which derives judgments from other judgments. The basic judgment of type theory, which in certain cases can be interpreted as ``$A$ has a proof", is written ``$a : A"$ and pronounced as ``the term $a$ has type $A$"m or in HoTT ``$a$ is a point of $A$". When $A$ is a type representing a proposition, then $a$ may be called a \textbf{witness} to the provability of $A$, or \textbf{evidence} of the truth of $A$. In this case the judgment $a : A$ is derivable in type theory for som $a$ precisely when the analogous judgment ``$A$ has a proof" is derivable in first order logic.

Note, internally in type theory we cannot ``prove" statements about judgments, nor can we ``disprove" a judgment. Additionally, we cannot talk about an element ``$a$" in isolation: every element by its very nature is a term of some type, and that type is uniquely determined, generally speaking. That is, ``$a:A$" is an \textbf{atomic statement} in type theory.

A last major difference between set and type theory is the treatment of equality. The classical notion of equality is a proposition (we can disprove or assume an equality). Since in type theory propositions are types, this means that equality is a type: for elements $a,b:A$, we have a type ``$a=_Ab$". When $a=_Ab$ is inhabited (i.e. has terms), we say that $a$ and $b$ are \textbf{(propositionally) equal}.

However, we also need in type theory an equality judgment, existing at the same level as the judgment ``$x:A$". This is called \textbf{judgmental equality} or \textbf{definitional equality}, and we write it as $a \equiv b:A$ or simply $a\equiv b:A$. This can be thought of as meaning ``equal by definition". Whether or not two expressions are equal by definition is a matter of expanding out the definitions; in particular, it is algorithmically decidable (though the algorithm is meta-theoretic, not internal to the theory).

If we interpret a deductive system as an algebraic theory, judgmental equality is equality in that theory. A judgmental notion of equality allows us to control the other form of judgment, ``$a:A$". Then we should have a rule saying that given the judgments $a:A$ and $A\equiv B$, we may derive the judgment $a:B$. For us type theory will be a deductive system based on two forms of judgment: \begin{itemize}
    \item $a:A$ - ``$a$ is an object of type $A$"
    \item $a\equiv b:A$ - ``$a$ and $b$ are definitially equal objects of type $A$"
\end{itemize}
As we shall see later, for types $A$ and $B$ we use ``$A\rightarrow B$" as notation for the type of functions from $A$ to $B$. Then $f:A\rightarrow B$ can be interpreted as a typing judgment.

Judgments may depend on assumptions of the form $x:A$, where $x$ is a variable and $A$ is a type. For example, assuming $A$ is a type, $x,y :A$, and $p:x=_Ay$, we may construct an element $p^{-1}:y=_Ax$. The collection of all such assumptions is called the \textbf{context}; from a topological point of view this is analogous to a parameter space. Technically the context must be an ordered list of assumptions since we can have later assumptions depend on previous ones.

If the type $A$ in an assumption $x:A$ represents a proposition, then the assumption is a type theoretic version of a hypothesis. Under this meaning of the word assumption, we can assume a propositional equality, by assuming a variable $p:x=y$, but we cannot assume a judgmental equality $x\equiv y:A$, since it is not a type that can have an element. However, if we have a type or an element which involves a variable $x:A$, then we can substitute any particular element $a:A$ for $x$ to obtain a more specific type or element. By the same token we cannot \textbf{prove} a judgmental equality either, since it is not a type in which we can exhibit a witness. Nonetheless, we will sometimes state judgmental equalities as part of a theorem, for example ``there exists $f:A\rightarrow B$ such that $f(x) \equiv y$".

\section{Function Types}

Given types $A$ and $B$, we can construct the type $A\rightarrow B$ of \textbf{functions} with domain $A$ and codomain $B$. Unlike in set theory, functions in type theory are a primitive concept. We explain the function type by describing what we can do with functions, how to construct them, and what equalities they induce.

Given a function $f:A\rightarrow B$ and an element of the domain $a: A$, we can \textbf{apply} the function to obtain an element of the codomain $B$, denoted $f(a):B$, and called the \textbf{value} of $f$ at $a$. But how can we construct elements of type $A\rightarrow B$? There are two equivalent ways: either by direct definition or by using $\lambda$-abstraction. Introducing a function by definition means that we introduce a function by giving it a name, let's say $f$, and saying we define $f:A\rightarrow B$ by giving an equation $$f(x) :\equiv \Phi$$ where $x$ is a variable and $\Phi$ is an expression which may use $x$. In order for this to be valid, we have to check that $\Phi:B$ assuming $x:A$.

We can compute $f(a)$ by replacing the variable $x$ in $\Phi$ with $a$. For example, consider $f:\N\rightarrow \N$, which is defined by $f(x):\equiv x+x$. Then $f(2)$ is judgmentally equal to $2+2$.

If we don't want to introduce a name for the function, we can use $\lambda$-\textbf{abstraction}. Given an expression $\Phi$ of type $B$ which may use $x:A$, as above, we write $\lambda(x:A).\Phi$ to indicate the same function defined previously as $f$. Thus we have $$(\lambda(x:A).\Phi):A\rightarrow B$$
For example we have the typing judgment \begin{equation*}
    (\lambda(x:\N).x+x):\N\rightarrow \N
\end{equation*}
As another example, for any types $A$ and $B$ and any $y:B$, we have a constant function $(\lambda(x:A).y):A\rightarrow B$. The type of the variable $x$ in the $\lambda$-abstraction is often ommitted, and infered from context. The scope of the variable binding ``$\lambda x.$" is the entire rest of the expression, unless delimited with parentheses. Another equivalent notation is $$(x\mapsto \Phi):A\rightarrow B$$
We may also write something like $g(x,-)$ for $\lambda y.g(x,y)$. For $\lambda$-abstraction we have the following \textbf{computation rule}, which is definitional equality: $$(\lambda x.\Phi)(a) \equiv \Phi'$$ 
where $\Phi'$ is the expression $\Phi$ in which all occurrences of $x$ have been replaced by $a$. For any function $f:A\rightarrow B$ we can also construct a lambda abstraction function $\lambda x.f(x)$. Since this is by definition the function that applies $f$ to its argument, we consider it to be definitionally equal to $f$: $$f\equiv (\lambda x.f(x))$$
This equality is the \textbf{uniqueness principle for function types}. We can then read a definition of $f:A\rightarrow B$ by $f(x) :\equiv \Phi$ as $f :\equiv \lambda x.\Phi$. 

As a matter of analogy, a $\lambda$-abstraction binds a dummy variable in exactly the same way that an integral does. Now, to define a function in several variables we could use the cartesian product, to be introduced later. A function with aprameters $A$ and $B$ and results in $C$ would be given the type $f:A\times B\rightarrow C$. However, we have another choice that avoids product types which is called \textbf{currying}. Currying involves representing a function of two inputs $a:A$ and $b:B$ as a function which takes one input, $a:A$, and returns another function which then takes a second input $b:B$ and returns the result. That is, we consider two-variable functions to belong to an iterated function type, $f:A\rightarrow (B\rightarrow C)$.

We can define a named function $f:A\rightarrow (B\rightarrow C)$ by giving an equation $$f(x)(y):\equiv \Phi$$ 
where $\Phi:C$ assuming $x:A$ and $y:B$. Using $\lambda$-abstraction this corresponds to $$f:\equiv \lambda x.\lambda y.\Phi$$
which may also be written as \begin{equation*}
    f : \equiv x\mapsot (y\mapsto \Phi)
\end{equation*}


\section{Universes and Families}

So far we have been using the expression ``$A$ is a type" informally. We now make this precise with \textbf{universes}.

\begin{definition}
    A \textbf{universe} is a type whose elements are types. To avoid Russell's paradox we introduce a hierarchy of universes, $$U_0:U_1:U_2:\cdots$$
    where every universe $U_i$ is an element of the next universe $U_{i+1}$. Moreover, we assume that our universes are \textbf{cumulative}, so all elements of the $i$th universe are also elements of the $(i+1)$th universe. This implies that elements no longer have unique types.
\end{definition}
When we say $A$ is a type, we mean it inhabits some universe $U_i$. We often write the universes just as $U$, with $U:U$ meaning $U_i:U_{i+1}$, creating a kind of \textbf{typical ambiguity}.

To model a collection of types varying over a given type $A$, we use function $B:A\rightarrow U$ whose codomain is a universe. These functionar are called \textbf{families of types} (or sometimes \textbf{dependent types}). An example of a type family is the family of finite sets $\text{Fin}:\N\rightarrow U$.

A simpler example of a type family is the \textbf{constant} type family at a type $B:U$, which is of course the constant function $(\lambda(x:A).B):A\rightarrow U$.

\section{Dependent Function Types}

In type theory we often use a more general version of function types, called $\prod$-\textbf{type} or \textbf{dependent function type}. The elements of a $\prod$-type are functions whose codomain type can vary depending on the element of the domain to which the function is applied, called \textbf{dependent functions}. The name ``$\prod$-type" is used because this type can also be regarded as the cartesian product over a given type.

\begin{definition}
    Given a type $A:U$, and a family $B:A\rightarrow U$, we may construct the type of dependent functions $\prod_{(x:A)}B(x):U$.If $B$ is a constant family, then the dependent product type is the ordinary function type: $$\prod_{(x:A)}B\equiv (A\rightarrow B)$$
\end{definition}

To define $f:\prod_{(x:A)}B(x)$, where $f$ is the name of a dependent function to be defined, we need an expression $\Phi:B(x)$ possibly involving the variable $x:A$, and we write $$f(x) :\equiv \Phi,\;\;\text{for }x:A$$
Alternatively, we can use $\lambda$-abstraction $$\lambda x.\Phi:\prod_{x:A}B(x)$$
As with non-dependent functions, we can \textbf{apply} a dependent function $f:\prod_{(x:A)}B(x)$ to an argument $a :A$ to obtain an element $f(a):B(a)$. The equalities are the same as the ordinary function type in the sense that for $a:A$, we have $f(a) \equiv \Phi'$ and $(\lambda x.\Phi)(a) \equiv \Phi'$, where $\Phi'$ is obtained by replacing all occurrences of $x$ in $\Phi$ by $a$. Similarly we have the uniqueness principle $f \equiv (\lambda x.f(x))$.

An important class of dependent functions types are functions which are \textbf{polymorphic} over a given universe.

\begin{definition}
    A function is polymorphic if it takes a type as one of its arguments, and then acts on elements of that type (or of other types constructed from it).
\end{definition}
An example is the polymorphic identity function $\id:\prod_{(A:U)}A\rightarrow A$, which we define by $\id :\lambda(A:U).\lambda(x:A).x$.

We sometimes will write arguments of a dependent function type as subscripts, so $\id(A,x) = \id_A(x) :\equiv x$.

\begin{example}
    A non-trivial example is the ``swap" operation which switches the order of arguments of a curried two-argument function: $$\text{swap}:\prod_{(A:U)}\prod_{(B:U)}\prod_{(C:U)}(A\rightarrow B\rightarrow C)\rightarrow (B\rightarrow A\rightarrow C)$$
    We define this by \begin{equation*}
        \text{swap}(A,B,C,g) :\equiv \lambda b.\lambda a.g(a)(b)
    \end{equation*}
    We can also write $$\text{swap}_{A,B,C}(g)(b,a) :\equiv g(a,b)$$
\end{example}

Note as with ordinary functions we use currying to define dependent functions of several arguments. However, the second domain in the dependent case may depend on the first one, and the codomain may depend on both.

\section{Product Types}

Given types $A,B:U$ we introduce the type $A\times B:U$, which we call their \textbf{cartesian product}. We also introduce a nullary product type, called the \textbf{unit type} $\mathbf{1}:U$. A term of $A\times B$ is a pair $(a,b) : A\times B$, where $a:A$ and $b:B$, and the only term of $\mathbf{1}$ is some particular object $\star:\mathbf{1}$. As with functions, ordered pairs are a primitive concept in Type theory.

\begin{remark}
    We have a general pattern for introducing a new kind of type in Type theory. To specify a type, we specify: \begin{itemize}
        \item[(i)] how to form new types of this kind, via \textbf{formation rules}. (Example: we can form the dependent function type $\prod_{(x:A)}B(x)$ when $A$ is a type and $B(x)$ is a type for $x \in A$).
        \item[(ii)] how to construct elements of that type. These are called the type's \textbf{constructors} or \texstbf{introduction rules}. (Example: A function type has one constructor, $\lambda$-abstraction)
        \item[(iii)] how to use elements of that type. These are called the type's \textbf{eliminators} or \textbf{elimination rules}. (Example: Function application for function types)
        \item[(iv)] a \textbf{computation rule} (i.e. $\beta$-\textbf{reduction}), which expresses how an eliminator acts on a constructor. (Example: For functions, the computation rule states that $(\lambda x.\Phi)(a)$ is judmentally equal to the substitution of $a$ for $x$ in $\Phi$.)
        \item[(v)] an optional \textbf{uniqueness principle} (i.e. $\eta$-\textbf{expansion}), which expresses uniqueness of maps into or out of that type. For some types, the uniqueness principle characterizes maps into the type, by stating that every element of the type is uniquely determined by the results of applying eliminators to it, and can be reconstructed from these results by applying a constructor. (Example: For functions, the uniqueness principle says that any function $f$ is judgmentally equal to the expanded function $\lambda x.f(x)$, and thus is uniquely determined by its values) For other types the uniqueness principle says that every map from that type is uniquely determined by some data.
    \end{itemize}

    When the uniqueness principle is not taken as a rule of judgmental equality, iti s often nevertheless provable as a propositional equality from the other rules for the type. In this case we call it a \textbf{propositional uniqueness principle}.
\end{remark}

We will assert the uniqueness principle for products, ``every element of $A\times B$ is a pair," as a propositional equality later. Now, how can we use pairs? Consider the definition of a non-dependent function $f:A\times B\rightarrow C$. Since we intend the only elements of $A \times B$ to be pairs, we expect to be able to define a function by prescribing the result when $f$ is applied to a pair $(a,b)$. We can prescribe these results by using currying, $g:A\rightarrow (B\rightarrow C)$. Thus we introduce the elimination rule for products, which says that for any such $g$, we can define a function $f:A\times B\rightarrow C$ by $$f((a,b)) :\equiv g(a)(b)$$

Unlike in set theory, type theory assumes that a function on $A\tiems B$ is well-defined as soon as we specify its values on pairs, and from this we will be able to prove that every element of $A\times B$ is a pair. From a category-theoretic perspective we can say that we define the product $A\times B$ to be the left adjoint to the ``exponential" $B\rightarrow C$.

As an example, we can derive the \textbf{projection} functions \begin{align*}
    &\text{pr}_1:A\times B\rightarrow A \\
    &\text{pr}_2:A\times B\rightarrow B
\end{align*}
with the defining equations \begin{align*}
    &\text{pr}_1((a,b)) :\equiv a \\
    &\text{pr}_2((a,b)) :\equiv b 
\end{align*}
Rather than invoking the principle of function definition, every time we want to define a function, an alternative approach is to invoke it once in a universal case, and then apply the resulting function in all other cases. That is we may define a function of type \begin{equation}
    \text{rec}_{A\times B}:\prod_{C:U}(A\rightarrow B\rightarrow C)\rightarrow A\times B\rightarrow C
\end{equation}
with the defining equation $$\text{rec}_{A\times B}(C,g,(a,b)) :\equiv g(a)(b)$$
Then, we can define these functions by \begin{align*}
    &\text{pr}_1 :\equiv \text{rec}_{A\times B}(A,\lambda a.\lambda b.a) \\
    &\text{pr}_2 :\equiv \text{rec}_{A\times B}(B,\lambda a.\lambda b.b)
\end{align*}
We refer to the function $\text{rec}_{A\times B}$ as the \textbf{recursor} for the product types. We also speak of the \textbf{recursion principle} for cartesian products, meaning the fact that we can define a function $f:A\times B\rightarrow C$ as above by its value on pairs.

We also have a recursor for the unit type \begin{equation*}
    \text{rec}_{\mathbf{1}}:\prod_{C:U}C\rightarrow \mathbf{1}\rightarrow C
\end{equation*}
with the defining equation \begin{equation*}
    \text{rec}_{\mathbf{1}}(C,c,\star) :\equiv c
\end{equation*}
To define dependent functions over the product type, we have to generalize the recursor. Given $C:A\times B\rightarrow U$, we may define a function $f:\prod_{(x:A\times B)}C(x)$ by providing a function $g:\prod_{(x:A)}\prod_{(y:B)}C((x,y))$ with defining equation $$f((x,y)) :\equiv g(x)(y)$$
Then we can prove the propositional uniqueness principle. Specifically, we can construct a function \begin{equation*}
    \text{uniq}_{A\times B}:\prod_{x:A\times B}((\text{pr}_1(x),\text{pr}_2(x)) =_{A\times B}x)
\end{equation*}
We will see later that the identity type has a reflexivity element $\text{refl}_x:x=_Ax$ for any $x:A$. Given this, we can define $$\text{uniq}_{A\times B}((a,b)) :\equiv \text{refl}_{(a,b)}$$
This construction works, because in the case that $x:\equiv (a,b)$, we can calculate $$(\text{pr}_1((a,b)),\text{pr}_2((a,b))) \equiv (a,b)$$
using the defining equations for the projections. Therefore, $$\text{refl}_{(a,b)}:(\text{pr}_1((a,b)),\text{pr}_2((a,b))) = (a,b)$$
is well-typed, since both sides of the equality are judgmentally equal.

The ability to define dependent functions in this way means that to prove a property for all elements of a product, it is enough to prove it for its canonical elements. Applying this principle in the universal case, we call the resulting function \textbf{induction} for product types: given $A,B:U$ we have $$\text{ind}_{A\times B}:\prod_{C:A\times B\rightarrow U}\left(\prod_{(x:A)}\prod_{(y:B)}C((x,y))\right)\rightarrow \prod_{x:A\times B}C(x)$$
with the defining equation $$\text{ind}_{A\times B}(C,g,(a,b)) :\equiv g(a)(b)$$
Similarly, we may speak of a dependent function defined on pairs being obtained from the \textbf{induction principle} of the cartesian product. Induction is also called the \textbf{dependent eliminator} since it describes how to use an element of the product type, and recursion is the \textbf{non-dependent eliminator}.

Induction for the unit type is \begin{equation*}
    \text{ind}_{\mathbf{1}}:\prod_{C:\mathbf{1}\rightarrow U}C(\star)\rightarrow \prod_{x:\mathbf{1}}C(x)
\end{equation*}
with the defining equation $$\text{ind}_{\mathbf{1}}(C,c,\star) :\equiv c$$
This enables us to prove the propositional uniqueness principle for $\mathbf{1}$, which asserts that its only inhabitant is $\star$. That is, we can construct $$\text{uniq}_{\mathbf{1}}:\prod_{x:\mathbf{1}}x=\star$$
by using the defining equations $$\text{uniq}_{\mathbf{1}}(\star):\equiv \text{refl}_{\star}$$
or equivalently using induction $$\text{uniq}_{\mathbf{1}} :\equiv \text{ind}_{\mathbf{1}}(\lambda x.x = \star,\text{refl}_{\star})$$


\section{Dependent Pair Types}

We now look at generalizing the product type to allow the type of the second component of a pair to vary depending on the choice of the first component. This is called a \textbf{dependent pair type} or \textbf{$\sum$-type}, because it corresponds to an indexed sum (disjoint union or coproduct) in set thoery.

Given a type $A:U$ and a family $B:A\rightarrow U$, the dependent pair type is written as $\sum_{(x:A)}B(x):U$. The way to construct elements of a dependent pair type is by pairing: we have $(a,b) : \sum_{(x:A)}B(x)$ given $a :A$ and $b:B(a)$. If $B$ is constant, then the dependent pair type is the ordinary cartesian product type: $$\left(\sum_{x:A}B\right) \equiv (A\times B)$$
All the constructions on $\sum$-types arise as generalizations of the ones for product types, with dependent functions often replacing non-dependent ones.

For instance, the recursion principle says that to define a non-dependent function out of a $\sum$-type $f:\left(\sum_{(x:A)}B(x))\rightarrow C$, we provide a function $g:\prod_{(x:A)}B(x)\rightarrow C$, and then we can define $f$ via the defining equation $$f((a,b)) :\equiv g(a)(b)$$
For instance, we can derive the first projectino from a $\sum$-type: $$\text{pr}_1:\left(\sum_{x:A}B(x)\right)\rightarrow A$$
by the defining equation $\text{pr}_1((a,b)) :\equiv a$. However, since the type of the second component of the pair is $B(a)$, the second projection must be a dependent function, whose type involves the first projection function: $$\text{pr}_2:\prod_{p:\sum_{(x:A)}B(x)}B(\text{pr}_1(p))$$
Thus we need the induction principle for $\sum$-types. This says that to construct a dependent function out of a $\sum$-type into a family $C:\left(\sum_{(x:A)}B(x))\rightarrow U$, we need a function $$g:\prod_{(a:A)}\prod_{(b:B(a))}C((a,b))$$
We can then derive a function $$f:\prod_{p:\sum_{(x:A)}B(x)}C(p)$$
with defining equation $$f((a,b)) :\equiv g(a)(b)$$
Applying this with $C(p) :\equiv B(\text{pr}_1(p))$, we can define $\text{pr}_2:\prod_{(p:\sum_{(x:A)}B(x))}B(\text{pr}_1(p))$ with the equation $\text{pr}_2((a,b)) :\equiv b$.

\begin{definition}
    The recursor for $\sum$ is \begin{equation*}
        \text{rec}_{\sum_{(x:A)}B(x)}:\prod_{(C:U)}\left(\prod_{(x:A)}B(x)\rightarrow C\right)\rightarrow \left(\sum_{(x:A)}B(x)\right)\rightarrow C
    \end{equation*}
    with defining equation \begin{equation*}
        \text{rec}_{\sum_{(x:A)}B(x)}(C,g,(a,b)) :\equiv g(a)(b)
    \end{equation*}
    and the corresponding induction operation \begin{equation*}
        \text{ind}_{\sum_{(x:A)}B(x)}:\prod_{(C:(\sum_{(x:A)}B(x))\rightarrow U)}\left(\prod_{(a:A)}\prod_{(b:B(a))}C((a,b))\right)\rightarrow \prod_{(p:\sum_{(x:A)}B(x))}C(p)
    \end{equation*}
    with the defining equation \begin{equation*}
        \text{ind}_{\sum_{(x:A)}B(x)}(C,g,(a,b)) :\equiv g(a)(b)
    \end{equation*}
\end{definition}


\section{Coproduct Types}

Given $A,B : U$, we introduce the \textbf{coproduct type} $A+B:U$.

\section{The type of booleans}


\section{The natural numbers}


The rules we have introduced so far do not allow us to construct any infinite types. The simplest infinite type is the type $\N:U$ of natural numbers. The elements of $\N$ are constructed using $0:\N$ and the successor operation $\text{succ}:\N\rightarrow \N$. When denoting natural numbers we write $1 :\equiv \text{succ}(0), 2:\equiv \text{succ}(1),3:\equiv \text{succ}(2)$,...

The essential property of the naturals is that we can define functions by recursion and perform proofs by induction. To construct a non-dependent function $f:\N \rightarrow C$ out of the natural numbers by recursion, it is enough to provide a starting point $c_0:C$ and a ``next step" function $c_s:\N\rightarrow C\rightarrow C$. This gives rise to $f$ with the defining equations \begin{align*}
    f(0) &:\equiv c_0, \\
    f(\text{succ}(n)) &:\equiv c_s(n,f(n))
\end{align*}
We say that $f$ is defined by \textbf{primitive recursion}. 

We can define multi-variable functions by primitive recursion as well, by currying and allowing $C$ to be a function type. As in the previous cases we can package the principle of primitive recursion into a recursor: $$\text{rec}_{\N}:\prod_{(C:U)}C\rightarrow (\N\rightarrow C\rightarrow C)\rightarrow \N\rightarrow C$$
with defining equations \begin{align*}
    \text{rec}_{\N}(C,c_0,c_s,0) &:\equiv c_0 \\
    \text{rec}_{\N}(C,c_0,c_s,\text{succ}(n)) &:\equiv c_s(n,\text{rec}_{\N}(C,c_0,c_s,n))
\end{align*}
Note all functions definable only using the primitive recursion principle will be \textbf{computable}.

\section{Pattern Matching and Recursion}


\section{Propositions as Types}


\section{Identity Types}

By the propositions-as-types conception, the proposition that two elements of the same type $a,b:A$ are equal must correspond to some type. These \textbf{equality types} or \textbf{identity types} must be type families dependent on two copies of $A$.

We may write the family as $\text{Id}_A:A\rightarrow A\rightarrow U$, so that $\text{Id}_A(a,b)$ is the type representing the proposition of equality between $a$ and $b$, written previously as ``$a =_A b$" to specify the type $A$. If we have an term of $a=_Ab$, we say that it is evidence of the equality of $a$ and $b$, or that $a$ and $b$ are \textbf{propositionally equal}. The type $a=_Ab$ can contain more information than just equality. In the homotopical interpretation, terms of $a=_Ab$ are paths or equivalences between $a$ and $b$ in the space $A$. In this way we can interpret $a=_Ab$ as the type of identifications of $a$ and $b$.

The formation rule for the identity type says that given a type $A:U$, and two elements $a,b:A$, we can form the type $(a=_Ab):U$ in the same universe. The basic way to construct an element of $a=_Ab$ is to know that $a$ and $b$ are the same. Thus, the introduction rule is a dependent function $$\text{refl}:\prod_{a:A}(a=_Aa)$$
called \textbf{reflexivity}, which says that every element of $A$ is equal to itself (in a specified way). We regard $\text{refl}_a$ as being the constant path at the point $a$.

If $a$ and $b$ are judgmentally equal, $a \equiv b$, then we also have an element $\text{refl}_a: a=_Ab$. This is well-typed because $a \equiv b$ means that also the type $a =_A b$ is judgmentally equal to $a =_Aa$, which is the type of $\text{refl}_a$.

The induction principle (i.e. elimination rule) for the identity types is one of the subtlest parts of type theory, and crucial to homotopy interpretation. We begin by consiering an important consequence of it.

\begin{theorem}[Indiscernability of Identicals]
    For every family $C:A\rightarrow U$, there is a function $$f:\prod_{(x,y:A)}\prod_{(p:x=_Ay)}C(x)\rightarrow C(y)$$
    such that $$f(x,x,\text{refl}_x) :\equiv \text{id}_{C(x)}$$
\end{theorem}

This says that every family of types $C$ respects equality, in the sense $C$ to equal elements of $A$ also results in a function between the resulting types. The displayed equality states that the function associated to reflexivity is the identity function. This can be regarded as a recursion principle for the identity type. Just as $\text{rec}_{\N}$ gives a specified map $\N\rightarrow C$ for any other type $C$ of a certain sort, indiscernability of identicals gives a specified map from $x=_Ay$ to certain other reflexive, binary relations on $A$, namely those of the form $C(x)\rightarrow C(y)$ for some unary predicate $C(x)$.

\subsection{Path Induction}

The induction principle for the identity type is called \textbf{path induction}. It can be seen as stating that the family of identity types is freely generated by the elements of the form $\text{refl}_x:x=x$.

\begin{definition}[Path Induction]
    Given a family $C:\prod_{x,y:A}(x=_Ay)\rightarrow U$ and a function $c:\prod_{x:A}C(x,x,\text{refl}_x)$, there is a function $$f:\prod_{(x,y:A)}\prod_{(p:x=_Ay)}C(x,y,p)$$
    such that $f(x,x,\text{refl}_x):\equiv c(x)$.
\end{definition}

Path induction allows us to define specified functions which exhibit appropriate computational behaviour. Consider the case when $C$ does not depend on $p$, so $:A\rightarrow A\rightarrow U$, which can be regarded as a predicate depending on two elements of $A$. We are interesting in when $C(x,y)$ holds for some $x,y:A$. The hypotheses of path induction say that we know $C(x,x)$ holds for all $x:A$, so $C$ is a reflexive relation. The conclusion then tells us that $C(x,y)$ holds whenever $x=y$.

Packaging up path induction into a single function gives \begin{equation*}
    \text{ind}_{=_A}:\prod_{(C:\prod_{(x,y:A)}(x=_Ay)\rightarrow U)}\left(\prod_{(x:A)}C(x,x,\text{refl_x})\right)\rightarrow \prod_{(x,y:A)}\prod_{(p:x=_Ay)}C(x,y,p)
\end{equation*}
with the equality \begin{equation*}
    \text{ind}_{=_A}(C,c,x,x,\text{refl}_x) :\equiv c(x)
\end{equation*}

\begin{theorem}[Based Path Induction]
    Fix an element $a:A$, and suppose we are given a family $C:\prod_{x:A}(a=_Ax)\rightarrow U$, and an element $c:C(a,\text{refl}_a)$. Then we obtain a function $$f:\prod_{(x:A)}\prod_{(p:a=x)}C(x,p)$$
    such that $f(a,\text{refl}_a) :\equiv c$.
\end{theorem}

Packaged as a function, based path induction becomes: \begin{equation*}
    \text{ind}'_{=_A}:\prod_{(a:A)}\prod_{(C:\prod_{(x:A)}(a=_Ax)\rightarrow U)}C(a,\text{refl}_a)\rightarrow \prod_{(x:A)}\prod_{(p:a=_Ax)}C(x,p)
\end{equation*}
with the equality $$\text{ind}'_{=_A}(a,C,c,a,\text{refl}_a) :\equiv c$$
This is equivalent to path induction. 


\begin{remark}
    Note it is not the identity type that is inductively defined, but the identity family. In particular, path induction says that the family of types $(x=_Ay)$, as $x,y$ vary over all elements of $A$, is inductively defined by the elements of the form $\text{refl}_x$. This means that to give an element of any other family $C(x,y,p)$ dependent on a generic element $(x,y,p)$ of the identity type, it suffices to consider the cases of the form $(x,x,\text{refl}_x)$. This says that triples $(x,y,p)$ of endpoints of the path $p$, is inductively generated by the constant loops at each point $x$.
\end{remark}


