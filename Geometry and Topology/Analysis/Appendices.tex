%%%%%%%%%%%%%%%%%%%%%% - Appendices
\begin{appendices}
    \section{Multivariate Calculus - with Applications}
    
    \subsection{Vector Functions and Derivatives}
    
    \begin{defn}
        A \Emph{vector function} of one variable is a function $\vec{f}:J\subseteq \R\rightarrow \R^n$ defined by $t \mapsto \vec{f}(t)$, where $\vec{f}(t) \in \R^n$ is unique. 
    \end{defn}
    
    \begin{defn}
        Let $\vec{f}:\R\rightarrow \R^n$ be a vector valued function. Then we define the derivative of $\vec{f}$ at $t$ by \begin{equation}
            \frac{d\vec{f}(t)}{dt} = \lim\limits_{\Delta t\rightarrow 0} \frac{\vec{f}(t+\Delta t) - \vec{f}(t)}{\Delta t}
        \end{equation}
    \end{defn}
    
    \begin{rmk}[Properties]
        Let $\vec{f}:J\subseteq \R\rightarrow \R^n$ and $\vec{g}:I\subseteq \R\rightarrow \R^n$ be vector functions such that \begin{equation}
            \vec{f} = \langle f_1,...,f_n\rangle \; and\;\vec{g} = \langle g_1,...,g_n\rangle 
        \end{equation}
        The for all $t \in J \cap I$ and all $\lambda: D_{\lambda}\subseteq \R\rightarrow \R$ we have \begin{enumerate}
            \item $(\vec{f}+\vec{g})(t) := \vec{f}(t) + \vec{g}(t)$
            \item $(\lambda\vec{f})(t) := \lambda(t)\vec{f}(t)$
            \item $(\vec{f}\cdot\vec{g})(t) = \vec{f}(t)\cdot \vec{g}(t)$
            \item For $n = 3$, $(\vec{f} \times \vec{g})(t) = \vec{f}(t) \times \vec{g}(t)$
        \end{enumerate}
    \end{rmk}
        
    \begin{rec}
        Given $A \in \R^{n\times n}$, the lagrange expansion is \begin{equation}
            \det(A) = \sum\limits_{i=1}^n(-1)^{i+j}\det(A_{ij})\;(\text{along $j$-th column})
        \end{equation}
        \begin{equation}
            \det(A) = \sum\limits_{j=1}^n(-1)^{i+j}\det(A_{ij})\;(\text{along $i$-th row})
        \end{equation}
        where $A_{ij}$ is the minor matrix of $A$ obtained by deleting the ith ro and jth column of $A$.
    \end{rec}

    \begin{defn}
        Suppose $\vec{f}(t) = \langle f_1(t),...,f_n(t)\rangle$ and $\vec{L} = \langle L_1,...,L_n\rangle$ Then \begin{equation}
            \lim\limits_{t\rightarrow t_0}\vec{f}(t) = \vec{L} \implies \lim\limits_{t\rightarrow t_0}f_i(t) = L_i, \forall 1\leq i \leq n
        \end{equation}
    \end{defn}

    \begin{rmk}
        The right hand limit $\lim_{t\rightarrow t_0^+}\vec{f}(t)$ and the left hand limit $\lim_{t\rightarrow t_0^-}\vec{f}(t)$ are defined in the same way.
    \end{rmk}

    \begin{rmk}[Limit Rules]
        If $\lim_{t\rightarrow t_0}$ for $\vec{f}(t)$, $\vec{g}(t)$, and $\lambda(t)$ exist and $k \in \R$, then \begin{enumerate}
            \item $\lim\limits_{t\rightarrow t_0}(\vec{f}(t)+\vec{g}(t)) = \lim\limits_{t\rightarrow t_0}\vec{f}(t) + \lim\limits_{t\rightarrow t_0}\vec{g}(t)$
            \item $\lim\limits_{t\rightarrow t_0}k\vec{f}(t) = k\lim\limits_{t\rightarrow t_0}\vec{f}(t)$
            \item $\lim\limits_{t\rightarrow t_0}\lambda(t)\vec{f}(t) = (\lim\limits_{t\rightarrow t_0}\lambda(t))(\lim\limits_{t\rightarrow t_0}\vec{f}(t))$
            \item $\lim\limits_{t\rightarrow t_0}\vec{f}(t)\cdot \vec{g}(t) = (\lim\limits_{t\rightarrow t_0}\vec{f}(t))\cdot(\lim\limits_{t\rightarrow t_0}\vec{g}(t))$
            \item $\lim\limits_{t\rightarrow t_0}\vec{f}(t)\times \vec{g}(t) = (\lim\limits_{t\rightarrow t_0}\vec{f}(t))\times(\lim\limits_{t\rightarrow t_0}\vec{g}(t))$, for $n = 3$.
        \end{enumerate}
    \end{rmk}


    \begin{defn}[Continuity]
        A vector function $\vec{f}(t) = \langle f_1(t),...,f_n(t)\rangle$ is said to be continuous at $t = t_0$ if \begin{equation}
            \lim\limits_{t\rightarrow t_0}\vec{f}(t) = \vec{f}(t_0)
        \end{equation}
        In other words, each component function is continuous at $t = t_0$.
    \end{defn}

    \begin{defn}[Differentiability]
        A vector function $\vec{f}(t) = \langle f_1(t),...,f_n(t)\rangle$ is said to be differentiable at $t = t_0$ if $\vec{f}(t)$ is defined at and around $t$ and \begin{equation}
            \lim\limits_{t\rightarrow t_0}\frac{\vec{f}(t) - \vec{f}(t_0)}{t - t_0}
        \end{equation}
        exists. We call this limit the derivative of $\vec{f}(t)$ at $t = t_0$ and is denoted by $\vec{f}'(t_0)$ if it exists.
    \end{defn}

    \begin{thm}
        We say that the vector function is differentiable at $t = t_0$ if and only if its component functions are differentiable at $t = t_0$, and \begin{equation}
            \vec{f}'(t_0) = \langle f_1'(t_0),...,f_n'(t_0)\rangle
        \end{equation}
    \end{thm}



    \begin{rmk}[Differentiation Rules]
        Let $\vec{f}(t), \vec{g}(t)$ and $\lambda(t)$ be differentiable and $k \in \R$. Then \begin{enumerate}
            \item $(\vec{f}+\vec{g})'(t) = \vec{f}'(t) + \vec{g}'(t)$
            \item $(k\vec{f})'(t) = k\vec{f}'(t)$
            \item $(\lambda\vec{f})'(t) = \lambda'(t)\vec{f}(t) + \lambda(t)\vec{f}'(t)$
            \item $(\vec{f}\cdot\vec{g})'(t) = \vec{f}'(t)\cdot \vec{g}(t) + \vec{f}(t)\cdot \vec{g}'(t)$
            \item $(\vec{f}\times \vec{g})'(t) = \vec{f}'(t)\times \vec{g}(t) + \vec{f}(t)\times \vec{g}'(t)$ for $n = 3$
            \item $(\vec{f}(\lambda(t)))' = \vec{f}'(\lambda(t))\lambda'(t)$
        \end{enumerate}
    \end{rmk}


    \begin{defn}
        Let $\vec{f}(t) = \langle f_1(t),...,f_n(t)\rangle$ be a vector function defined on a closed interval $[a,b]$. We say that $\vec{f}(t)$ is \Emph{integrable} on $[a,b]$ if each $f_i(t)$ is integrable on $[a,b]$. When that is the case we define \begin{equation}
            \int_a^b\vec{f}(t)dt := \left\langle \int_a^bf_1(t)dt,...,\int_a^bf_n(t)dt\right\rangle
        \end{equation}
        the definite integral of $\vec{f}(t)$ on $[a,b]$
    \end{defn}


    \begin{rmk}[Properties]
        Let $\vec{f}(t),\vec{g}(t)$ be integrable on $[a,b]$ and $k \in \R$. Then \begin{enumerate}
            \item $\int\limits_a^bk\vec{f}(t)dt = k\int\limits_a^b\vec{f}(t)dt$
            \item $\int\limits_a^b(\vec{f}(t)+\vec{g}(t))dt = \int\limits_a^b\vec{f}(t)dt + \int\limits_a^b\vec{g}(t)dt$
            \item $\int\limits_a^b\vec{f}(t)dt = \int\limits_a^c\vec{f}(t)dt + \int\limits_c^b\vec{f}(t)dt$, $a \leq c \leq b$.
            \item $\left|\int\limits_a^b\vec{f}(t)dt\right| \leq \int\limits_a^b\left|\vec{f}(t)\right|dt$ (\Emph{The triangle inequality})
        \end{enumerate}
    \end{rmk}


    \subsection{Parametric Curves and Paths}

    \begin{defn}
        Let $\vec{f}:J\subseteq \R\rightarrow \R^n$ given by $t \mapsto \langle f_1(t),..., f_n(t)\rangle$, and $f_j: J\subseteq \R\rightarrow \R$ be the $j$th component function of $\vec{f}$. We define the image \begin{equation}
            \vec{f}(J) = \mathscr{C}
        \end{equation}
        and call $\mathscr{C}$ a \Emph{curve parametrized} by $\vec{f}$. If $J = [a,b]$ for $a \leq b \in \R$ and $\vec{f}(a) = \vec{f}(b)$, then $\mathscr{C}$ is a \Emph{closed curve}. If there exists a parametrization $\vec{g}:I \subseteq \R \rightarrow \R^n$ of $\mathscr{C}$ such that $\vec{g}$ is injective (except maybe at end points), then $\mathscr{C}$ is said to be a \Emph{non-self intersecting} curve. If such a curve is closed it is called a \Emph{simple closed curve}.
    \end{defn}

    \begin{rmk}
        The pair $(\vec{f}(t), J)$ is called a \Emph{parametrization} of the curve $\mathscr{C}$. The triple $(\vec{f}(t), J, \mathscr{C})$ is called a \Emph{path with curve $\mathscr{C}$}.
    \end{rmk}

    \begin{defn}
        If $\vec{f}:J \subseteq \R\rightarrow \R^n$ is a one-to-one function, then the image $\vec{f}(J) = \mathscr{C}$ is an \Emph{oriented curve} and $\vec{f}$ is a \Emph{consistently oriented path} which covers $\mathscr{C}$.
    \end{defn}

    \begin{rmk}[Ellipse]
        The parametrization of an ellipse with equation $\frac{(x-x_0)^2}{a^2} + \frac{(y-y_0)^2}{b^2} = 1$, $a,b > 0$ are constants, is $\vec{f}(t) = \langle x_0 + a\cos(t), y_0 + b\sin(t)\rangle$, for $t \in [0,2\pi)$.
    \end{rmk}


    \begin{rmk}[Line]
        Let $A$ and $B$ be points in $\R^n$. The parametrization of the line segment connecting $A$ to $B$ is \begin{equation}
            (\vec{f}(t) = \overline{OA} + t(\overline{OB} - \overline{OA}), [0,1])
        \end{equation}
    \end{rmk}


    \begin{defn}
        The tangent vector to a curve $\mathscr{C}$ with parametrization $(\vec{f}, I)$ exists at a point $t = t_0$ if $\vec{f}$ is $\vec{f}'(t)$ exists.
    \end{defn}


    \begin{defn}
        Let $\mathscr{C}$ be a parametric curve with parametrization $(\vec{f}(t),[a,b])$. If $\vec{f}(t)$ is differentiable at $t = t_0$, then $\vec{f}(t_0)$ is called a \Emph{tangent vector} to $\mathscr{C}$ at $P_0 = tip(\vec{f}(t_0))$, provided $\vec{f}'(t_0) \neq \vec{0}$. If $\vec{f}(t)$ is differentiable at every $t \in (a,b)$ and $\vec{f}'(t) \neq \vec{0}$, we say that the parametric curve $\mathscr{C}$ is a \Emph{smooth parametric curve}.
    \end{defn}
    

    \begin{defn}
        Let $\mathscr{C}$ be a parametric curve and let $(\vec{f}(t), I)$ be a parametrization. If $P_0 = tip(\vec{f}(t_0))$, for $t_0 \in I$, such that $\vec{f}'(t_0) \neq \vec{0}$. Then \begin{equation}
            \vec{T}(t_0) = \frac{1}{|\vec{f}'(t_0)|}\vec{f}'(t_0)
        \end{equation}
        is called the \Emph{unit tangent vector} associated with the parametrization $(\vec{f}(t), I)$. $\vec{T}(t_0)$ always forces the direction in which $\vec{f}(t)$ traces $\mathscr{C}$. The vector \begin{equation}
            \vec{N}(t_0) = \frac{1}{|\vec{T}'(t_0)|}\vec{T}'(t_0)
        \end{equation}
        is perpendicular to $\vec{T}(t_0)$ and is called the \Emph{unit principal normal} to $\mathscr{C}$ at $P_0$.
    \end{defn}


    
    \begin{defn}
        A curve $\mathscr{C}$ is called \Emph{piecewise smooth} if it consists of a finite number of smooth parametric curves $\mathscr{C}_1,...,\mathscr{C}_k$, where the endpoint of $\mathscr{C}_i$ is the starting point of $\mathscr{C}_{i+1}$ for $i = 1,2,...,k-1$.
    \end{defn}


    \begin{defn}
        Let $\mathscr{C}$ be a bounded continuous curve specified by a parametrization $\vec{f}:[a,b]\rightarrow \R^n$. We consider partitions of $[a,b]$ into $n$-subintervals by \begin{equation}
            a= t_0 < t_1 < ... < t_n = b
        \end{equation}
        So the points $\vec{f}(t_i)$ subdivide $\mathscr{C}$, and using the \Emph{chord length} $|\vec{f}(t_i) - \vec{f}(t_{i-1})|$ we define the sequence of lengths approximating $\mathscr{C}$ by \begin{equation}
            s_n = \sum\limits_{i=1}^n|\vec{f}(t_i) - \vec{f}(t_{i-1})|
        \end{equation}
        We say $\mathscr{C}$ is \Emph{rectifiable} if there exists $K \in \R$ such that $s_n \leq K$ for all $n \in \N$ and all choices of points. From the completeness axiom of $\R$ there exists a least such $K$. This $K$ we define as the \Emph{length} of $\mathscr{C}$ and we denote it by $s$. Let $\Delta t_i = t_i - t_{i-1}$ and $\Delta \vec{f}_i = \vec{f}(t_i) - \vec{f}(t_{i-1})$ so \begin{equation}
            s_n = \sum\limits_{i=1}^n\left|\frac{\Delta \vec{f}_i}{\Delta t_i}\right|\Delta t_i
        \end{equation}
        If $\vec{f}(t)$ has a continuous derivative, then \begin{equation}
            s = \lim\limits_{\underset{sup\Delta t_i\rightarrow 0}{n\rightarrow \infty}} s_n = \int\limits_a^b\left|\frac{d\vec{f}}{dt}\right|dt
        \end{equation}
        is the \Emph{arclength}.
    \end{defn}


    \begin{defn}
        Let $\vec{f}:[a,b] \rightarrow \R^n$ be a smooth parammetrization of a curve $\mathscr{C}$. Then the \Emph{arclength function} of $\vec{f}$ is a function $s:[a,b] \rightarrow \R$ where \begin{equation}
            s(t) := \int\limits_a^b\left|\frac{d\vec{f}}{dt}\right|dt
        \end{equation}
        and the \Emph{arclength element} for $\mathscr{C}$ is given by \begin{equation}
            ds := \left|\frac{d}{dt}\vec{f}\right|dt
        \end{equation}
    \end{defn}

    \begin{defn}
        If $\vec{f}:J\subseteq \R\rightarrow \R^n$ parametrizes a curve $\mathscr{C}$ with the parameter being the arclenght along the curve relative to some inital point, then we call this an \Emph{arclength} or \Emph{intrinsic parametrization}. Such a parametrization traces $\mathscr{C}$ at unit speed \begin{equation}
            \left|\frac{d\vec{f}(s)}{dt}\right| = 1
        \end{equation}
    \end{defn}
    

    

    \subsection{Curvature and the Frenet Frame}

    The Frenet Frame forms a right handed orthogonal basis at any point along a smooth curve.  The curvature measures the rate at which a curve is turning (away from its tangent line) at any point. The Torsion measures the rate at which the curve is twisting (out of the plane in which it is turning) at any point.

    \begin{defn}
        Let $\gamma:I\subseteq \R\rightarrow \R^n$ denote a smooth parametric curve. Then $d\gamma/dt$ gives a tangent vector to the curve at any point, and it points in the direction of the orientation of the curve. We assume that this derivative is nowhere zero so we can define a \Emph{unit tangent vector}, $\hat{T}(t)$, at $\gamma(t)$ by \begin{equation*}
            \hat{T}(t) := \frac{d\gamma(t)/dt}{||d\gamma(t)/dt||}
        \end{equation*}
    \end{defn}
    
    Note that when the parameterization is in terms of arclength the denominator is $1$. If $\gamma(t)$ has a continuous nonvanishing derivative, then $\hat{T}(t)$ is continuous and the angle it makes with any fixed vector is also continuous.

    Having unit length the tangent vector field satisfies $\hat{T}(t)\cdot\hat{T}(t) = 1$, so $2\hat{T}(t)\cdot \frac{d\hat{T}(t)}{dt} = 0$. Thus, the derivative of the tangent is orthogonal to the tangent at any point on the curve.

    \begin{defn}
        The \Emph{curvature} of a smooth curve $\gamma:I\subseteq \R\rightarrow \R^n$ parameterized in terms of arclength is defined to be \begin{equation*}
            \kappa(s) = \left|\frac{d\hat{T}(s)}{ds}\right|
        \end{equation*}
        The \Emph{radius of curvature}, denoted $\rho$, is the reciprocal of the curvature: \begin{equation*}
            \rho(s) = \frac{1}{\kappa(s)}
        \end{equation*}
    \end{defn}

    The curvature of $\gamma$ measures the rate of turning of the tangent line to the curve at any point.

    \begin{defn}
        If the curvature of a smooth curve $\gamma$ is nonzero, we define the \Emph{unit principal normal} to $\gamma$ in terms of an arclength parameterization to be \begin{equation*}
            \hat{N}(s) = \frac{d\hat{T}(s)/ds}{||d\hat{T}(s)/ds||}
        \end{equation*}
    \end{defn}

    Note that the principal normal is perpendicular to the curve at any point, and points in the direction that the unit tangent, and consequently the curve, is turning.

    \begin{thm}
        Let $\kappa > 0$ on an interval containing $s$ and let $\delta \theta$ be the angle between $\hat{T}(s+\delta s)$ and $\hat{T}(s)$, the unit tangent vectors at neighboring points on the curve. Then \begin{equation*}
            \kappa(s) = \lim\limits_{\delta s\rightarrow 0} \left|\frac{\delta s}{\delta s}\right|
        \end{equation*}
    \end{thm}

    The plane passing through $\gamma(s)$ and spanned by $\hat{T}(s)$ and $\hat{N}(s)$ is called the \Emph{osculating plane} to the curve at $\gamma(s)$.

    Assuming $\kappa(s) \neq 0$, let $\gamma_c(s) = \gamma(s)+\rho(s)\hat{N}(s)$. Then $\gamma_c(s)$ is called the \Emph{center of curvature} to $\gamma$ at $\gamma(s)$, and the circle in the osculating plane with center $\gamma_c(s)$ and radius $\rho(s)$ is the best approximation of the curve near $\gamma(s)$, and is called the \Emph{osculating circle}.


    \begin{defn}
        At any point on $\gamma(s)$ where $\hat{T}(s),\hat{N}(s)$ are both defined, we define the \Emph{unit binormal} field to be \begin{equation*}
            \hat{B}(s) = \hat{T}(s)\times \hat{N}(s)
        \end{equation*}
    \end{defn}

    The three vectors $\{\hat{T}(s),\hat{N}(s),\hat{B}(s)\}$ correspond to a right-handed coordinate system, the \Emph{Frenet Frame} for $\gamma$ at any point $\gamma(s)$. Note that \begin{equation*}
        \frac{d\hat{B}}{ds} = \hat{T}\times \frac{d\hat{N}}{ds}
    \end{equation*}
    and as with any unit normal field, $\hat{B}$'s derivative is orthogonal to itself, so we have that $\frac{d\hat{B}}{ds}$ lies on the line spanned by $\hat{N}$. 

    \begin{defn}
        On any interval where $\kappa(s) \neq 0$ there exists a function $\tau(s)$ such that \begin{equation*}
            \frac{d\hat{B}}{ds} = -\tau(s)\hat{N}(s)
        \end{equation*}
        The number $\tau(s)$ is called the \Emph{torsion} of $\gamma$ at $\gamma(s)$.
    \end{defn}

    The torsion measures the degree of twisting that the curve exhibits near a point, that is, the extent to which the curve fails to be planar.

    Let $\delta \psi$ be the angle between $\hat{B}(s)$ and $\hat{B}(s+\delta s)$. Then \begin{equation*}
        |\tau(s)| = \lim\limits_{\delta s\rightarrow 0} \left|\frac{\delta \psi}{\delta s}\right|
    \end{equation*}

    \begin{prop}
        The \Emph{Frenet-Serret Formulas} for our Frenet Frame are given by \begin{align*}
            \frac{d\hat{T}}{ds} &= \kappa(s)\hat{N}(s) \\
            \frac{d\hat{N}}{ds} &= -\kappa(s)\hat{T}(s)+\tau(s) \hat{B}(s) \\
            \frac{d\hat{B}}{ds} &= -\tau(s)\hat{N}(s)
        \end{align*}
        These can be summarized by the matrix equation \begin{equation*}
            \frac{d}{ds}\begin{bmatrix} \hat{T}(s) \\ \hat{N}(s) \\ \hat{B}(s) \end{bmatrix} = \begin{bmatrix} 0 & \kappa(s) & 0 \\ -\kappa(s) & 0 & \tau(s) \\ 0 & -\tau(s) & 0 \end{bmatrix} \begin{bmatrix} \hat{T}(s) \\ \hat{N}(s) \\ \hat{B}(s) \end{bmatrix} 
        \end{equation*}
    \end{prop}

    \begin{thm}
        Let $\gamma_1$ and $\gamma_2$ be two curves, both of which have the same nonvanishing curvature function $\kappa(s)$ and the same torsion function $\tau(s)$. Then the curves are congruent. That is, there exists a rigid motion so as to coincide one curve exactly with the other.
    \end{thm}





    \subsection{Functions of Several Variables}

    \begin{defn}
        A function $f:\mathscr{D}(f) \subseteq \R^n\rightarrow \R$, where $\mathscr{D}(f)$ is the \Emph{domain} of $f$, is called a \Emph{scalar field on $\mathscr{D}(f)$}. The image of $f$ is \begin{equation}
            \ran(f):=\{x \in \R:\exists\vec{v} \in \mathscr{D}(f), f(\vec{v}) = x\}
        \end{equation}
        The natural domain of $f$ is the largest subset of $\R^n$ such that $f$ is well-defined.
    \end{defn}

    
    \begin{defn}
        The \Emph{graph} of a function $f:A\rightarrow B$ is the set \begin{equation}
            \Gamma(f):=\{(x,f(x)):x \in A\}
        \end{equation}
        For \begin{equation}
            f:\prod\limits_{i=1}^nX_i\rightarrow B
        \end{equation}
        we have the graph \begin{equation}
            \Gamma(f):= \{((x_1,...,x_n),f(x_1,...,x_n)):(x_1,...,x_n) \in \prod\limits_{i=1}^nX_i\}
        \end{equation}
    \end{defn}

    \begin{rmk}
        The grap of a function $f:\R^n\rightarrow \R$ can be considered as a surface in $\R^{n+1}$ by a natural embedding.
    \end{rmk}


    \begin{defn}
        Given a function $f:\R^n\rightarrow \R$, a \Emph{k-level surface} of $f$ is a set \begin{equation}
            S_k := \{\vec{x} \in \R^n: f(\vec{x}) = k\}
        \end{equation}
        where $k$ is a fixed constant.
    \end{defn}


    \begin{defn}
        Let $\vec{x}_0 \in \R^n$ and $r > 0$ a real number. Then the \Emph{open ball of radius $r$} centered at $\vec{x}_0$ is defined as \begin{equation}
            B_r(\vec{x}_0) := \{\vec{x} \in \R^n: ||\vec{x} - \vec{x}_0|| < r\}
        \end{equation}
        the \Emph{closed ball} is defined by \begin{equation}
            \overline{B}_r(\vec{x}_0) := \{\vec{x} \in \R^n: ||\vec{x} - \vec{x}_0|| \leq r\}
        \end{equation}
    \end{defn}

    \begin{defn}
        A \Emph{neighborhood} of a point $\vec{x}_0 \in \R^n$ is any set $U \subseteq \R^n$ such that there exists $r > 0$ so that $B_r(\vec{x}_0) \subseteq U$.
    \end{defn}


    \begin{defn}
        Let $E \subseteq \R^n$, where we equip $\R^n$ with $\mathscr{T}_{st}$. We say $E$ is \Emph{open} if $E \in \mathscr{T}_{st}$. Equivalently, $E$ is \Emph{open} if for all $\vec{x} \in E$, $E$ is a \Emph{neighborhood} of $\vec{x}$. We sau $E$ is \Emph{closed} if its \Emph{complement} $E^C = \R^n\backslash E$ is \Emph{open}.
    \end{defn}

    \begin{defn}
        A point $\vec{x}_0 \in \R^n$ is called a \Emph{boundary point} of $E$ if for any $r > 0$, $B_r(\vec{x}_0) \cap E \neq \emptyset$ and $B_r(\vec{x}_0) \cap E^C \neq \emptyset$.
    \end{defn}

    \begin{defn}
        The set of all \Emph{boundary points} of a set $E \subseteq \R^n$ is called the \Emph{boundary} of $E$.
    \end{defn}

    \begin{defn}
        Let $E \subseteq \R^n$. We say that $E$ is \Emph{bounded} if there exists $R > 0$ such that $||\vec{x}|| \leq R$ for all $\vec{x} \in E$. $E$ is \Emph{unbounded} if for all $R > 0$ there exists $\vec{x}_0 \in E$ such that $||\vec{x}_0|| > R$.
    \end{defn}


    \begin{defn}
        Let $f:\R^n\rightarrow \R$ be a function. We say $\lim_{\vec{x}\rightarrow \vec{x}_0}f(\vec{x}) = L$, provided that \begin{enumerate}
            \item Every punctered neighborhood $B_r^*(\vec{x}_0)$ of $\vec{x}_0$ intersects $\mathscr{D}(f)$ 
                \begin{equation}
                        B_r^*(\vec{x}_0) \cap \mathscr{D}(f) \neq \emptyset
                \end{equation}
                that is, $\vec{x}_0$ is a limit point of $\mathscr{D}(f)$.
            \item For all $\varepsilon > 0$, there exists $\delta > 0$ such that $f(\vec{x}) \in B_{\varepsilon}(L)$ whenever $\vec{x} \in B_{\delta}^*(\vec{x}_0) \cap \mathscr{D}(f)$. That is \begin{equation}
                    f(\mathscr{D}(f)\cap B_{\delta}^*(\vec{x}_0)) \subseteq B_{\varepsilon}(L)
            \end{equation}
        \end{enumerate}
    \end{defn}

    
    \begin{rmk}
        As $\R^n$ and $\R$ are metric spaces, they are Hausdorff, so if the limit exists it is unique.
    \end{rmk}


    \begin{rmk}[Limit Properties]
        Let $f:\R^n\rightarrow \R$ and $g:\R^n\rightarrow \R$ and $\vec{x}_0\in \R^n$ such that $\lim_{\vec{x}\rightarrow \vec{x}_0}f(\vec{x}_0) = L$ and $\lim_{\vec{x}\rightarrow \vec{x}_0}g(\vec{x}) = M$. Then if $\vec{x}_0$ is not an isolated point of $\mathscr{D}(f) \cap \mathscr{D}(g)$, then \begin{enumerate}
            \item $\lim_{\vec{x}\rightarrow \vec{x}_0}(f(\vec{x}) \pm g(\vec{x})) = L \pm M$
            \item $\lim_{\vec{x}\rightarrow \vec{x}_0}f(\vec{x})g(\vec{x}) = LM$
            \item $\lim_{\vec{x}\rightarrow \vec{x}_0}\frac{f(\vec{x})}{g(\vec{x})} = \frac{L}{M}$ if $M \neq 0$
            \item If $F:\R\rightarrow X$ is continuous at $L$, then $\lim_{\vec{x}\rightarrow \vec{x}_0}F(f(\vec{x})) = F(L)$.
        \end{enumerate}
    \end{rmk}

    \begin{defn}
        We say a function $f:\R^n\rightarrow \R$ is \Emph{continuous at a point} $\vec{x}_0\in \R^n$ if \begin{equation}
            \lim_{\vec{x}\rightarrow \vec{x}_0} f(\vec{x}) = f(\vec{x}_0)
        \end{equation}
    \end{defn}


    

    \begin{rmk}
        If setting $x = x_0$ and $y = y_0$ in the expression for $f(x,y)$ does not evalutate to a real number, then we can try using polar coordinates: $x = x_0 + r\cos(\theta)$ and $y = y_0 + r\sin(\theta)$. Recall $r = \sqrt{(x-x_0)^2 + (y-y_0)^2}, 0 \leq \theta < 2\pi$. As a result, $(x,y) \rightarrow (x_0,y_0)$ is equivalent to $r\rightarrow 0$, so \begin{equation}
            \lim\limits_{(x,y) \rightarrow (x_0,y_0)}f(x,y) = \lim\limits_{r\rightarrow 0}f(x_0+r\cos(\theta), y_0+r\sin(\theta))
        \end{equation}
    \end{rmk}

    \begin{thm}[Squeeze Theorem]
        Let $f(x,y), g(x,y)$ and $h(x,y)$ be defined in a neighborhood $U$ of $(x_0,y_0)$, except maybe at $(x_0,y_0)$, and such that \begin{equation}
            g(x,y) \leq f(x,y) \leq h(x,y), \forall (x,y) \in U\backslash\{(x_0,y_0)\}
        \end{equation}
        If \begin{equation}
            \lim\limits_{(x,y)\rightarrow (x_0,y_0)}g(x) = L,\lim\limits_{(x,y)\rightarrow (x_0,y_0)}h(x) = L
        \end{equation}
        Then $\lim\limits_{(x,y)\rightarrow (x_0,y_0)} f(x) = L$.
    \end{thm}

    \begin{thm}
        If one can find two continuous parametric curves $\mathscr{C}_1$ and $\mathscr{C}_2$ that pass through the point $(x_0,y_0)$ such that \begin{equation}
            \lim\limits_{\underset{(x,y) \in \mathscr{C}_1}{(x,y)\rightarrow (x_0,y_0)}}f(x,y) = L_1, \lim\limits_{\underset{(x,y) \in \mathscr{C}_2}{(x,y)\rightarrow (x_0,y_0)}}f(x,y) = L_2,\;and\;L_1 \neq L_2
        \end{equation}
        then $\lim\limits_{(x,y)\rightarrow (x_0,y_0)}f(x,y)$ does not exist.
    \end{thm}

   
    \begin{defn}
        Let $f:\R^n\rightarrow \R$ be a function. \begin{enumerate}
            \item We say that $f$ is \Emph{continuous} at $\vec{x}_0 \in \R^n$ if \begin{enumerate}
                    \item There exists a neighborhood $U$ of $\vec{x}_0$ such that $f(U)$ is defined
                    \item $\lim_{\vec{x}\rightarrow \vec{x}_0}f(\vec{x}) = f(\vec{x}_0)$
            \end{enumerate}
            \item We say that $f$ is continuous on a region $D$ if it s continuous at every point $\vec{x}$ in the region.
        \end{enumerate}
    \end{defn}


    \begin{rmk}[Constructing Continuous Functions]
        Let $f,g:\R^n\rightarrow \R$ be continuous at $\vec{x}_0 \in \R^n$, and if $\lambda \in \R$, then \begin{enumerate}
            \item $f\pm g$, $f\cdot g$, and $\lambda f$ are continuous at $\vec{x}_0$
            \item $f/g$ is continuous at $\vec{x}_0$ provided $g(\vec{x}_0) \neq 0$.
        \end{enumerate}
        Suppose $u(t)$ is continuous at $t_0 = f(\vec{x}_0)$. Then $u(f(\vec{x}))$ is continuous at $\vec{x}_0$.
    \end{rmk}

    \begin{namthm}[Extreme Value Theorem]
        If $f:\R^n \rightarrow \R$ is continuous on a closed and bounded region $D \subseteq \R^n$, then there exist $\vec{x}_m,\vec{x}_M \in D$ such that \begin{equation}
            f(\vec{x}_m) \leq f(\vec{x}) \leq f(\vec{x}_M), \forall \vec{x} \in D
        \end{equation}
        $m = f(\vec{x}_m)$ is called the \Emph{absolute minimum} of $f$ on $D$, while $M = f(\vec{x}_M)$ is called the \Emph{absolute maximum} of $f$ on $D$.
    \end{namthm}
    

    \subsection{Partial Derivatives}

    \begin{defn}
        The \Emph{first partial derivatives} of a function $f:\R^n\rightarrow \R$ with respect to the variable $x_i$, $1 \leq i \leq n$, is the function \begin{equation}
            f_i(x_1,...,x_n) = \lim_{h\rightarrow 0}\frac{f(x_1,...,x_i+h,...,x_n) - f(x_1,...,x_i,...,x_n)}{h}
        \end{equation}
        provided the limit exists and $f$ is defined in a neighbrohood of $(x_1,...,x_n)$.
    \end{defn}

    \begin{nota}
        We often write \begin{equation}
            \frac{\partial}{\partial x_i}f(x_1,...,x_n) = f_i(x_1,...,x_n) = D_if(x_1,...,x_n)
        \end{equation}
        and \begin{equation}
            \left(\frac{\partial}{\partial x_i}f(\vec{x})\right)\Big\rvert_{\vec{x}_0} = f_i(\vec{x}_0) = D_if(\vec{x}_0)
        \end{equation}
    \end{nota}
    

    \begin{rmk}
        If a function $f:J\subseteq \R^n\rightarrow \R$ has first partial derivatives at $\vec{x}_0$ in a region $D \subseteq \R^n$, then this defines $n$ new functions \begin{equation}
            \frac{\partial f}{\partial x_i}\Big\rvert:\R\rightarrow \R
        \end{equation}
        where we differentiate $f$ with respect to $x_i$.
    \end{rmk}

    \begin{defn}
        Given a function $f:\R^n\rightarrow \R$, the \Emph{gradient} of $f$ is the vector function \begin{equation}
            \nabla f = grad(f):\R^n\rightarrow\R^n
        \end{equation}
        such that \begin{equation}
            \nabla f(x_1,...,x_n) = \left\langle \frac{\partial}{\partial x_1}f,...,\frac{\partial}{\partial x_n}f\right\rangle
        \end{equation}
        where $\nabla$ is the \Emph{del operator} \begin{equation}
            \nabla = \left[\frac{\partial}{\partial x_1},...,\frac{\partial}{\partial x_n}\right]^T
        \end{equation}
    \end{defn}


    \begin{defn}
        Let $f:\R^n\rightarrow \R$ be defined in a neighborhood of a point $\vec{x}_0$ such that its first partial derivatives exist at $\vec{x}_0$. Then by definition, the \Emph{linear approximation} of $f$ at $\vec{x}_0$ is the polynomial of degree $1$, $L(\vec{x})$, that matches $f$ at $\vec{x}_0$ and matches its partials at $\vec{x}_0$. In particular, we have that \begin{equation}
            L(\vec{x},\vec{x}_0) = f(\vec{x}_0) + \sum_{i=1}^n \frac{\partial}{\partial x_i}f(\vec{x}_0)(x_i - x_{i,0})
        \end{equation}
    \end{defn}


    \begin{defn}
        Let $f:D\subseteq \R^n\rightarrow \R$ be a function defined around $\vec{x}_0 \in \R^n$ with first partials also defined. Let $L(\vec{x})$ be its linear approximation at $\vec{x}_0$. We say that $f$ is \Emph{differentiable} at $\vec{x}_0$ if the limit \begin{equation}
            \lim_{\vec{x}\rightarrow \vec{x}_0} \frac{f(\vec{x}) - L(\vec{x})}{||\vec{x}-\vec{x}_0||} = 0
        \end{equation}
    \end{defn}


    \begin{defn}
        Let $f:D\subseteq \R^n\rightarrow \R^m$ be a multivariate function defined in a neighborhood of $\vec{x}_0 \in \R^n$ with first partial derivatives also defined. Then the \Emph{Jacobian matrix} of $f$ is defined to be \begin{equation}
            Df(\vec{x}) = \begin{bmatrix} \partial_1f_1(\vec{x}) & \partial_2f_1(\vec{x}) & \hdots & \partial_nf_1(\vec{x}) \\
                \partial_1f_2(\vec{x}) & \ddots & \ddots & \vdots \\
                \vdots & \ddots & \ddots & \vdots \\
                \partial_1f_m(\vec{x}) & \hdots & \hdots & \partial_nf_m(\vec{x})
            \end{bmatrix}
        \end{equation}
        Then we say that $f$ is differentiable at $\vec{x}_0$ if \begin{equation}
            \lim_{\vec{x}\rightarrow \vec{x}_0}\frac{||f(\vec{x}) - f(\vec{x}_0) - Df(\vec{x} - \vec{x}_0)||}{||\vec{x} - \vec{x}_0||} = 0
        \end{equation}
        If all the first partial derivatives of $f$ are continuous in a neighborhood of $\vec{x}_0$ then this holds.
    \end{defn}


    \begin{rmk}[Properties]
        For $f:D \subseteq \R^n\rightarrow \R^m$: \begin{enumerate}
            \item If $f$ is differentiable at $\vec{x}_0$, then $f$ is continuous at $\vec{x}_0$
            \item If $f$ and $g$ are differentiable at $\vec{x}_0$, then $f\pm g, kf, fg$ ($m = 1$) are differentiable at $\vec{x}_0$.
            \item If the partials of $f$ are continuous in a neighborhood of $\vec{x}_0$, then $f$ is differentiable at $\vec{x}_0$. The converse is not true in general.
        \end{enumerate}
    \end{rmk}

    \begin{defn}
        Consider $f:D\subseteq \R^n\rightarrow \R$, we have \begin{equation}
            \partial_if(\vec{x}_0) = \lim_{t\rightarrow 0}\frac{f(\vec{x}_0+t\hat{e}_i) - f(\vec{x}_0)}{t}
        \end{equation}
        so we can generalize this to define the \Emph{directional derivative} in the direction of $\hat{u}$: \begin{equation}
            \partial_{\hat{u}}f(\vec{x}_0) = \lim_{t\rightarrow 0}\frac{f(\vec{x}_0+t\hat{u}) - f(\vec{x}_0)}{t}
        \end{equation}
    \end{defn}

    \begin{thm}
        If $f:D\subseteq \R^n\rightarrow \R$ is differentiable at $\vec{x}_0$, then the directional derivative of $f$ at $\vec{x}_0$ exists in the direction of $\hat{u}$, and is equal to \begin{equation}
            \partial_{\hat{u}}f(\vec{x}_0) = \nabla f(\vec{x}_0) \cdot \hat{u}
        \end{equation}
    \end{thm}


    \begin{thm}
        Notice if $f:D\subseteq \R^n\rightarrow \R$ is differentiable at $\vec{x}_0$, and $\hat{u}$ is a unit vector, we have that \begin{equation}
            \partial_{\hat{u}}f(\vec{x}_0) = \nabla f(\vec{x}_0) \cdot \hat{u} = |\nabla f(\vec{x}_0)|\cos(\theta)
        \end{equation}
        with $\theta$ being the angle between $\nabla f(\vec{x}_0)$ and $\hat{u}$. As a result: \begin{enumerate}
            \item The largest value of $\partial_{\hat{u}}f(\vec{x}_0)$ is equal to $|\nabla f(\vec{x}_0)|$, and occurs when $\hat{u}$ is in the same direction as the gradient
            \item The smallest value of $\partial_{\hat{u}}f(\vec{x}_0)$ is equal to $-|\nabla f(\vec{x}_0)$ when $\hat{u}$ is in the same direction as $-\nabla f(\vec{x}_0)$
            \item When $\hat{u}$ is perpendicular to $\nabla f(\vec{x}_0)$, the directional derivative is zero.
        \end{enumerate}
    \end{thm}

    \begin{thm}[Chain Rule V1]
        Let $f:D_f\subseteq \R^n\rightarrow \R$ and $g:D_g\subseteq \R\rightarrow \R$ such that $g(t) = f(\vec{x}(t))$. Then \begin{equation}
            \frac{dg}{dt} = \nabla f(\vec{x}) \cdot \frac{d}{dt}\langle x_1,...,x_n\rangle = \sum_{i=1}^n \partial_i f\frac{dx_i}{dt}
        \end{equation}
    \end{thm}

    \begin{thm}[Chain Rule V2]
        Let $f:D_f\subseteq \R^n\rightarrow \R$ and $g:D_g\subseteq \R^m\rightarrow \R$ such that $g(\vec{x}) = f(y_1(\vec{x}),...,y_n(\vec{x}))$. Then \begin{equation}
        \partial_i g(\vec{x}) = \nabla f(y_1(\vec{x}),...,y_n(\vec{x})) \cdot \frac{\partial}{\partial x_i}\vec{y}
        \end{equation}
        for $\vec{y} = \langle y_1,...,y_n\rangle$.
    \end{thm}


    \begin{thm}[Clairout's Theorem]
        Suppose $f:\R^n\rightarrow \R$ has continous first and second partials on an open ball $B_r$. Then $f_{ij}(\vec{x}) = f_{ji}(\vec{x})$ for all $\vec{x} \in D$.
    \end{thm}


    \subsection{Implicit Differentiation}

    \begin{thm}[Implicit Function Theorem (Two variables)]
        Consider $F(x,y) = 0$. Let $(x_0,y_0) \in \R^2$ such that $F(x_0,y_0) = 0$, and suppose $F$'s first partials are continuous in a neighborhood of $(x_0,y_0)$. Then \begin{enumerate}
            \item If $F_y(x_0,y_0) \neq 0$, then $F(x,y) = 0$ uniquely defines $y$ as a continuously differentiable function of $x$ in a neighborhood of $x_0$, and we have that \begin{equation}
                    \frac{dy}{dx} = -\frac{\partial_x F(x,y)}{\partial_y F(x,y)}
                \end{equation}
            \item Similarly for $F_x(x_0,y_0) \neq 0$.
        \end{enumerate}
    \end{thm}


    \begin{thm}[Implicit Function Theorem (n variables)]
        Consider $F(\vec{x}) = 0 (\star)$, $\vec{x} = (x_1,...,x_n)$. Let $\vec{a}$ satisfy $F(\vec{a}) = 0$, and suppose $F(\vec{x})$ has continuous first partial derivatives at and in a neighborhood of $\vec{a}$. Let $\beta$ be one of the variables $\{x_1,...,x_n\}$, and let $\vec{\alpha}$ be the rest. If $\partial_{\beta}F(\vec{a}) \neq 0$, then the equation $(\star)$ uniquely defines the variable $\beta$ as a continuously differentiable function of $\vec{\alpha}$, and for $x_j \neq \beta$, we have \begin{equation}
            \partial_{x_j}\beta(\vec{\alpha}) = -\frac{\partial_{x_j}F(\vec{a})}{\partial_{\beta}F(\vec{a})}
        \end{equation}
    \end{thm}


    \begin{namthm}[Implicit Function Theorem (General)]
        Consider a system of $n$ equations in $n+m$ variables \begin{equation}
            \left\{\begin{array}{l} F_{(1)}(x_1,...,x_m,y_1,...,y_n) = 0 \\ \vdots \\ F_{(n)}(x_1,...,x_m,y_1,...,y_n) = 0 \end{array}\right.
        \end{equation}
        and a point $P_0$ which satisfies the system. Suppose each $F_{(i)}$ is differentiable near $P_0$, so they have continuous first partial derivatives. Finally, suppose \begin{equation}
            \frac{\partial(F_{(1)},...,F_{(n)})}{\partial(y_1,...,y_n)}\Big\rvert_{P_0} \neq 0
        \end{equation}
        Then the system defines $y_1,...,y_n$ uniquely as continuously differentiable functions of $x_1,...,x_m$ in some neighborhood of $P_0$. Moreover, \begin{equation}
            \partial_{x_j}y_i = -\frac{\frac{\partial(F_{(1)},...,F_{(n)})}{\partial(y_1,...,x_j,...,y_n)}}{\frac{\partial(F_{(1)},...,F_{(n)})}{\partial(y_1,...,y_i,...,y_n)}}
        \end{equation}
        This formula is a consequence of Cramer's Rule applied to the n linear equations in $n$ unknowns which is the system differentiated with respect to $x_j$.
    \end{namthm}



    \subsection{Differentials}


    \begin{defn}
        Let $f:D\subseteq \R^n\rightarrow \R$ be a function defined at and around a point $\vec{a}$. Given $\Delta \vec{x}$, of small magnitude, \begin{equation}
            \Delta f_{\vec{a}}(\Delta \vec{x}) = f(\vec{a}+\Delta \vec{x}) - f(\vec{a})
        \end{equation}
        represents the change in the value of the function associated with the change $\Delta \vec{x}$ in $\vec{x}$ at $\vec{a}$. Then, we approximate this change with the \Emph{differential} at $\vec{a}$ defined by \begin{equation}
            df_{\vec{a}}(\Delta \vec{x}) = \nabla f(\vec{a}) \cdot \Delta \vec{x}
        \end{equation}
        If $\Delta \vec{x}$ is sufficiently small, these changes are approximately equal.
    \end{defn}


    
    \subsection{Taylor Polynomials}
    
    \begin{namthm}[Taylor's Theorem (One Variable)]
        Let $f(x)$ be a function wit $n+1$ continuous derivatives in the open interval $(a,b)$. Let \begin{equation}
            T_n(x) := \sum\limits_{i=0}^n\frac{f^{(i)}(c)(x-c)^i}{i!}
        \end{equation}
        be the \Emph{degree n Taylor polynomial} of $f(x)$ centered at $x = c \in (a,b)$. Then, for any $x \in (a,b)$, there exists a number $\theta$ between $c$ and $x$ such that \begin{equation}
            f(x) = T_n(x) + \frac{f^{(n+1)}(\theta)}{(n+1)!}(x-c)^{n+1}
        \end{equation}
    \end{namthm}
    
    \begin{defn}[Two Variable Taylor Polynomial]
        Let $f(x,y)$ be a smooth function (continuous partial derivatives up to whatever degree needed) in a open set $D \subset \R^2$. The \Emph{degree $n$ Taylor polynomial} of $f(x,y)$ at a point $(a,b) \in D$, is the polynomial $T_n(x,y)$ of degree $n$ that equals $f(x,y)$ and its first $n$ partial derivatives at $(a,b)$. It can be written as \begin{equation}
            T_n(x,y) := \sum\limits_{i=0}^n\frac{\left[(x-a)\partial_x + (y-b)\partial_y\right]^{(i)}f(a,b)}{i!}
        \end{equation}
        where $\left[(x-a)\partial_x + (y-b)\partial_y\right]^{(i)}$ is to be expanded as an algebraic expression and the products of $\partial_x$ and $\partial_y$ correspond to composition of operators.
    \end{defn}
    
    \begin{namthm}[Taylor's Theorem (Two variables)]
        Let $f(x,y)$ be a function with continuous partial derivatives up to $(n+1)$ in some neighborhood $D$ of $(a,b) \in \R^2$. Let $T_n(x,y)$ be the degree $n$ Taylor polynomial of $f(x,y)$ at $(a,b)$. Then for any $(x,y) \in D$, there exists $(\alpha,\beta) \in D$ such that \begin{equation}
            f(x,y) = T_n(x,y) + \frac{\left[(x-a)\partial_x + (y-b)\partial_y\right]^{(n+1)}f(\alpha,\beta)}{(n+1)!}
        \end{equation}
        This is called the \Emph{Taylor formula/expansion of order $n$} of $f$ at $(a,b)$. \begin{equation}
            R_n(x,y) := \frac{\left[(x-a)\partial_x + (y-b)\partial_y\right]^{(n+1)}f(\alpha,\beta)}{(n+1)!} = f(x,y) - T_n(x,y)
        \end{equation}
        is called the \Emph{remainder} of the expansion.
    \end{namthm}
    
    \begin{rmk}
        If all partial derivatives of order $(n+1)$ are bounded by some constant $M > 0$, then \begin{equation}
            |f(x,y) - T_n(x,y)| \leq \frac{M}{(n+1)!}\left[\sum\limits_{j=0}^{n+1}\begin{pmatrix} n + 1 \\ j \end{pmatrix}|x-a|^{n+1-j}|y-b|^j\right]
        \end{equation}
    \end{rmk}
    
    
    \begin{namthm}[Taylor's Theorem (General)]
        Let $f:D\subset\R^n \rightarrow \R$ be a function with continuous partial derivatives of order up to $m+1$ in a neighborhood $D$ of $\vec{a} \in \R^n$. Then for all $\vec{x} \in D$ there exists $\vec{theta} \in D$ such that \begin{equation}
            f(\vec{x}) = T_m(\vec{x}) + R_m(\vec{x},\vec{\theta})
        \end{equation}
        where \begin{equation}
            T_m(\vec{x}) := \sum\limits_{k=0}^m\frac{\left[(\vec{x} - \vec{a})\cdot \nabla\right]^{(k)}f(\vec{a})}{k!}
        \end{equation}
        is the degree $m$ Taylor polynomial of $f$ at $\vec{a}$, and \begin{equation}
            R_m(\vec{x}, \vec{\theta}) := \frac{\left[(\vec{x} - \vec{a})\cdot \nabla\right]^{(m+1)}f(\vec{\theta})}{(m+1)!}
        \end{equation}
        is the remainder. If all partial derivatives of $f$ are continuous and there exists $r \in \R^{+}$ such that whenever $||\vec{x} - \vec{a}|| < r$ we have for all $t \in [0,1]$ \begin{equation}
            \lim_{m\rightarrow \infty} R_m(\vec{x}, \vec{a}+t(\vec{x} - \vec{a})) = 0
        \end{equation}
        Then we can represent $f(\vec{x})$ as the \Emph{Taylor series} \begin{equation}
            f(\vec{x}) = \sum\limits_{n = 0}^{\infty}\frac{\left[(\vec{x} - \vec{a})\cdot \nabla\right]^{(n)}f(\vec{a})}{n!}
        \end{equation}
    \end{namthm}
    
    \subsection{Local Extrema of Multivariate Functions}
    
    \begin{defn}
        Let $f:\R^n\rightarrow \R$ be a function of $n$ variables defined in a neighborhood of $\vec{x}_0 \in \R^n$: \begin{enumerate}
            \item We say $f$ has a \Emph{local maximum} at $\vec{x}_0$ if there exists a neighborhood $D$ of $\vec{x}_0$ for which $f$ is defined and \begin{equation}
                f(\vec{x}) \leq f(\vec{x}_0), \forall \vec{x} \in D
            \end{equation}
            \item We say $f$ has a \Emph{local minimum} at $\vec{x}_0$ if there exists a neighborhood $D$ of $\vec{x}_0$ for which $f$ is defined and \begin{equation}
                f(\vec{x}) \geq f(\vec{x}_0), \forall \vec{x} \in D
            \end{equation}
            \item[$\drsh$] If $f$ has a local maximum or minimum at $\vec{x}_0$, we say $f$ has a \Emph{local extremum} at $\vec{x}_0$.
        \end{enumerate}
    \end{defn}
    
    \begin{thm}[Fermat]
        If $f:\R^n\rightarrow \R$ has a local extremum at $\vec{x}_0$, then one of the following must hold:\begin{enumerate}
            \item $\nabla f(\vec{x}_0) = \vec{0}$ when all first partials of $f$ exist at $\vec{x}_0$
            \item At least one of the first partials of $f$ are not defined at $\vec{x}_0$
        \end{enumerate}
    \end{thm}
    
    \begin{defn}
        Suppose $f:\R^n\rightarrow \R$ is defined in a neighborhood of $\vec{x}_0$. If one of the conditions of Fermat's Theorem is satisfied by $\vec{x}_0$, we say $\vec{x}_0$ is a \Emph{critical point} of $f$.
    \end{defn}
    
    \begin{rmk}
        To determine if $f$ has a local max or min at a critical point $\vec{x}_0$, study the sign of \begin{equation}
            f(\vec{x}_0 + \vec{h}) - f(\vec{x}_0)
        \end{equation}
        for small $|\vec{h}|$. If it is always positive, $f$ has a local minimum, if it is always negative $f$ has a local maximum, and if it changes sign, $f$ does not have a local extremum and in this case we say $f$ has a \Emph{saddle point} at $\vec{x}_0$.
    \end{rmk}
    
    \begin{namthm}[Second Derivative Test]
        Suppose $f(x,y)$ has continuous second partial derivatives in a neighborhood of a critical point $(x_0,y_0)$. Define the \Emph{Hessian} matrix of $f$ at $(x_0,y_0)$ to be \begin{equation}
            H_f(x_0,y_0) := \begin{bmatrix} f_{xx}(x_0,y_0) & f_{xy}(x_0,y_0) \\ f_{yx}(x_0,y_0) & f_{yy}(x_0,y_0) \end{bmatrix}
        \end{equation}
        Let $\delta_1 = f_{xx}(x_0,y_0)$ and $\delta_2 = \det(H_f(x_0,y_0))$. Then \begin{enumerate}
            \item If $\delta_1 > 0$ and $\delta_2 > 0$, then $f$ has a local minimum at $(x_0,y_0)$
            \item If $\delta_1 < 0$ and $\delta_2 > 0$, then $f$ has a local maximum at $(x_0,y_0)$
            \item If $\delta_2 \neq 0$ but neither case 1 nor case 2 hold, then $f$ has a saddle point at $(x_0,y_0)$.
            \item If $\delta_2 = 0$ the test is inconclusive.
        \end{enumerate}
    \end{namthm}
    
    \begin{namthm}[Second Derivative Test (general)]
        Suppose $f:D \subset \R^n \rightarrow \R$ has continuous second partial derivatives in a neighborhood of a critical point $\vec{x}_0 \in D$. Define the \Emph{Hessian} matrix of $f$ at $\vec{x}_0$ to be \begin{equation}
            H_f(\vec{x}_0) := \begin{bmatrix} f_{11}(\vec{x}_0) & f_{12}(\vec{x}_0) & \hdots & f_{1n}(\vec{x}_0) \\ f_{21}(\vec{x}_0) & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & \vdots \\ f_{n1}(\vec{x}_0) & \hdots & \hdots & f_{nn}(\vec{x}_0) \end{bmatrix}
        \end{equation}
        Denote the the $k$th principal minor of $H_f(\vec{x}_0)$ by \begin{equation}
            \delta_k := \begin{vmatrix} f_{11}(\vec{x}_0) & f_{12}(\vec{x}_0) & \hdots & f_{1k}(\vec{x}_0) \\ f_{21}(\vec{x}_0) & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & \vdots \\ f_{k1}(\vec{x}_0) & \hdots & \hdots & f_{kk}(\vec{x}_0) \end{vmatrix}
        \end{equation}. Then \begin{enumerate}
            \item If for all $i \in \{1,2,...,n\}$, $\delta_i > 0$, then $f$ has a local minimum at $\vec{x}_0$
            \item If for all $i \in \{1,2,...,n\}$, $\delta_{2i-1} <  0$ and $\delta_{2i} > 0$, then $f$ has a local maximum at $\vec{x}_0$
            \item If $\delta_n = \det(H_f(\vec{x}_0) \neq 0$ but neither case 1 nor case 2 hold, then $f$ has a saddle point at $\vec{x}_0$.
            \item If $\delta_n = \det(H_f(\vec{x}_0) = 0$ the test is inconclusive.
        \end{enumerate}
    \end{namthm}
    
    \subsection{Vector Fields}
    
    \begin{defn}
        A \Emph{vector field} is a vector function $\vec{F}:D\subset \R^n \rightarrow \R^n$. In the case of three variables we write \begin{equation}
            \vec{F}(x,y,z) = \langle P(x,y,z), Q(x,y,z), R(x,y,z)\rangle
        \end{equation}
    \end{defn}
    
    \begin{rmk}
        A vector field $\vec{F}:D\subset \R^n \rightarrow \R^n$ is said to be of class $C^k$ for $k \in \Z^{+}$ in $D$ if the first $k$ partial derivatives of the component functions of $\vec{F}$ are continuous in $D$.
    \end{rmk}
    
    \begin{defn}[Conservative Fields]
        A vector field $\vec{F}:D\subset \R^n \rightarrow \R^n$ is called \Emph{conservative} in a region $E \subseteq D$ if there exists a scalar function $f:D_f \subset \R^n \rightarrow \R$ such that \begin{equation}
            \vec{F}(\vec{x}) = \nabla f(\vec{x}), \forall \vec{x} \in E
        \end{equation}
        where $f$ is called a \Emph{potential function} of the vector field $\vec{F}$.
    \end{defn}
    
    \begin{defn}
        Let $\vec{F}:D\subset \R^n \rightarrow \R^n$ be a differentiable vector field. The \Emph{divergence} of $\vec{F}$ is the scalar field \begin{equation}
            \nabla\cdot \vec{F} = \sum\limits_{i=1}^n\partial_iF_i
        \end{equation}
        where $F_i$ are the component function of $\vec{F}$.
    \end{defn}
    
    \begin{defn}
        Let $\vec{F}:D\subset \R^3 \rightarrow \R^3$ be a differentiable vector field. The \Emph{curl} of $\vec{F}$ is the vector field \begin{equation}
            \nabla\times \vec{F} = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ \partial_x & \partial_y & \partial_z \\ P & Q & R \end{vmatrix}
        \end{equation}
        where $F_i$ are the component function of $\vec{F}$.
    \end{defn}
    
    \begin{prop}[Properties of Divergence]
        If $\vec{F}:D\subset \R^n \rightarrow \R^n$ and $\vec{G}:D\subset \R^n \rightarrow \R^n$ vector fields and $f:D\subset \R^n \rightarrow \R$ is a scalar field, and $C_1,C_2 \in \R$, then \begin{enumerate}
            \item (Linearity) $\nabla \cdot (C_1\vec{F} + C_2\vec{G}) = C_1\nabla \cdot \vec{F} + C_2\nabla \cdot \vec{F}$
            \item (Product rule) $\nabla \cdot (f\vec{F}) = \nabla f \cdot \vec{F} + f\nabla\cdot \vec{F}$
            \item (Laplacian) $\Delta f = \nabla \cdot \nabla f = \partial_{xx}^2f + \partial_{yy}^2f + \partial_{zz}^2f$
        \end{enumerate}
    \end{prop}
    
    \begin{prop}[Properties of Divergence]
        If $\vec{F}:D\subset \R^3 \rightarrow \R^3$ and $\vec{G}:D\subset \R^3 \rightarrow \R^3$ vector fields and $f:D\subset \R^3 \rightarrow \R$ is a scalar field, that are all defined and differentiable in $D$. Let $C_1,C_2 \in \R$, then \begin{enumerate}
            \item (Linearity) $\nabla \times (C_1\vec{F} + C_2\vec{G}) = C_1\nabla \times \vec{F} + C_2\nabla \times \vec{F}$
            \item (Product rule) $\nabla \times (f\vec{F}) = \nabla f \times \vec{F} + f(\nabla\times \vec{F})$
            \item (Conservative Property) $\nabla \times (\nabla f) = 0$
            \item $\nabla \times(\nabla \times \vec{F}) = \nabla(\nabla\cdot \vec{F}) - (\nabla \cdot \nabla)\vec{F}$ (provided $\vec{F}$ has continuous second partial derivatives, and where $(\nabla \cdot \nabla)\vec{F} = (\Delta P, \Delta Q, \Delta R)$)
        \end{enumerate}
    \end{prop}
    
    \begin{defn}
        Let $E \subseteq \R^n$ \begin{enumerate}
            \item We say that $E$ is \Emph{path connected} if for any two points $A$ and $B$ in $E$ if there exists a continuous function $f:[0,1] \rightarrow E$ such that $f(0) = A$ and $f(1) = B$.
            \item We say $E$ is \Emph{simply connected} if $E$ is path connected and any simple closed curve $\mathcal{C}$ that completely lies in $E$ can be continuously deformed into a single point without leaving $E$.
        \end{enumerate}
    \end{defn}
    
    
    \begin{thm}
        Let $\vec{F}$ be a class $C^1$ in an open region $E \subseteq \R^3$. If $\vec{F}$ is conservative in $E$, then $\nabla \times \vec{F} = \vec{0}$ at every point of $E$. Moreover, if $\nabla \times \vec{F} = \vec{0}$ at every point of $E$ and $E$ is simply connected, then $\vec{F}$ is conservative in $E$.
    \end{thm}
    
    
    \subsection{Line Integrals}
    
    \begin{defn}
        Let $\mathcal{C}$ be a bounded continuous parametric curve in $\R^n$. Recall that $\mathcal{C}$ is a \Emph{smooth curve} if it has a parameterization $\vec{r}:I\subset \R\rightarrow \R^n$ such that $\frac{d\vec{r}}{dt}$ is continuous and nonzero in $I$. We say $\mathcal{C}$ is a \Emph{smooth arc} if it is a smooth curve with finite parameter interval $I = [a,b]$.  
    \end{defn}
    
    \begin{defn}
        Given a smooth curve $\mathcal{C}$ with parameterization $\vec{r}:[a,b]\rightarrow \R^n$ we have \begin{equation}
            l_{\mathcal{C}} = \int\limits_{\mathcal{C}}ds = \int\limits_a^b\left|\frac{d\vec{r}}{dt}\right|dt
        \end{equation}
        In general, for a scalar field $f:\R^n \rightarrow \R$, we define the \Emph{line integral along $\mathcal{C}$} to be \begin{equation}
            \int\limits_{\mathcal{C}}f(\vec{x})ds = \int\limits_a^bf(\vec{r}(t))\left|\frac{d\vec{r}}{dt}\right|dt
        \end{equation}
        This definition is parameterization independent.
    \end{defn}
    
    \begin{defn}
        If $\vec{F}$ is a continuous vector field and $\mathcal{C}$ is an oriented smooth curve, then the \Emph{line integral of the tangential component of $\vec{F}$ along $\mathcal{C}$} is \begin{equation}
            \int_{\mathcal{C}}\vec{F}\cdot d\vec{r} = \int_{\mathcal{C}}\vec{F}\cdot \hat{T}ds
        \end{equation}
    \end{defn}
    
    \begin{defn}
        If $\mathcal{C}$ is a closed curve we also call this line integral the \Emph{circulation} of $\vec{F}$ around $\mathcal{C}$, and we denote it by \begin{equation}
            \oint_{\mathcal{C}}\vec{F} \cdot d\vec{r}
        \end{equation}
    \end{defn}
    
    \begin{rmk}
        A line integral over a piecewise smooth path is the sum of the line integrals over the individual smooth arcs \begin{equation}
            \int\limits_{\bigcup_{i=1}^n\mathcal{C}_i}ds = \sum\limits_{i=1}^n\int\limits_{\mathcal{C}_i}ds
        \end{equation}
    \end{rmk}
    
    \subsection{Line Integral Theorems}
    
    \begin{namthm}[Fundamental Theorem of Line Integrals]
        Let $\mathcal{C}$ be a piecewise smooth parametric curve with initial point $A$ and terminal point $B$. If $f:D \subseteq \R^n \rightarrow \R$ is a scalar function with continuous first partial derivatives in an open region containing $\mathcal{C}$, then \begin{equation}
            \int_{\mathcal{C}}\nabla f\cdot d\vec{r} = f(B) - f(A)
        \end{equation} 
    \end{namthm}
    
    \begin{cor}
        If $\mathcal{C}$ is a piecewise smooth closed curve contained in a region $D$ where the vector field $\vec{F}:D\subseteq \R^n\rightarrow \R^n$ is conservative, then \begin{equation}
            \oint_{\mathcal{C}}\vec{F}\cdot d\vec{r}
        \end{equation}
    \end{cor}
    
    \begin{defn}
        A vector field $\vec{F}$ is said to be \Emph{path-independent} in a region $\Omega$ if for every pair of points $A$ and $B$ in $\Omega$ and every pair of piecewise smooth curves $\mathcal{C}_1$ and $\mathcal{C}_2$ with initial point $A$ and terminal point $B$, we have \begin{equation}
            \int_{\mathcal{C}_1}\vec{F}\cdot d\vec{r} = \int_{\mathcal{C}_2}\vec{F}\cdot d\vec{r}
        \end{equation}
    \end{defn}
    
    
    \begin{thm}
        Let $D$ be an open connected domain in $\R^n$ and let $\vec{F}$ be a smooth vector field defined on $D$. Then the following properties are equivalent:\begin{enumerate}
            \item $\vec{F}$ is conservative in $D$ 
            \item $\oint_{\mathcal{C}} \vec{F}\cdot d\vec{r} = 0$ for every piecewise smooth closed curve $\mathcal{C} \subset D$ 
            \item $\vec{F}$ is path independent in $D$.
        \end{enumerate}
    \end{thm}
    
    \begin{namthm}[Green's Theorem]
        Let $R$ be a closed region in the $xy$-plane whose boundary $\partial R$ consists of a finite number of piecewise smooth simple closed curves that are positively oriented with respect to $R$. If $\vec{F}$ is a smooth vector field on $R$, then \begin{equation}
            \oint_{\partial R}\vec{F}\cdot d\vec{r} = \int\int_{R}(\nabla \times \vec{F})\cdot \hat{k}dA
        \end{equation}
        In particular $\hat{k}$ is the unit normal field specifying the orientation of $R$, and $\partial R$ is oriented such that its principal normal field $\vec{N}$ points away from the region and \begin{equation}
            \hat{N} = \hat{T} \times \hat{k}
        \end{equation}
    \end{namthm}
    
    \begin{rmk}
        You don't need anything past this point yet.
    \end{rmk}
    
    \begin{namthm}[Plane Divergence Theorem]
        Let $R$ be a closed region in the $xy$-plane whose boundary $\partial R$ consists of a finite number of piecewise smooth simple closed curves. Let $\vec{N}$ denote the unit outward (from $R$) normal field on $\mathcal{C}$. If $\vec{F}$ is a smooth vector field on $R$, then \begin{equation}
            \oint_{\partial R}\vec{F}\cdot \hat{N} ds = \int\int_{R}\nabla \cdot \vec{F}dA
        \end{equation}
    \end{namthm}
    
    \begin{namthm}[Stoke's Theorem]
        Let $\mathcal{S}$ be a piecewise smooth oriented surface in $3$-space having a unit normal field $\hat{N}$ and boundary $\mathcal{C}$ consisting of a finite number of piecewise smooth closed curves with orientation inherited from $\mathcal{S}$. If $\vec{F}$ is a smooth vector field defined on an open set containing $\mathcal{S}$, then \begin{equation}
            \oint_{\mathcal{C}}\vec{F}\cdot d\vec{r} = \int\int_{\mathcal{S}}(\nabla \times \vec{F})\cdot \hat{N}dS
        \end{equation}
    \end{namthm}
    
    \subsection{Surface Integrals}
    
    \begin{defn}
        A \Emph{parametric surface} in $3$-space is a continuous function $\vec{r}: R \subseteq \R^2 \rightarrow \R^3$ for some rectangle $R$ \begin{equation}
            R = \{(u,v) \in \R^2: a \leq u \leq b, c \leq v \leq d\}
        \end{equation}
        in the $uv$-plane having values in $3$-space: \begin{equation}
            \vec{r}(u,v) = \langle x(u,v), y(u,v), z(u,v)\rangle, (u,v) \in R
        \end{equation}
    \end{defn}
    
    \begin{rmk}
        If $\vec{r}$ is one-to-one the surface does not intersect itself. In this case $\vec{r}$ maps the boundary of $R$ onto a curve in $3$-space called the \Emph{boundary of the parametric surface}. A surface with no boundary is called a \Emph{closed surface}.
    \end{rmk}
    
    \begin{defn}
        If a finite number of parametric surfaces are joined pairwise along their boundaries one obtains a \Emph{composite surface}, or just a surface thinking geometrically.
    \end{defn}
    
    \begin{defn}
        A set $S \subseteq\R^3$ is a \Emph{smooth surface} if any point $P \in S$ has a neighborhood $N$ that is the domain of a smooth function $g:N \rightarrow \R$ satisfying \begin{enumerate}
            \item $N \cap S = \{Q \in N:g(Q) = 0\}$
            \item $\nabla g(Q) \neq \vec{0}$ if $Q \in N \cap S$
        \end{enumerate}
    \end{defn}
    
    \begin{rmk}
        This means the surface has a unique tangent plane at any non-boundary point $P$.
    \end{rmk}
    
    \begin{defn}
        If $\vec{r}:D\subseteq 
        \R^2 \rightarrow \R^3$ is a parameterization of a smooth surface $S$, the \Emph{normal vector} to $S$ at $\vec{r}(u,v)$ is \begin{equation}
            \vec{n} = \frac{\partial \vec{r}}{\partial u}\times \frac{\partial \vec{r}}{\partial v}
        \end{equation}
    \end{defn}
    
    \begin{defn}
        The \Emph{area element} at $\vec{r}(u,v)$ on $S$ is given by \begin{equation}
            dS = \left|\frac{\partial \vec{r}}{\partial u}\times \frac{\partial \vec{r}}{\partial v}\right|dudv
        \end{equation}
        Then if $f(\vec{r})$ is continuous on $\mathcal{S}$ and the domain of $\vec{r}$ is $D$ in the $uv$-plane \begin{equation}
            \int\int_{\mathcal{S}}fdS = \int\int_{D}f(\vec{r}(u,v)) \left|\frac{\partial \vec{r}}{\partial u}\times \frac{\partial \vec{r}}{\partial v}\right|dudv
        \end{equation}
    \end{defn}
    
    \begin{defn}
        A smooth surface $\mathcal{S}$ in $3$-space is said to be \Emph{orientable} if there exists a unit vector field $\hat{N}$ defined on $\mathcal{S}$ that varies \Emph{continuously} over $\mathcal{S}$, and is everywhere normal to $\mathcal{S}$. Any such vector field $\hat{N}$ induces an orientation on $\mathcal{S}$. The side of $\mathcal{S}$ out of which $\hat{N}$ points is the \Emph{positive side}, and the other side is the \Emph{negative side}. An \Emph{oriented surface} is a smooth surface with a particular choice of orienting unit normal vector field $\hat{N}$.
    \end{defn}
    
    \begin{rmk}
        An oriented surface $\mathcal{S}$ \Emph{induces an orientation} on any of its boundary curves $\mathcal{C}$; if we stand on the positive side of the surface $\mathcal{S}$ and walk around $\mathcal{C}$ in the direction of its orientation, then $\mathcal{S}$ will be on our left side.
    \end{rmk}
    
    
    \begin{defn}
        Given any continuous vector field $\vec{F}$, the \Emph{flux} of $\vec{F}$ across the orientable surface $\mathcal{S}$ is the surface integral of the normal component of $\vec{F}$ over $\mathcal{S}$ \begin{equation}
            \int\int_{\mathcal{S}}\vec{F}\cdot \hat{N}dS = \int\int_{\mathcal{S}}\vec{F}\cdot d\vec{S}
        \end{equation}
        and when the surface is closed we write \begin{equation}
            \oiint_{\mathcal{S}}\vec{F}\cdot \hat{N}dS = \oiint_{\mathcal{S}}\vec{F}\cdot d\vec{S}
        \end{equation}
    \end{defn}
    
    
    \begin{rmk}
        If $\vec{r}(u,v)$ parametrizes $\mathcal{S}$ with domain $D$, we have normal \begin{equation}
            \vec{n} = \frac{\partial \vec{r}}{\partial u}\times \frac{\partial \vec{r}}{\partial v}
        \end{equation}
        and $dS = |\vec{n}|dudv$. Hence \begin{equation}
            d\vec{S} = \hat{N}dS = \pm\frac{\vec{n}}{|\vec{n}|}|\vec{n}|dudv = \pm\vec{n}dudv
        \end{equation}
        where the sign reflects the orientation of the surface and parameterization.
    \end{rmk}
    
    \begin{namthm}[Divergence Theorem]
        Let $D$ be a three dimensional domain bounded by piecewise smooth closed surfaces. Suppose its boundary $\mathcal{S}$ is an oriented closed surface with unit normal field $\hat{N}$ pointing out of $D$. If $\vec{F}$ is a smooth vector field defined on $D$, then \begin{equation}
            \oiint_{\mathcal{S}}\vec{F}\cdot \hat{N}dS = \int\int\int_D\nabla\cdot \vec{F}dV
        \end{equation}
    \end{namthm}
    
\end{appendices}