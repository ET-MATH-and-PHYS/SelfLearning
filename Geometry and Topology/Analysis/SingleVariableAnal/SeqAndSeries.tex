%%%%%%%%%% Sequences And Series %%%%%%%%%%
\chapter{Sequences and Series}

\section{Approximation by Polynomial Functions}

\begin{defn}
    Given a function $f$ that is $n$ times differentiable in a neighborhood of a point $a$, let \begin{equation*}
        a_k = \frac{f^{(k)}(a)}{k!},\;\;0\leq k \leq n,
    \end{equation*}
    and define \begin{equation*}
        P_{n,a}(x) = a_0 + a_1(x-a) + \hdots + a_x(x-a)^n
    \end{equation*}
    The polynomial $P_{n,a}$ is called the \Emph{Taylor polynomial of degree $n$ for $f$ at $a$}. 
\end{defn}

\begin{rmk}
    The Taylor polynomial has been defined so that \begin{equation*}
        P_{n,a}^{(k)}(a) = f^{(k)}(a)\;\;\text{ for }\;0\leq k \leq n
    \end{equation*}
    in fact, it is evidently the only polynomial of degree $\leq n$ with this property.
\end{rmk}

\begin{eg}
    Consider the $\sin$ function. We have \begin{align*}
        \sin(0) &= 0 \\
        \sin'(0) &= \cos 0 = 1 \\
        \sin''(0) &= -\sin 0 = 0 \\
        \sin'''(0) &= -\cos 0 = -1 \\
        \sin^{(4)}(0) &= \sin 0 = 0
    \end{align*}
    From this point on, the derivatives repeat modulo $4$. The coefficients become \begin{equation*}
        a_k = \frac{\sin^{(k)}(0)}{k!} = \left\{\begin{array}{lc} 0 & \text{if } \exists l \in \N; k = 2l \\
            \frac{(-1)^{l}}{(2l+1)!} & \text{if } \exists l \in \N; k = 2l+1
        \end{array}\right.
    \end{equation*}
    Therefore, the Taylor polynomial $P_{2n+1,0}$ of degree $2n+1$ for $\sin$ at $0$ is \begin{equation*}
        P_{2n+1,0}(x) = x-\frac{x^3}{3!}+\frac{x^5}{5!} - \frac{x^7}{7!}+\hdots + (-1)^n\frac{x^{2n+1}}{(2n+1)!}
    \end{equation*}
    Via a similar derivation we find that the Taylor polynomial $P_{2n,0}$ of degree $2n$ for $\cos$ at $0$ is \begin{equation*}
        P_{2n,0} = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \hdots + (-1)^n\frac{x^{2n}}{(2n)!}
    \end{equation*}
\end{eg}


\begin{eg}
    Note that for all $k \geq 0$, $\exp^{(k)}(0) = \exp(0) = 1$, so the Taylor polynomial of degree $n$ for $\exp$ at $0$ is \begin{equation*}
        P_{n,0}(x) = 1 + \frac{x}{1!} + \frac{x^2}{2!} + \frac{x^3}{3!} + \hdots + \frac{x^n}{n!}
    \end{equation*}
    For $\log$, observe that \begin{align*}
        \log'(x) &= \frac{1}{x}, &\log'(1) = 1 \\
        \log''(x) &= -\frac{1}{x^2}, &\log''(1) = -1 \\
        \log'''(x) &= \frac{2}{x^3}, &\log'''(1) = 2
    \end{align*}
    in general \begin{equation*}
        \log^{(k)}(x) = \frac{(-1)^{k-1}(k-1)!}{x^k}, \;\;\log^{(k)}(1) = (-1)^{k-1}(k-1)!
    \end{equation*}
    for $k \geq 1$, and $\log(1) =0$. Therefore, the Taylor polynomial of degree $n$ for $\log$ at $1$ is \begin{equation*}
        P_{n,1}(x) = (x-1) -\frac{(x-1)^2}{2} +\frac{(x-1)^3}{3} - \hdots + \frac{(-1)^{n-1}(x-1)^n}{n}
    \end{equation*}
    If we consider the function $f(x) = \log(1+x)$, then the Taylor polynomial of degree $n$ of $f$ at $0$ is \begin{equation*}
        P_{n,0}(x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \hdots + \frac{(-1)^{n-1}x^n}{n}
    \end{equation*}
\end{eg}


\begin{thm}\label{thm:smoldiff}
    Suppose that $f$ is a function and $a \in \R$ such that \begin{equation*}
        f'(a),...,f^{(n)}(a)
    \end{equation*}
    all exist. Let \begin{equation*}
        a_k = \frac{f^{(k)}}{k!}, 0\leq k \leq n
    \end{equation*}
    and define \begin{equation*}
        P_{n,a}(x) = a_0 + a_1(x-a) + \hdots + a_n(x-a)^n
    \end{equation*}
    Then \begin{equation*}
        \lim\limits_{x\rightarrow a}\frac{f(x)-P_{n,a}(x)}{(x-a)^n} = 0
    \end{equation*}
\end{thm}
\begin{proof}
    Weiting out $P_{n,a}(x)$ expliticly we obtain \begin{equation*}
        \frac{f(x) - P_{n,a}(x)}{(x-a)^n} = \frac{f(x) - \sum\limits_{i=0}^{n-1}\frac{f^{(i)}(a)}{i!}(x-a)^i}{(x-a)^n} - \frac{f^{(n)}}{n!}
    \end{equation*}
    Let us introduce the new functions \begin{equation*}
        Q(x) = \sum\limits_{i=0}^{n-1}\frac{f^{(i)}(a)}{i!}(x-a)^i\;\text{ and }\; g(x) = (x-a)^n;
    \end{equation*}
    now we must prove that \begin{equation*}
        \lim\limits_{x\rightarrow a}\frac{f(x) - Q(x)}{g(x)} = \frac{f^{(n)}(a)}{n!}
    \end{equation*}
    Note that $Q^{(k)}(a) = f^{(k)}(a)$ for $k \leq n-1$, and $g^{(k)}(x) =n!\frac{(x-a)^{n-k}}{(n-k)!}$. By the continuity of $f^{(k)}$ and $Q^{(k)}$ for $k \leq n-1$, we have that \begin{equation*}
        \lim\limits_{x\rightarrow a}\left[f^{(k)}(x) - Q^{(k)}(x)\right] = f^{(k)}(a) - Q^{(k)}(a) = 0
    \end{equation*}
    and \begin{equation*}
        \lim\limits_{x\rightarrow a}g^{(k)}(x) = 0
    \end{equation*}
    Then applying l'Hopital's Rule $n-1$ times, we obtain \begin{equation*}
        \lim\limits_{x\rightarrow a}\frac{f(x)-Q(x)}{(x-a)^n} = \lim\limits_{x\rightarrow a}\frac{f^{(n-1)}(x) - Q^{(n-1)}(x)}{n!(x-a)}
    \end{equation*}
    But the $n-1$st derivative of $Q$ is constant, and in fact $Q^{(n-1)}(x) = f^{(n-1)}(a)$, so \begin{equation*}
        \lim\limits_{x\rightarrow a}\frac{f(x)-Q(x)}{(x-a)^n} = \lim\limits_{x\rightarrow a}\frac{f^{(n-1)}(x)-f^{(n-1)}(a)}{n!(x-a)} = \frac{f^{(n)}(a)}{n!}
    \end{equation*}
    by definition of $f^{(n)}(a)$, as desired.
\end{proof}


\begin{thm}
    Suppose that \begin{equation*}
        f'(a) = ... = f^{(n-1)}(a) = 0,\;\text{ and }\;f^{(n)}(a) \neq 0
    \end{equation*}
    \begin{enumerate}
        \item If $n$ is even and $f^{(n)}(a) > 0$, then $f$ has a local minimum at $a$.
        \item If $n$ is even and $f^{(n)}(a) < 0$, then $f$ has a local maximum at $a$.
        \item If $n$ is odd, then $f$ has neither a local maximum nor a local minimum at $a$.
    \end{enumerate}
\end{thm}
\begin{proof}
    Let $f$ be as in the hypothesis, and without loss of generality let $f(a) = 0$, as otherwise one can replace $f$ with $f-f(a)$ without affecting the hypothesis. THen, since the first $n-1$ derivatives of $f$ at $a$ are $0$, the Taylor polynomial$ P_{n,a}$ of $f$ is \begin{align*}
        P_{n,a}(x) &= \sum\limits_{i=0}^n\frac{f^{(i)}(a)}{i!}(x-a)^i \\
        &= \frac{f^{(n)}(a)}{n!}(x-a)^n 
    \end{align*}
    Thus, Theorem \ref{thm:smoldiff} states that \begin{equation*}
        0 = \lim\limits_{x\rightarrow a} \frac{f(x) - P_{n,a}(x)}{(x-a)^n} = \lim\limits_{x\rightarrow a} \left[\frac{f(x)}{(x-a)^n} - \frac{f^{(n)}(a)}{n!}\right]
    \end{equation*}
    Consequently, if $x$ is sufficiently close to $a$, then $f(x)/(x-a)^n$ has the same sign as $f^{(n)}(a)/n!$. Suppose now that $n$ is even. In this case $(x-a)^n > 0$ for all $x \neq a$. Since $f(x)/(x-a)^n$ has the same sign as $f^{(n)}(a)/n!$ for $x$ sufficeintly close to $a$, it follows that $f(x)$ itself has the same sign as $f^{(n)}(a)/n!$ for $x$ sufficiently close to $a$. If $f^{(n)}(a) > 0$, this means that \begin{equation*}
        f(x) > 0 = f(a)
    \end{equation*}
    for $x$ close to $a$. Consequently, $f$ has a local minimum at $a$. If $f^{(n)}(a) < 0$, this means that \begin{equation*}
        f(x) < 0 = f(a)
    \end{equation*}
    for $x$ close to $a$, so $f$ has a local minimum at $a$.

    Conversely, suppose that $n$ is odd. Then if $x$ is sufficiently close to $a$ $f(x)/(x-a)^n$ always has the same sign, since it has the same sign as $f^{(n)}(a)/n!$ which is constant. But $(x-a)^n >0$ for $x >a$ and $(x-a)^n < 0$ for $x < a$. Therefore $f(x)$ has different signs for $x > a$ and $x <a$. Hence, $f$ has neither a local maximum nor a local minimum at $a$.
\end{proof}

\begin{defn}
    Two functions $f$ and $g$ are \Emph{equal up to order $n$ at $a$} if \begin{equation*}
        \lim\limits_{x\rightarrow a}\frac{f(x) - g(x)}{(x-a)^n} = 0
    \end{equation*}
\end{defn}


\begin{thm}
    Let $P$ and $Q$ be two polynomials in $(x-a)$, of degree $\leq n$, and suppose that $P$ and $Q$ are equal up to order $n$ at $a$. Then $P = Q$.
\end{thm}
\begin{proof}
    Let $R= P-Q$. Since $R$ is a polynomial of degree $\leq n$, it is only necessariy to prove that if $R(x) = b_0+\hdots +b_n(x-a)^n$ satisfies \begin{equation*}
        \lim\limits_{x\rightarrow a}\frac{R(x)}{(x-a)^n} = 0
    \end{equation*}
    then $R = 0$. Now the hypothesis on $R$ surely implies that \begin{equation*}
        \lim\limits_{x\rightarrow a}\frac{R(x)}{(x-a)^i} = 0\;\;\text{ for } 0\leq i \leq n
    \end{equation*}
    For $i = 0$ this condition reads that $\lim\limits_{x\rightarrow a}R(x) = 0$. On the other hand $\lim\limits_{x\rightarrow a}R(x) = b_0$. Thus $b_0 = 0$. Similarly, we find that \begin{equation*}
        \frac{R(x)}{x-a} = b_1+b_2(x-a)+\hdots + b_n(x-a)^{n-1}
    \end{equation*}
    and \begin{equation*}
        \lim\limits_{x\rightarrow a}\frac{R(x)}{x-a} = b_1
    \end{equation*}
    so $b_1 = 0$. Continuing in this way, by induction we find that \begin{equation*}
        b_0 = b_1 = ... = b_n = 0
    \end{equation*}
    so $R(x) = 0$ and $P = Q$.
\end{proof}


\begin{cor}
    Let $f$ be $n$-times differentiable at $a$, and suppose that $P$ is a polynomial in $(x-a)$ of degree $\leq n$, which equals $f$ up to order $n$ at $a$. Then $P = P_{n,a,f}$ (The Taylor polynomial of $f$ of degree $n$ at $a$).
\end{cor}
\begin{proof}
    Since $P$ and $P_{n,a,f}$ both equal $f$ up to order $n$ at $a$, using the triangle inequality and an epsilon-delta proof it can be shown that $P$ equals $P_{n,a,f}$ up to order $n$ at $a$. Consequently, $P = P_{n,a,f}$ by the preceding Theorem.
\end{proof}

\begin{defn}
    If $f$ is a function for which $P_{n,a}(x)$ exists, we define the \Emph{remainder term} $R_{n,a}(x)$ by \begin{equation*}
        R_{n,a}(x) = f(x) - P_{n,a}(x)
    \end{equation*}
    If $f^{(n+a)}$ is continuous on $[a,x]$, then \begin{equation*}
        R_{n,a}(x) = \int_a^x\frac{f^{(n+1)}(t)}{n!}(x-t)^ndt
    \end{equation*}
\end{defn}

\begin{rmk}
    If $m$ and $M$ are the minimum and maximum of $f^{(n+1)}{n!}$ on $[a,x]$, then $R_{n,a}(x)$ satisfies \begin{equation*}
        m\in_a^x(x-t)^ndt\leq R_{n,a}(x)\leq M\int_a^x(x-t)^ndt
    \end{equation*}
    so we can write \begin{equation*}
        R_{n,a}(x) = \alpha\cdot \frac{(x-a)^{n+1}}{n+1}
    \end{equation*}
    for some number $\alpha$ between $m$ and $M$. Since we have assumed that $f^{(n+1)}$ is continuous, for some $t \in (a,x)$ we can also write \begin{equation*}
        R_{n,a}(x) = \frac{f^{(n+1)}(t)}{n!}\frac{(x-a)^{n+1}}{n+1} = \frac{f^{(n+1)}(t)}{(n+1)!}(x-a)^{n+1}
    \end{equation*}
    This is known as the \Emph{Lagrange form of the remainder}.
\end{rmk}


\begin{lem}
    Suppose that the function $R$ is $(n+1)$-times differentiable on $[a,b]$, and \begin{equation*}
        R^{(k)}(a) = 0\;\;\text{ for } k =0,1,2,...,n
    \end{equation*}
    Then for any $x \in (a,b]$ we have \begin{equation*}
        \frac{R(x)}{(x-a)^{n+1}} = \frac{R^{(n+1)}(t)}{(n+1)!}\;\text{ for some $t$ in } (a,x)
    \end{equation*}
\end{lem}
\begin{proof}
    For $n = 0$, this is simply \ref{thmname:meanval}, and we will proceed for the remaining $n$ by induction on $n$. To do this we use \ref{thmname:caumeanval} to write \begin{equation*}
        \frac{R(x)}{(x-a)^{n+2}} = \frac{R'(z)}{(n+2)(z-a)^{n+1}} = \frac{1}{n+2}\frac{R'(z)}{(z-a)^{n+1}}\;\;\text{ for some $z$ in } (a,x),
    \end{equation*}
    and then apply the induction hypothesis to $R'$ on the interval $[a,z]$ to get \begin{align*}
        \frac{R(x)}{(x-a)^{n+2}} &= \frac{1}{n+2}\frac{(R')^{(n+1)}(t)}{(n+1)!}\;\text{ for some $z$ in } (a,x), \\
        &= \frac{R^{(n+2)}(t)}{(n+2)!}
    \end{align*}
    as desired.
\end{proof}

\begin{namthm}[Taylor's Theorem]\label{thmname:taythm}
    Suppose $f',...,f^{(n+1)}$ are defined on $[a,x]$, and that $R_{n,a}(x)$ is defined by \begin{equation*}
        R_{n,a}(x) = f(x) - P_{n,a}(x)
    \end{equation*}
    Then \begin{equation*}
        R_{n,a}(x) = \frac{f^{(n+1)}(t)}{(n+1)!}(x-a)^{n+1}\;\;\text{ for some $t$ in } (a,x)
    \end{equation*}
\end{namthm}
\begin{proof}
    The function $R_{n,a}$ satisfies the conditions of the preceding Lemma by the definition of the Taylor polynomial, so \begin{equation*}
        \frac{R_{n,a}(x)}{(x-a)^{n+1}} = \frac{R_{n,a}^{(n+1)}(t)}{(n+1)!}
    \end{equation*}
    for some $t$ in $(a,x)$. But, \begin{equation*}
        R_{n,a}^{(n+1)} = f^{(n+1)}
    \end{equation*}
    since $R_{n,a} - f$ is a polynomial of degree $n$. Substituting gives the Lagrange form for the remainder, as desired.
\end{proof}

\begin{eg}
    Applying \ref{thmname:taythm} to the functions $\sin$, $\cos$, and $\exp$, with $a = 0$, we obtain the following formulas: \begin{align*}
        \sin x &= \sum\limits_{i=0}^n\frac{(-1)^ix^{2i+1}}{(2i+1)!} + \frac{\sin^{(2n+2)}(t)}{(2n+2)!}x^{2n+2} \\
        \cos x &= \sum\limits_{i=0}^n\frac{(-1)^ix^{2i}}{(2i)!} + \frac{\cos^{(2n+1)}(t)}{(2n+1)!}x^{2n+1} \\
        \exp x &= \sum\limits_{i=0}^n\frac{x^n}{n!} + \frac{\exp t}{(n+1)!}x^{n+1}
    \end{align*}
    (of course, we could go one power higher in the remainder terms for $\sin$ and $\cos$)
\end{eg}


\section{Infinite Sequences}

\begin{defn}
    An \Emph{infinite sequence} of real numbers is a real valued function whose domain is $\N$.
\end{defn}


\begin{defn}
    A sequence $\{a_n\}$ \Emph{converges to $\ell \in \R$}, denoted by $\lim\limits_{n\rightarrow \infty}a_n = \ell$, if for every $\epsilon > 0$ there exists $N \in \N$ such that for all $n \in \N$, if $n \geq N$ then \begin{equation*}
        |a_n - \ell| < \epsilon
    \end{equation*}
    A sequence $\{a_n\}$ is said to \Emph{converge} if such an $\ell$ exists, and to \Emph{diverge} otherwise.
\end{defn}

\begin{thm}[Limit Laws]
    If $\lim\limits_{n\rightarrow \infty}a_n$ and $\lim\limits_{n\rightarrow \infty}b_n$ both exist, then \begin{align*}
        \lim\limits_{n\rightarrow \infty}(a_n+b_n) &= \lim\limits_{n\rightarrow \infty}a_n + \lim\limits_{n\rightarrow \infty}b_n \\
        \lim\limits_{n\rightarrow \infty}a_n\cdot b_n &= \lim\limits_{n\rightarrow \infty}a_n\cdot \lim\limits_{n\rightarrow \infty} b_n 
    \end{align*}
    moreover, if $\lim\limits_{n\rightarrow \infty}b_n\neq 0$, then $b_n \neq 0$ for all $n$ greater than some $N$, and \begin{equation*}
        \lim\limits_{n\rightarrow \infty}a_n/b_n = \lim\limits_{n\rightarrow \infty}a_n/\lim\limits_{n\rightarrow \infty}b_n
    \end{equation*}
\end{thm}
\begin{proof}
    (To be completed)
\end{proof}


\begin{thm}
    Let $f$ be a function defined in an open interval containing $c$, except perhaps at $c$ itself, with \begin{equation*}
        \lim\limits_{x\rightarrow c}f(x) = l
    \end{equation*}
    Suppose that $\{a_n\}$ is a sequence such that \begin{enumerate}
        \item each $a_n$ is in the domain of $f$,
        \item each $a_n \neq c$,
        \item $\lim\limits_{n\rightarrow \infty}a_n = c$
    \end{enumerate}
    Then the sequence $\{f(a_n)\}$ satisfies \begin{equation*}
        \lim\limits_{n\rightarrow \infty}f(a_n) = l
    \end{equation*}
    Conversely, if this is true for every sequence $\{a_n\}$ satisfying the above conditions, then $\lim\limits_{x\rightarrow c}f(x) = l$.
\end{thm}
\begin{proof}
    Suppose first that $\lim\limits_{x\rightarrow c}f(x) = l$. Then for every $\varepsilon > 0$ there is a $\delta > 0$ such that, for all $x$, \begin{equation*}
        \text{if } 0 < |x-c| < \delta, \text{ then } |f(x) - l| < \varepsilon
    \end{equation*}
    If the sequence $\{a_n\}$ satisfies $\lim\limits_{n\rightarrow \infty}a_n = c$, then there is a natural number $N$ such that for all $n \in \N$, \begin{equation*}
        \text{if } n \geq N, \text{ then } 0 < |a_n - c| < \delta
    \end{equation*}
    By our choice of $\delta$ this implies that \begin{equation*}
        |f(a_n) - l| <\varepsilon
    \end{equation*}
    showing that $\lim\limits_{n\rightarrow \infty} f(a_n) = l$.


    Suppose, conversely, that $\lim\limits_{n\rightarrow \infty}f(a_n) = l$ for every sequence $\{a_n\}$ satisfying our three conditions. If $\lim\limits_{x\rightarrow c}f(x) = l$ were not true, there would be some $\varepsilon > 0$ such that for all $\delta > 0$ there exists $x$ such that $0 < |x-c| < \delta$ but $|f(x) - l| \geq \varepsilon$. In particular, for each $n$ there would exist $x_n$ such that $0<|x_n - c| < 1/n$, but $|f(x_n) - l| \geq \varepsilon$. Define a sequence $\{x_n\}$ using these $x_n$. Then $x_n$ is in the domain of $f$ for each $n$, and as $0 < |x_n-c|$ for each $n$, $x_n \neq c$ for all $n$. Moreover, for all $\varepsilon' > 0$ there exists $N \in \N$ such that $\varepsilon' > 1/N > 0$ (by the Archimedean Property of $\R$), so for all $n \geq N$, $0 < |x_n - c| < 1/n < \varepsilon'$. Consequently, $\lim\limits_{n\rightarrow \infty}x_n = c$, so the sequences satisfies all of our initial conditions. However, then by assumption $\lim\limits_{n\rightarrow \infty}f(x_n) = l$. But, by construction, for $\varepsilon$ we have that for all $n \in \N$, $0 < |x_n - c| < 1/n$ but $|f(x_n) - l| \geq \varepsilon$, so $f(x_n)$ does not converge to $l$, contradicting our hypothesis. Thus $\lim\limits_{x\rightarrow c}f(x) = l$ must be true.
\end{proof}


\begin{defn}
    A sequence $\{a_n\}$ is \Emph{increasing} if $a_{n+1} > a_n$ for all $n$, \Emph{nondecreasing} if $a_{n+1} \geq a_n$ for all $n$, and \Emph{bounded above} if there is a number $M$ such that $a_n \leq M$ for all $n$. Similarly, a sequence $\{a_n\}$ is \Emph{decreasing} if $a_{n+1} < a_n$ for all $n$, \Emph{nonincreasing} if $a_{n+1} \leq a_n$ for all $n$, and \Emph{bounded below} if there is a number $m$ such that $a_n \geq m$ for all $n$.
\end{defn}

\begin{thm}
    If $\{a_n\}$ is nondecreasing and bounded above, then $\{a_n\}$ converges.
\end{thm}
\begin{proof}
    The set $A := \{a_n:n\in\N\}$ is, by assumption, bounded above, so $A$ has a least upper bound $\alpha \in \R$. We claim that $\lim\limits_{n\rightarrow \infty}a_n = \alpha$. If $\varepsilon > 0$, then $\alpha - \varepsilon$ is not an upper bound for $A$ so there exists $a_N$ in $A$ such that $a_N > \alpha - \varepsilon$, so $\alpha - a_N < \varepsilon$. Then for all $n \geq N$, $a_n \geq a_N$ since $\{a_n\}$ is nondecreasing so \begin{equation*}
        |\alpha - a_n| = \alpha - a_n \leq \alpha - a_N < \varepsilon
    \end{equation*}
    Consequently, we conclude that $\lim\limits_{n\rightarrow \infty}a_n = \alpha$.
\end{proof}


\begin{defn}
    A \Emph{subsequence} of a sequence $\{a_n\}$ is a sequence \begin{equation*}
        a_{n_1},a_{n_2},a_{n_3},...
    \end{equation*}
    where the $n_j$ are natural numbers with $n_1 < n_2 < n_3 < ...$.
\end{defn}

\begin{lem}
    Any sequence $\{a_n\}$ contains a subsequence which is either nondecreasing or nonincreasing.
\end{lem}
\begin{proof}
    Call a natural number $n$ a ``peak point" of a sequence $\{a_n\}$ if $a_m < a_n$ for all $m > n$.\begin{itemize}[leftmargin=+1in]
        \item[Case 1.] The sequence has infinitely many peak points. In this case, if $n_1 < n_2 < n_3 < ...$ are the peak points, then $a_{n_1} > a_{n_2} > a_{n_3} > ...$, so $\{a_{n_j}\}$ is a nonincreasing subsequence of $\{a_n\}$.
        \item[Case 2.] THe sequence has only finitely many peak points. In this case, let $n_1$ be greater than all peak points. Since $n_1$ is not a peak point, there is some $n_2 > n_1$ such that $a_{n_2} \geq a_{n_1}$. Since $n_2$ is not a peak point, there is some $n_3 > n_2$ such that $a_{n_3} > a_{n_2}$. Suppose there exists $k \geq 3$ such that for all $1 \leq m < k$, $a_{n_m} \leq a_{n_{m+1}}$ and $n_m < n_{m+1}$. Then since $n_k$ is not a peak point, there is some $n_{k+1}$ such that $n_{k+1} > n_k$ and $a_{n_{k+1}} \geq a_{n_k}$. Thus, by recursive definition we have constructed a nondecreasing subsequence $\{a_{n_k}\}$ of $\{a_n\}$.
    \end{itemize}
\end{proof}


\begin{cor}[The Bolzano-Weierstrass Theorem]\label{cor:bolz}
    Every bounded sequence has a convergent subsequence.
\end{cor}


\begin{defn}
    A sequence $\{a_n\}$ is a \Emph{Cauchy sequence} if for every $\varepsilon > 0$ there is a natural number $N$ such that, for all $m,n \in \N$, if $m,n \geq N$, then \begin{equation*}
        |a_n-a_m| <\varepsilon
    \end{equation*}
    (This can be written as $\lim\limits_{m,n\rightarrow \infty}|a_m-a_n| = 0$)
\end{defn}


\begin{thm}
    A sequence $\{a_n\}$ converges if and only if it is a Cauchy sequence.
\end{thm}
\begin{proof}
    First assume that $\lim\limits_{n\rightarrow \infty}a_n = l$ for some $l \in \R$. Then given $\varepsilon > 0$, there exists $N \in \N$ such that if $n \geq N$, then $|a_n - l| < \varepsilon/2$. Hence, if $m,n \geq N$, then $$|a_n - a_m| \leq |a_n - l| + |a_m-l| < \varepsilon/2+\varepsilon/2 = \varepsilon$$
    Thus, $\{a_n\}$ is Cauchy.

    Conversely, suppose that $\{a_n\}$ is a Cauchy sequence. I claim that this implies $\{a_n\}$ is bounded. First, for $\varepsilon = 1 > 0$, there exists $N \in \N$ such that if $m,n \geq N$, then $|a_m-a_n| < \varepsilon = 1$. In particular, for all $n \geq N$, $|a_N - a_n| < 1$. Take $M = \max(a_N + 1, a_{N-1},...,a_1)$, and $m = \min(a_N - 1, a_{N-1},...,a_1)$. Then for all $k \leq N$ we have that $m \leq a_k \leq M$. On the other hand, for $k \geq N$, we have that $a_N - 1 < a_k < a_N + 1$, so $m < a_k < M$. Thus $\{a_n:n \in \N\}$ is bounded. 

    Then, by \ref{thmname:bolz} $\{a_n\}$ has a convergent subsequence $\{a_{n_k}\}$. Let $\lim\limits_{k\rightarrow \infty}a_{n_k} = l$, for some $l \in \R$. Then, fix $\varepsilon > 0$. It follows that there exist $K, K' \in \N$ such that for $m,n \geq K$ and $j \geq K'$, $|a_m - a_n| < \varepsilon/2$, while $|a_{n_j} - l| < \varepsilon/2$. Note that $n_j \geq j$, since the sequence $\{n_k\}$ is increasing. Then, for all $i \geq \max(K,K')$ we have that \begin{equation*}
        |a_i - l| \leq |a_i - a_{n_i}| + |a_{n_i} - l| < \varepsilon/2 + \varepsilon/2 = \varepsilon
    \end{equation*}
    Therefore, by definition $\{a_n\}$ converges to $l$ as well.
\end{proof}


\section{Infinite Series}

\begin{defn}
    The sequence $\{a_n\}$ is \Emph{summable} if the sequence $\{s_n\}$ converges, where \begin{equation*}
        s_n = \sum\limits_{i=1}^na_i
    \end{equation*}
    is the $n$-th \Emph{partial sum}. In this case, $\lim_{n\rightarrow \infty}s_n$ is denoted by \begin{equation*}
        \sum\limits_{n=1}^{\infty}a_n
    \end{equation*}
    and is called the \Emph{sum} of the sequence $\{a_n\}$.
\end{defn}

\begin{rmk}
    If $\{a_n\}$ and $\{b_n\}$ are summable, then \begin{align*}
        \sum\limits_{n=1}^{\infty}(a_n+b_n) &= \sum\limits_{i=1}^{\infty}a_n + \sum\limits_{i=1}^{\infty}b_n \\
        \sum\limits_{n=1}^{\infty}c\cdot a_n &= c\cdot\sum\limits_{i=1}^{\infty}a_n
    \end{align*}
    for all $c \in \R$. 
\end{rmk}

\begin{namthm}[The Cauchy Criterion]\label{thmname:cauchcrit}
    The sequence $\{a_n\}$ is summable if and only if for all $\varepsilon > 0$ there exists $N \in \N$ such that for all $m \geq n \in \N$, if $m,n \geq N$, then \begin{equation*}
        \left|\sum\limits_{i=1}^ma_i - \sum\limits_{i=1}^na_i\right| = \left|\sum\limits_{i=n+1}^ma_i\right| <\varepsilon
    \end{equation*}
\end{namthm}

\begin{rmk}
    This result is a direct conseqeunce of the fact that a sequence in $\R$ is Cauchy if and only if it converges applied to the sequence of partial sums for $\{a_n\}$.
\end{rmk}


\begin{namthm}[The Vanishing Condition]
    If $\{a_n\}$ is summable, then \begin{equation*}
        \lim\limits_{n\rightarrow \infty}a_n = 0
    \end{equation*}
\end{namthm}
\begin{proof}
    If $\lim\limits_{n\rightarrow \infty}s_n = l$, then \begin{align*}
        \lim\limits_{n\rightarrow \infty}a_n &= \lim\limits_{n\rightarrow \infty}(s_n - s_{n-1}) = \lim\limits_{n\rightarrow \infty}s_n - \lim\limits_{n\rightarrow \infty}s_{n-1} \\
        &= l - l = 0
    \end{align*}
\end{proof}

\begin{eg}
    The \Emph{geometric series} are of the form \begin{equation*}
        \sum\limits_{n=0}^{\infty}r^n = 1+r+r^2+r^3+\hdots
    \end{equation*}
    For $|r| \geq 1$, $\lim\limits_{n\rightarrow \infty}r^n \neq 0$, so $\{r^n\}$ is not summable. On the other hand, if $|r| < 1$ the sequence is summable. First write $$s_n = 1+r+r^2+...+r^n,\;\text{ and }\;rs_n = r+r^2+r^3+...+r^{n+1}$$
    It follows that $$s_n(1-r) = 1 - r^{n+1}$$ so $$s_n = \frac{1-r^{n+1}}{1-r}$$ since $r \neq 1$. Finally, it follows that \begin{align*}
        \sum\limits_{n=0}^{\infty}r^n = \lim\limits_{n\rightarrow \infty}s_n = \lim\limits_{n\rightarrow \infty}\frac{1-r^{n+1}}{1-r} = \frac{1}{1-r}
    \end{align*}
    as $|r| < 1$.
\end{eg}

\begin{defn}
    A sequence $\{a_n\}$ such that $a_n\geq 0$ for all $n \in \N$ is said to be \Emph{nonnegative}.
\end{defn}

\begin{namthm}[The Boundedness Criterion]\label{thmname:boundcrit}
    A nonnegative sequence $\{a_n\}$ is summable if and only if the set of partial sums $s_n$ is bounded.
\end{namthm}
\begin{proof}
    Since $\{a_n\}$ is nonnegative, $\{s_n\}$ is nondecreasing. Hence from our previous results on monotone sequences, $\{s_n\}$ converges if and only if $\{s_n\}$ is bounded.
\end{proof}


\begin{namthm}[The Comparison Test]\label{thmname:comptest}
    Suppose that $\{a_n\}$ and $\{b_n\}$ are sequences such that $0 \leq a_n \leq b_n$ for all $n \in \N$. Then if $\sum\limits_{n=1}^{\infty}b_n$ converges, so does $\sum\limits_{n=1}^{\infty}a_n$.
\end{namthm}
\begin{proof}
    Let $s_n$ denote the $n$-th partial sum of $\{a_n\}$, and let $t_n$ denote the $n$-th partial sum of $\{b_n\}$. Then $0 \leq s_n \leq t_n$ for all $n \in \N$. Now $\{t_n\}$ converges by assumption, so it is bounded. Hence, there exists $M \in \R$ such that $0 \leq s_n \leq t_n \leq M$ for all $n\in \N$, so $\{s_n\}$ is also bounded. Thus by \ref{thmname:boundcrit} $\{a_n\}$ is summable, so by definition $\sum\limits_{n=1}^{\infty}a_n$ converges.
\end{proof}


\begin{namthm}[The Limit Comparison Test]\label{thmname:limcomptest}
    If $a_n, b_n > 0$ for convergent sequences $\{a_n\}$ and $\{b_n\}$, and $\lim\limits_{n\rightarrow \infty}a_n/b_n = c \neq 0$, then $\sum\limits_{n=1}^{\infty}a_n$ converges if and only if $\sum\limits_{n=1}^{\infty}b_n$ converges.
\end{namthm}
\begin{proof}
    Suppose $\sum\limits_{n=1}^{\infty}b_n$ converges. Since $\lim\limits_{n\rightarrow \infty}a_n/b_n = c$, there is some $N$ such that \begin{equation*}
        a_n/b_n - c \leq c \implies a_n \leq 2cb_n,\;\;\text{ for } n\geq N
    \end{equation*}
    But the sequence $2c\sum\limits_{n=N}^{\infty}b_n$ certainly converges. Then by \ref{thmname:comptest} we have that $\sum\limits_{n=N}^{\infty}a_n$ converges, and this implies convergence of the whole series $\sum\limits_{n=1}^{\infty}a_n$, which only has finitely many additional terms.
    
    Note that \begin{equation*}
        \lim\limits_{n\rightarrow \infty}b_n/a_n = \frac{1}{\lim\limits_{n\rightarrow\infty}a_n/b_n} = 1/c \neq 0
    \end{equation*}
    so the converse follows immediately.
\end{proof}


\begin{namthm}[The Ratio Test]\label{thmname:ratio}
    Let $\{a_n\}$ be a positive sequence, and suppose that \begin{equation*}
        \lim\limits_{n\rightarrow \infty}\frac{a_{n+1}}{a_n} = r
    \end{equation*}
    for some $r \geq 0$. Then $\sum\limits_{n=1}^{\infty}a_n$ converges if $r < 1$. On the other hand, if $r > 1$, then the terms $a_n$ are unbounded, so $\sum\limits_{n=1}^{\infty}a_n$ diverges.
\end{namthm}
\begin{proof}
    Suppose first that $r<1$. Choose any number $s$ with $r < s < 1$. The hypothesis $\lim\limits_{n\rightarrow \infty}a_{n+1}/a_n = r < 1$ implies that there is some $N \in \N$ such that if $n \geq N$, \begin{equation*}
        a_{n+1}/a_n - r < s-r \implies a_{n+1}/a_n < s
    \end{equation*}
    This can be written as $a_{n+1} < sa_n$. Thus, \begin{align*}
        a_{N+1} &< sa_N \\
        a_{N+2} &< sa_{N+1} < s^2a_N \\
        &\vdots \\
        a_{N+k} &< s^ka_N
    \end{align*}
    Since $\sum\limits_{k=0}^{\infty}a_Ns^k = a_N\sum\limits_{k=0}^{\infty}s^k$ converges, since $|s| < 1$, \ref{thmname:comptest} shows that \begin{equation*}
        \sum\limits_{n=N}^{\infty}a_n = \sum\limits_{k=0}^{\infty}a_{N+k}
    \end{equation*}
    converges as $a_{N+k} < a_Ns^k$ for all $k \geq 0$. This implies that $\sum\limits_{n=0}^{\infty}a_n$ as a whole converges.

    If $r > 1$, choose some $s \in \R$ such that $1 < s < r$. Then there is a number $N \in \N$ such that \begin{equation*}
        r - a_{n+1}/a_n < r-s \implies s < a_{n+1}/a_n
    \end{equation*}
    for all $n \geq N$. This implies that \begin{equation*}
        a_{N+k} > a_Ns^k,
    \end{equation*}
    for all $k \in \N$, so the terms are unbounded.
\end{proof}


\begin{namthm}[The Integral Test]\label{thmname:inttest}
    Suppose that $f$ is positive and decreasing on $[1,\infty)$, and that $f(n) = a_n$ for all $n \in \N$. Then $\sum\limits_{n=1}^{\infty}a_n$ converges if and only if the limit \begin{equation*}
        \int_1^{\infty}f = \lim\limits_{A\rightarrow \infty}\int_1^Af
    \end{equation*}
    exists.
\end{namthm}
\begin{proof}
    The existence of $\lim\limits_{A\rightarrow \infty}\int_1^Af$ is equivalent to the convergence of the series \begin{equation*}
        \int_1^2f + \int_2^3f + \int_3^4f + ...
    \end{equation*}
    Since $f$ is decreasing, we have \begin{equation*}
        f(n+1) < \int_n^{n+1}f < f(n) 
    \end{equation*}
    The first half of this double inequality shows that the series $\sum\limits_{n=1}^{\infty}a_{n+1}$ may be compared to the series $\sum\limits_{n=1}^{\infty}\int_n^{n+1}$, proving that $\sum\limits_{n=1}^{\infty}a_{n+1}$ (and hence $\sum\limits_{n=1}^{\infty}a_n$) converges if $\lim\limits_{A\rightarrow \infty}\int_1^Af$ exists.

    The second half of the inequality shows that the series $\sum\limits_{n=1}^{\infty}\int_n^{n+1}f$ may be compared to the series $\sum\limits_{n=1}^{\infty}a_n$, proving that $\lim\limits_{A\rightarrow \infty}\int_1^Af$ must exist if $\sum\limits_{n=1}^{\infty}a_n$ converges.
\end{proof}


\begin{cor}
    The series $\sum\limits_{n=1}^{\infty}1/n^p$ converges if and only if $p > 1$.
\end{cor}
\begin{proof}
    If $p < 0$ then $\lim\limits_{n\rightarrow \infty}1/n^p \neq 0$. If $p > 0$, the convergence of $\sum\limits_{n=1}^{\infty}1/n^p$ is equivalent, by \ref{thmname:inttest}, to the existence of \begin{equation*}
        \lim\limits_{A\rightarrow\infty}\int_1^A\frac{1}{x^p}dx
    \end{equation*}
    Now, observe that \begin{equation*}
        \int_1^A\frac{1}{x^p}dx = \left\{\begin{array}{lc} -\frac{1}{p-1}\cdot\frac{1}{A^{p-1}} + \frac{1}{p-1}, & p \neq 1 \\ \log A, & p = 1\end{array}\right.
    \end{equation*}
    This shows that $\lim\limits_{A\rightarrow \infty}\int_1^A1/x^pdx$ exists if $p > 1$, but not if $p \leq 1$. Thus, $\sum\limits_{n=1}^{\infty}1/n^p$ converges precisely for $p > 1$. 
\end{proof}

\begin{defn}
    The series $\sum\limits_{n=1}^{\infty}a_n$ is \Emph{absolutely convergent} if the series $\sum\limits_{n=1}^{\infty}|a_n|$ is convergent. (In formal language, the sequence $\{a_n\}$ is said to be \Emph{absolutely summable} if the sequence $\{|a_n\}$ is summable.)
\end{defn}

\begin{thm}\label{thm:absconv}
    Every absolutely convergent series is convergent. Moreover, a series is absolutely convergent if and only if the series formed from its positive terms and the series formed from its negative terms both converge.
\end{thm}
\begin{proof}
    If $\sum\limits_{n=1}^{\infty}|a_n|$ converges, then, by \ref{thmname:cauchcrit}, \begin{equation*}
        \lim\limits_{m,n\rightarrow \infty}\sum\limits_{i=n+1}^m|a_i| = 0
    \end{equation*}
    Since \begin{equation*}
        \left|\sum\limits_{i=n+1}^ma_i\right| \leq \sum\limits_{i=n+1}^m|a_i|
    \end{equation*}
    it follows that  \begin{equation*}
        \lim\limits_{m,n\rightarrow \infty}\sum\limits_{i=n+1}^ma_i = 0
    \end{equation*}
    which shows that $\sum\limits_{n=1}^{\infty}a_n$ converges.

    To prove the second portion of the theorem, let $\{a_n^+\}$ and $\{a_n^-\}$ be sequences defined by \begin{align*}
        a_n^+ &= \left\{\begin{array}{lc} a_n, & \text{if } a_n\geq 0 \\ 0, & \text{if } a_n \leq 0 \end{array}\right. \\
            a_n^- &= \left\{\begin{array}{lc} a_n, & \text{if } a_n\leq 0 \\ 0, & \text{if } a_n \geq 0 \end{array}\right.
    \end{align*}
    It follows that \begin{equation*}
        \sum\limits_{n=1}^{\infty}|a_n| = \sum\limits_{n=1}^{\infty}[a_n^+-a_n^-] = \sum\limits_{n=1}^{\infty}a_n^+ - \sum\limits_{n=1}^{\infty}a_n^-
    \end{equation*}
    so if $\sum\limits_{n=1}^{\infty}a_n^+$ and $\sum\limits_{n=1}^{\infty}a_n^-$ are convergent, so is $\sum\limits_{n=1}^{\infty}|a_n|$.


    Conversely, suppose $\sum\limits_{n=1}^{\infty}|a_n|$ converges. Then by our initial argument $\sum\limits_{n=1}^{\infty}a_n$ converges also. Therefore, \begin{equation*}
        \sum\limits_{n=1}^{\infty}a_n^+ = \sum\limits_{n=1}^{\infty}\frac{1}{2}[a_n + |a_n|] = \frac{1}{2}\left(\sum\limits_{n=1}^{\infty}a_n + \sum\limits_{n=1}^{\infty}|a_n|\right)
    \end{equation*}
    and  \begin{equation*}
        \sum\limits_{n=1}^{\infty}a_n^- = \sum\limits_{n=1}^{\infty}\frac{1}{2}[a_n - |a_n|] = \frac{1}{2}\left(\sum\limits_{n=1}^{\infty}a_n - \sum\limits_{n=1}^{\infty}|a_n|\right)
    \end{equation*}
    both converge.
\end{proof}



\begin{rmk}
    A consequent of this result is that every convergent series with positive terms can be used to obtain infinitely many other convergent series simply by putting minus sings at random.
\end{rmk}

\begin{defn}
    A convergent series which is not absolutely convergent is said to be \Emph{conditionally convergent}.
\end{defn}


\begin{namthm}[Leibniz's Theorem]\label{thmname:alttest}
    Suppose that $\{a_n\}$ is a non-decreasing non-negative sequence such that \begin{equation*}
        \lim\limits_{n\rightarrow \infty} a_n = 0
    \end{equation*}
    Then the series \begin{equation*}
        \sum\limits_{n=1}^{\infty}(-1)^{n+1}a_n = a_1-a_2+a_3-a_4+...
    \end{equation*}
    converges.
\end{namthm}
\begin{proof}
    First, observe that \begin{enumerate}
        \item $s_2 \leq s_4 \leq s_6 \leq ...$
        \item $s_1 \geq s_3 \geq s_5 \geq ...$ 
        \item $s_k\leq s_l$ if $k$ is even and $l$ is odd.
    \end{enumerate}
    Indeed, note that for all $n$, $a_{2n+1} \geq a_{2n+2}$, so $a_{2n+1} - a_{2n+2} \geq 0$, so \begin{equation*}
        s_{2n+2} = s_{2n} + a_{2n+1} - a_{2n+2} \geq s_{2n}
    \end{equation*}
    and similarly as $a_{2n+2} \geq a_{2n+3}$, we have \begin{equation*}
        s_{2n+3} = s_{2n+1} -a_{2n+2}+a_{2n+3} \leq s_{2n+1}
    \end{equation*}
    To prove the third inequality, first notice that \begin{equation*}
        s_{2n} = s_{2n-1} - a_{2n} \leq s_{2n-1}
    \end{equation*}
    since $a_{2n} \geq 0$. Now, if $k$ is even and $l$ is odd, choose $n$ such that $k\leq 2n$ and $l \leq 2n-1$. Then \begin{equation*}
        s_k \leq s_{2n} \leq s_{2n-1} \leq s_l
    \end{equation*}
    which proves the third inequality.

    Now, the sequence $\{s_{2n}\}$ converges, because it is nondecreasing and is bounded above (by $s_l$ for any odd $l$). Let $\alpha = \sup\{s_{2n}\} = \lim\limits_{n\rightarrow\infty} s_{2n}$. Similarly, let $\beta = \inf\{s_{2n+1}\} = \lim\limits_{n\rightarrow \infty}s_{2n+1}$. It follows from our third inequality that $\alpha \leq \beta$; since \begin{equation*}
        s_{2n+1}-s_{2n} = a_{2n+1}\;\;\text{ and }\;\;\lim\limits_{n\rightarrow \infty}a_n = 0
    \end{equation*}
    it is actually the case that $\alpha = \beta$. This proves that $\alpha = \beta = \lim\limits_{n\rightarrow \infty} s_n$.
\end{proof}


\begin{defn}
    A sequence $\{a_n\}$ is a \Emph{rearrangement} of a sequence $\{a_n\}$ if each $b_n = a_{f(n)}$ where $f$ is a certain permutation on the natural numbers.
\end{defn}



\begin{thm}
    If $\sum\limits_{n=1}^{\infty}a_n$ converges, but does not converge absolutely; then for any number $\alpha$ there is a rearrangement $\{b_n\}$ of $\{a_n\}$ such that $\sum\limits_{n=1}^{\infty}b_n = \alpha$.
\end{thm}
\begin{proof}
    Let $\sum\limits_{n=1}^{\infty}p_n$ denote the series formed from the positive terms of $\{a_n\}$ and let $\sum\limits_{n=1}^{\infty}q_n$ denote the series of negative terms. It follows from Theorem \ref{thm:absconv} that at least one of these series does not converge. As a matter of fact, both must fail to converge, for if one had bounded partial sums, and the other had unbounded partial sums, then the original series $\sum\limits_{n=1}^{\infty}a_n$ would also have unbounded partial sums, contradicting the assumption that it converges.

    Let $\alpha$ be any number. Assume, for simplicity, that $\alpha > 0$. Since the series $\sum\limits_{n=1}^{\infty}p_n$ there is a number $N$ such that \begin{equation*}
        \sum\limits_{n=1}^Np_n > \alpha
    \end{equation*}
    We will choose $N_1$ to be the smallest $N$ with this property. This means that \begin{equation*}
        \sum\limits_{n=1}^{N_1-1}p_n \leq \alpha\;\;\text{ and }\;\;\sum\limits_{n=1}^{N_1}p_n > \alpha
    \end{equation*}
    Then if $S_1 = \sum\limits_{n=1}^{N_1}p_n$, we have $S_1 - \alpha \leq p_{N_1}$. Next, choose the smallest integer $M_1$ for which \begin{equation*}
        T_1 = S_1 + \sum\limits_{n=1}^{M_1}q_n < \alpha
    \end{equation*}
    As before, we have $\alpha - T_1 \leq -q_{M_1}$. We continue this process indefinitely. The sequence \begin{equation*}
        p_1,...,p_{N_1},q_1,...,q_{M_1},q_{N_1+1},...,p_{N_2},...
    \end{equation*}
    is a rearrangement of $\{a_n\}$. The partial sums of this rearrangement increase to $S_1$, then decrease to $T_1$, then increase to $S_2$, etc. To complete the proof we note that $|S_k -\alpha|$ and $|T_k - \alpha|$ are less that or equal to $p_{N_k}$ or $-q_{M_k}$, respectively, and that these terms, being numbers of the original sequence $\{a_n\}$, must decrease to $0$, since $\sum\limits_{n=1}^{\infty}a_n$ converges.
\end{proof}



\begin{thm}
    If $\sum\limits_{n=1}^{\infty}a_n$ converges absolutely, and $\{b_n\}$ is any rearrangement of $\{a_n\}$, then $\sum\limits_{n=1}^{\infty}b_n$ also converges (absolutely), and in particular \begin{equation*}
        \sum\limits_{n=1}^{\infty}a_n = \sum\limits_{n=1}^{\infty}b_n
    \end{equation*}
\end{thm}
\begin{proof}
    Denote the partial sums of $\{a_n\}$ by $s_n$, and the partial sums of $\{b_n\}$ by $t_n$. Suppose that $\varepsilon > 0$. Since $\sum\limits_{n=1}^{\infty}a_n$ converges, there is some $N$ such that \begin{equation*}
        \left|\sum\limits_{n=1}^{\infty}a_n - s_N\right| < \varepsilon
    \end{equation*}
    Moreover, since $\sum\limits_{n=1}^{\infty}|a_n|$ converges, we can also choose $N'$ so that \begin{equation*}
        \sum\limits_{n=1}^{\infty}|a_n| - (|a_1|+\hdots + |a_{N'}|) < \varepsilon
    \end{equation*}
    so that \begin{equation*}
        \sum\limits_{n=N+1}^{\infty}|a_n| < \varepsilon
    \end{equation*}
    Choose $M$ such that each $a_1,...,a_N$ appear among $b_1,...,b_M$. Then whenever $m > M$, the difference $t_m - s)N$ is the sum of certain $a_i$, where $i > N$. Consequently, \begin{equation*}
        |t_m-s_N| \leq \sum\limits_{n=N+1}^{\infty}|a_n| < \varepsilon
    \end{equation*}
    Thus, if $m > $, then \begin{align*}
        \left|\sum\limits_{n=1}^{\infty}a_n - t_m \right|&= \left|\sum\limits_{n=1}^{\infty}a_n - s_N - (t_m-s_N)\right| \\
        &\leq \left|\sum\limits_{n=1}^{\infty}a_n - s_N\right| +|(t_m-s_N)| \\
        &< \varepsilon + \varepsilon
    \end{align*}
    Since this is true for every $\varepsilon > 0$, the series $\sum\limits_{n=1}^{\infty}b_n$ converges to $\sum\limits_{n=1}^{\infty}a_n$.

    To show that $\sum\limits_{n=1}^{\infty}b_n$ converges absolutely, note that $\{|b_n|\}$ is a rearrangement of $\{|a_n|\}$; since $\sum\limits_{n=1}^{\infty}|a_n|$ converges absolutely, $\sum\limits_{n=1}^{\infty}|b_n|$ converges by the first part of the theorem.
\end{proof}


\begin{thm}
    If $\sum\limits_{n=1}^{\infty}a_n$ and $\sum\limits_{n=1}^{\infty}b_n$ converge absolutely, and $\{c_n\}$ is any sequence containing the products $a_ib_j$ for each pair $(i,j) \in \N\times \N$, then \begin{equation*}
        \sum\limits_{n=1}^{\infty}c_n = \sum\limits_{n=1}^{\infty}a_n\cdot\sum\limits_{n=1}^{\infty}b_n
    \end{equation*}
\end{thm}
\begin{proof}
    Notice first that the sequence \begin{equation*}
        p_L = \sum\limits_{i=1}^L|a_i|\cdot \sum\limits_{j=1}^L|b_j|
    \end{equation*}
    converges since $\{a_n\}$ and $\{b_n\}$ are absolutely convergent, and since the limit of a product is the product of the limits. So $\{p_L\}$ is a Cauchy sequence, which means that for any $\varepsilon > 0$, if there exists $N$ such that for all $L,L' \geq N$, \begin{equation*}
        \left|\sum\limits_{i=1}^{L'}|a_i|\cdot \sum\limits_{j=1}^{L'}|b_j| - \sum\limits_{i=1}^L|a_i|\cdot \sum\limits_{j=1}^L|b_j|\right| < \varepsilon/2
    \end{equation*}
    It follows that \begin{equation*}
        \sum\limits_{i\;or\;j > L} |a_i|\cdot|b_i| \leq \varepsilon/2 < \varepsilon \tag{1}
    \end{equation*}
    Now suppose that $N$ is such that the terms $c_n$ for $n \leq N$ include all $a_ib_j$ for $i,j \leq L$. Then the difference \begin{equation*}
        \sum\limits_{n=1}^Nc_n - \sum\limits_{i=1}^La_i\cdot \sum\limits_{j=1}^Lb_j
    \end{equation*}
    consists of terms $a_ib_j$ with $i > L$ of $j > L$, so \begin{equation*}
        \left|\sum\limits_{n=1}^Nc_n - \sum\limits_{i=1}^La_i\cdot \sum\limits_{j=1}^Lb_j\right| \leq \sum\limits_{i\;or\;j>L}|a_i|\cdot|b_j| < \varepsilon \tag{2}
    \end{equation*}
    But since the limit of a product is the product of the limits we also have \begin{equation*}
        \left|\sum\limits_{i=1}^{\infty}a_i\cdot \sum\limits_{j=1}^{\infty}b_j - \sum\limits_{i=1}^La_i\cdot \sum\limits_{j=1}^Lb_j\right| < \varepsilon \tag{3}
    \end{equation*}
    for sufficiently large $L$. Consequently, if we choose $L$ and then $N$ large enough, we will have \begin{align*}
        \left|\sum\limits_{i=1}^{\infty}a_i\cdot \sum\limits_{j=1}^{\infty}b_j - \sum\limits_{i=1}^Nc_i\right| \leq \left|\sum\limits_{i=1}^{\infty}a_i\cdot \sum\limits_{j=1}^{\infty}b_j - \sum\limits_{i=1}^La_i\cdot \sum\limits_{j=1}^Lb_j\right| \\
        &+ \left|\sum\limits_{n=1}^Nc_n - \sum\limits_{i=1}^La_i\cdot \sum\limits_{j=1}^Lb_j\right| \\
        &< 2\varepsilon
    \end{align*}
    which proves the theorem.
\end{proof}




\section{Uniform Convergence and Power Series}


\begin{rmk}
    We are now interested in the study of series of functions, or in other words functions of the form \begin{equation*}
        f(x) = f_1(x) + f_2(x) + f_3(x) + \hdots
    \end{equation*}
    In such a situation $\{f_n\}$ will be some sequence of functions; for each $x$ we obtain a sequence of numbers $\{f_n(x)\}$, and $f(x)$ is the sum of this sequence. Recall that each sum $f_1(x)+f_2(x)+f_3(x) + \hdots$ is, by definition, the limit of the sequence $f_1(x),f_1(x)+f_2(x),f_1(x)+f_2(x)+f_3(x),...$. If we define a new sequence of functions $\{s_n\}$ by \begin{equation*}
        s_n = f_1 + \hdots + f_n
    \end{equation*}
    then we can express this fact more succinctly by writing \begin{equation*}
        f(x) = \lim\limits_{n\rightarrow \infty}s_n(x)
    \end{equation*}
    for some $x \in \R$.
\end{rmk}


\begin{rmk}
    First let us consider functions of the form \begin{equation*}
        f(x) = \lim\limits_{n\rightarrow\infty}f_n(x)
    \end{equation*}
    All this form may seem simple, it is very important to note that \Emph{nothing one would hope to be true actually is}. Instead we have a flurry of lovely counter-examples.
\end{rmk}

\begin{eg}[Counter-Example 1]
    Even if each $f_n$ is continuous, the function $f$ may not be! Indeed, consider the sequence of functions \begin{equation*}
        f_n(x) = \left\{\begin{array}{lc} x^n, & 0\leq x \leq 1 \\ 1, & x \geq 1 \end{array}\right.
    \end{equation*}
    These functions are all continuous, but the function $f(x) = \lim\limits_{n\rightarrow \infty}f_n(x)$ is not continuous; in fact, \begin{equation*}
        \lim\limits_{n\rightarrow \infty}f_n(x) = \left\{\begin{array}{lc} 0, & 0\leq x < 1 \\ 1, & x \geq 1 \end{array}\right.
    \end{equation*}
    Another example of this phenomenon is illustrated by the family of functions \begin{equation*}
        f_n(x) = \left\{\begin{array}{lc} -1, &  x \leq -\frac{1}{n} \\ nx, & -\frac{1}{n} \leq x \leq \frac{1}{n} \\ 1, & \frac{1}{n} \leq x \end{array}\right.
    \end{equation*}
    In this case, if $x < 0$ $f_n(x)$ is eventually $-1$, and if $x > 0$, then $f_n(x)$ is eventually $1$, while $f_n(0) = 0$ for all $n$. Thus, \begin{equation*}
        \lim\limits_{n\rightarrow \infty}f_n(x) = \left\{\begin{array}{lc} -1, & x < 0 \\ 0, & x = 0 \\ 1, & x > 0 \end{array}\right.
    \end{equation*}
    so once again $f(x) = \lim\limits_{n\rightarrow \infty}f_n(x)$ is not continuous.
\end{eg}

\begin{eg}[Counter-Example 2]
    It is even possible to produce a sequence of differentiable functions $\{f_n\}$ for which the function $f(x) = \lim\limits_{n\rightarrow \infty}f_n(x)$ is not continuous. One such sequence is \begin{equation*}
        f_n(x) = \left\{\begin{array}{lc} -1, &  x \leq -\frac{1}{n} \\ \sin\left(\frac{n\pi x}{2}\right), & -\frac{1}{n} \leq x \leq \frac{1}{n} \\ 1, & \frac{1}{n} \leq x \end{array}\right.
    \end{equation*}
    These functions are differentiable, but we still have  \begin{equation*}
        \lim\limits_{n\rightarrow \infty}f_n(x) = \left\{\begin{array}{lc} -1, & x < 0 \\ 0, & x = 0 \\ 1, & x > 0 \end{array}\right.
    \end{equation*}
\end{eg}

\begin{defn}
    If $f$ is a function defined on some set $A$, and a sequence of functions $\{f_n\}$, all defined on the same set $A$, are such that only \begin{equation*}
        f(x) = \lim\limits_{n\rightarrow \infty}f_n(x)
    \end{equation*}
    for all $x \in A$. Precisely, $\{f_n\}$ is said to \Emph{converge pointwise to $f$ on $A$} if for all $\varepsilon > 0$, and for all $x \in A$, there is some $N$ such that if $n \geq N$, then $|f(x) - f_n(x)| < \varepsilon$.
\end{defn}

\begin{defn}[Pointwise Convergence (alt.)]
    Suppose $S \subseteq \R$ and $f_n:S\rightarrow \R$ is a real-valued function for each $n \in \N$. We say that the sequence of functions $\{f_n\}$ \Emph{converges pointwise on $S$} to $f:S\rightarrow \R$ if for every $x \in S$ and every $\varepsilon > 0$, there exists $N \in \N$ such that if $n \geq N$, $|f_n(x) - f(x)| < \varepsilon$.
\end{defn}

\begin{eg}
    Take $f_n(x) = x^n$ on $S = [0,1]$. If $0 \leq x < 1$ notice that $\lim\limits_{n\rightarrow \infty}x^n = 0$ (geometric sequence). If $x = 1$, then $f_n(1) = 1^n = 1$, which converges to $1$ as $n$ goes to infinity. Thus, $f_n$ converges pointwise to \begin{equation*}
        f(x) = \left\{\begin{array}{lc} 0 & 0 \leq x < 1 \\ 1 & x = 1\end{array}\right.
    \end{equation*}
    Notice each $f_n$ is continuous on $[0,1]$, but $f$ is not.
\end{eg}

This example answers the following question in the negative:

\begin{qst}
    Suppose $f_n$ converges pointwise to $f$ on $S \subseteq \R$. If $a \in S'$ (an accumulation/limit point for $S$), $\lim\limits_{x\rightarrow a}f(x)$ exists and $\lim\limits_{x\rightarrow a}f_n(x)$ exists for all $n$, is it true that \begin{equation*}
        \lim\limits_{x\rightarrow a}\lim\limits_{n\rightarrow \infty}f_n(x) = \lim\limits_{n\rightarrow \infty}\lim\limits_{x\rightarrow a}f_n(x)?
    \end{equation*}
\end{qst}

In particular, in our example we take $a = 1$, then $\lim\limits_{x\rightarrow a}f_n(x) = \lim\limits_{x\rightarrow 1}x^n = 1$, so $\lim\limits_{n\rightarrow \infty}1 = 1$, but $\lim\limits_{x\rightarrow 1}f(x) = 0$.


\begin{eg}
    Consider the sequence $g_n(x) = \frac{1}{1+x^n}$ on $S = (-\infty,-1)\cup(-1,\infty)$. As $n$ goes to infinity we have that the sequence converges pointwise to $1$ for $|x| < 1$, $1/2$ for $x = 1$, and $0$ for $|x| > 1$.
\end{eg}

\begin{eg}
    Let $S = [0,\infty)$ and define \begin{equation*}
        f_n(x) = \left\{\begin{array}{lc} n^2x & 0 \leq x \leq \frac{1}{n} \\ -n^2\left(x-\frac{2}{n}\right) & \frac{1}{n} < x \leq \frac{2}{n} \\ 0 & x > \frac{2}{n}\end{array}\right.
    \end{equation*}
    We claim that $\lim\limits_{n\rightarrow \infty}f_n(x) = 0$ for all $x \geq 0$. When $x=0$, $f_n(0) = 0$ which converges to $0$. If $0 < x$: By the Archemedean property there exists $N \in \N$ such that $\frac{2}{N} < x$. Then $f_N(x) = 0$ and $f_n(x) = 0$ for all $n \geq N$. Thus, $\lim\limits_{n\rightarrow \infty}f_n(x) = 0$, as claimed. This argument can be intuitively realized by noting that for $n$ large enough, the tent is always to the left of any $0 < x$.
\end{eg}

We note that this gives an example of an unbounded sequence $\{f_n\}$ converging pointwise to a bounded function.

\begin{eg}
    Take $S = \R$ and define $f_n(x) = \frac{\sin(nx)}{\sqrt{n}}$, waves of declining amplitude but increasing frequency. Note that $0 \leq |f_n(x)| = \left|\frac{\sin(nx)}{\sqrt{n}}\right| \leq \frac{1}{\sqrt{n}}$ which goes to $0$ as $n$ goes to infinity, so the sequence converges pointwise to $0$. Notice $f_n'(x) = \frac{n}{\sqrt{n}}\cos(nx) = \sqrt{n}\cos(nx)$, which has no limit for any $x \in \R$.
\end{eg}

This is an example of pointwise convergence where the derivatives do not converge pointwise. Moreover, as we will see, the original sequence actually converges uniformly (the bound on the terms does not depend on $x$), which suggests we need a stronger requirement for convergence of derivatives.


\begin{defn}
    Let $\{f_n\}$ be a sequence of functions defined on $A$, and let $f$ be a function which is also defined on $A$. Then $f$ is called the \Emph{uniform limit of $\{f_n\}$ on $A$} if for every $\varepsilon > 0$ there is some $N$ such that for all $x \in A$, \begin{equation*}
        \text{if } n> N, \text{ then } |f(x) - f_n(x)| < \varepsilon
    \end{equation*}
    We also say that $\{f_n\}$ \Emph{converges uniformly to $f$ on $A$}, or that $f_n$ \Emph{approaches $f$ uniformly on $A$}.
\end{defn}


\begin{defn}[Uniform Convergence (alt.)]
    Suppose $S \subseteq \R$ and $f_n:S\rightarrow \R$ are real-valued functions for each $n \in \N$. We say that $f_n$ \Emph{converges uniformly on $S$} to $f:S\rightarrow \R$ if for all $\varepsilon > 0$, there is an $N \in \N$ (depends only on $\varepsilon$) such that $n \geq N$ implies \begin{equation*}
        |f_n(x) - f(x)| < \varepsilon
    \end{equation*}
    for all $x \in S$.
\end{defn}

We will use the notation $f_n\rightarrow_uf$ sometimes to denote uniform convergence. Intuitively uniform convergence can be understood by saying that given any band or tube containing $f$ on $S$, there is a point past which the tail functions of the sequence reside entirely in this band.

\begin{rmk}
    Note that uniform convergence implies pointwise convergence, but the converse is not true.
\end{rmk}

\begin{defn}[Uniform Norm]
    Suppose $S \subseteq \R$ and $f$ is a function on $S$. The \Emph{uniform norm} of $f$ on $S$ is given by \begin{equation*}
        ||f||_S := \sup\limits_{x\in S}|f(x)|
    \end{equation*}
\end{defn}
We note that this may not be finite. When the context is clear we will write $||f||_{\infty}$. 

\begin{rmk}
    Suppose $f:S\rightarrow \R$ is a function: \begin{enumerate}
        \item[(i)] $f$ is bounded on $S$ if and only if $||f||_S < \infty$
        \item[(ii)] If $f$ is continous on $S$ and $S$ is compact, then $||f||_S = \max\limits_{s \in S}|f(x)|$ (e.g. $||x^n||_{[0,1]} = 1$)
    \end{enumerate}
\end{rmk}


\begin{prop}
    A sequence of functions $f_n$ converges uniformly to $f$ on $S$ if and only if $||f_n-f||_S\rightarrow 0$.
\end{prop}
\begin{proof}
    Let $f_n$ be a sequence of functions, each defined on $S$, and let $f$ be another function on $S$.

    First, let us suppose that the $f_n$ converge uniformly to $f$ on $S$. Fix $\varepsilon > 0$. Then there exists $N \in \N$ such that for $n \geq N$, $|f_n(x) - f(x)| < \varepsilon/2$ for all $x \in S$. This implies that $\varepsilon/2$ is an upper bound of all $|f_n(x)-f(x)|$, so by definition $||f_n-f||_S \leq \varepsilon/2 < \varepsilon$ for all $n \geq N$. Hence, as $||f_n-f||_S = |||f_n -f||_S - 0|$, we have that the sequence $||f_n-f||_S$ converges to $0$ in $\R$.

    Conversely, suppose that $||f_n-f||_S$ converges to $0$, and let $\varepsilon > 0$. Then, there exists $N \in \N$ such that for all $n \geq N$, $||f_n-f||_S < \varepsilon$. It follows that for all $x \in S$, $|f_n(x) - f(x)| \leq ||f_n-f||_S < \varepsilon$ for $n \geq N$, so we find that $f_n$ converges uniformly to $f$ on $S$ by definition.
\end{proof}

\begin{eg}
    Let $S = [0,1]$ and $f_n(x) = x^n$. $f_n$ does not converge uniformly to any function. Indeed if $f_n$ converges uniformly to anything on $[0,1]$, it must be the pointwise limit $f(x) = 0, 0 \leq x < 1$ and $f(x) = 1, x =1$, since uniform implies pointwise convergence (and limits are unique in Hausdorff spaces). This implies $\sup\limits_{x \in [0,1]}|f_n(x) - f(x)|$ goes to zero as $n$ goes to infinity. But if $x = 1-\frac{1}{n}$, then $|f_n(1-1/n)-f(1-1/n)| = (1-1/n)^n$, which converges to $e^{-1} = \frac{1}{e}$, and not zero. Further, this must be less than what the limit of the supremums converges to, so the supremums cannot converge to $0$, as that would result in a contradiction. Hence, the sequence does not converge uniformly.
\end{eg}

The tent function is another example of pointwise but not uniform convergence, since $||f_n-0||_{\infty} = n$, which does not converge to $0$. 

\begin{thm}[Cauchy Criterion for Uniform Convergence]
    Suppose $f_n:S\rightarrow \R$. Then $f_n$ converges uniformly on $S$ if and only if for all $\varepsilon > 0$ there is an $N \in \N$ such that $m,n \geq N$ implies $$||f_n - f_m||_{\infty} < \varepsilon$$
\end{thm}
\begin{proof}
    Let $f_n:S\rightarrow \R$ be a sequence of functions.

    First, suppose $f_n$ converges uniformly on $S$ to some $f:S\rightarrow \R$. Fix $\varepsilon > 0$. Then there exists $N \in \N$ such that for $n \geq N$, $|f_n(x) - f(x)| < \varepsilon/3$, for all $x \in S$. It follows by the triangle inequality that for $k,m \geq N$, $|f_k(x) - f_m(x)| < 2\varepsilon/3$ for all $x \in S$. Thus, $||f_k - f_m||_{\infty} \leq 2\varepsilon/3 < \varepsilon$, so $f_n$ is uniformly Cauchy (i.e. it is Cauchy with respect to the uniform norm $||\cdot||_{\infty}$)

    Conversely, suppose $f_n$ is uniformly Cauchy on $S$. Let $\varepsilon > 0$. Then there exists $N \in \N$ such that for $k,m \geq N$, $||f_k - f_m||_{\infty} < \varepsilon/2$. Then for all $x \in S$, $|f_k(x) - f_m(x)| < \varepsilon/2$ for $k,m \geq N$. As $(\R, |\cdot|)$ is a complete metric space, for each $x \in S$ there exists $f(x) \in \R$ such that $f_k(x)$ converges to $f(x)$. Then, taking the limit as $m$ goes to infinity in $|f_k(x) - f_m(x)| < \varepsilon/2$, and using the order properties of limits in $\R$, we have that $|f_k(x) - f(x)| \leq \varepsilon/2$ for all $x \in S$ and $k \geq N$. Thus, for $k \geq N$ we have that $||f_k - f||_{\infty} < \varepsilon$, so $f_k$ converges to $f$ uniformly on $S$, as desired.
\end{proof}

\begin{eg}
    Let $S = [0,1]$ and $f_n(x) = nx(1-x^2)^n$. We claim that $f_n$ converges pointwise to $0$, but not uniformly. If $x = 0,1,$ then $f_n(x) = 0\rightarrow 0$. If $0 < x < 1$, then $0 < 1-x^2 < 1$. Then $1/(1-x^2) > 1$, so there exists $y > 0$ such that $1/(1-x^2) = 1+y$. Then, for $n \geq 2$ we have by the binomial theorem that $(1+y)^n \leq 1 + ny + n(n+1)/2y^2 \leq 1 +ny + n^2y^2/2$. It follows that \begin{align*}
        0 \leq \lim\limits_{n\rightarrow \infty}nx(1-x^2)^n &\leq \lim\limits_{n\rightarrow \infty}\frac{nx}{1+ny+n^2y^2/2} \tag{for some $y > 0$} \\
        &= x\lim\limits_{n\rightarrow \infty}\frac{1}{1/n+y+ny^2/2} \\
        &= x\cdot 0 = 0
    \end{align*}
    as claimed. But, the convergence is not uniform since if $||f_n||_{\infty}\rightarrow 0$, then $|f_n(x_n)|\rightarrow 0$ for any sequence $x_n$ in $[0,1]$. Let $x_n = \frac{1}{\sqrt{n}}$. Then $f_n(x_n) = n\frac{1}{\sqrt{n}}(1-1/n)^n = \sqrt{n}(1-1/n)^n$, which converges to $\infty$ as $\sqrt{n}\rightarrow \infty$ and $(1-1/n)^n\rightarrow e^{-1}$.
\end{eg}

\begin{eg}
    Let $S = [0,b]$ for some $b > 0$. Then $f_n(x) = n\sin(x/n)$ converges uniformly on $S$ but not on $[0,\infty)$. Note if we fix $x$ and take $n$ large enough, then $\sin(x/n) \approx x/n$. Then $$\lim\limits_{n\rightarrow \infty}f_n(x) = \lim\limits_{n\rightarrow \infty}\frac{\sin(x/n)}{x/n}x = x$$ so $f_n(x)\rightarrow x$ for all $x$. We first show $f_n\cancel{\rightarrow_u} x$ on $[0,\infty)$. Suppose it did. Then $|f_n(x_n)-x_n|\rightarrow 0$ for any sequence $x_n$ in $[0,\infty)$. Try $x_n = n^2$. Then $|f_n(n^2) - n^2| = |n\sin(n)-n^2| = n|\sin(n) - n|\rightarrow \infty$, which is a contradiction, so the claim holds. To show uniform convergence on $[0,b]$, note that $\sin(x/n) \leq x/n$ for all $x \geq 0$, so $n\sin(x/n) \leq x$. Let $g(x) = x-n\sin(x/n) \geq 0$. Then $g'(x) = 1-\cos(x/n) \geq 0$. Thus, $g$ is a monotone increasing function on $[0,b]$. Thus, $||g||_{\infty} = g(b) = b-n\sin(b/n)$. Then $||f_n - x||_{[0,b]} = ||g||_{[0,b]} = b-n\sin(b/n)$, which converges to $b-b = 0$ as $n\rightarrow \infty$.
\end{eg}

Note that $[0,\infty) = \bigcup_{b > 0}[0,b]$, sp this gives an example of uniform convergence on many sets, but not on the union of those sets.





\begin{thm}
    Suppose $S \subseteq \R$, and $f_n:S\rightarrow \R$ are continuous functions. If $f_n$ converges uniformly to $f$ on $S$, then $f$ is also continuous on $S$.
\end{thm}
\begin{proof}
    First, fix $\varepsilon > 0$. Then there exists $N \in \N$ such that for $k \geq N$, $||f_k - f||_S < \varepsilon/3$. Let $x \in S$. As $f_N$ is continuous at $x$, there exists $\delta > 0$ such that for $y \in B_{\delta}^*(x)$, $|f_N(y) -f_N(x)| < \varepsilon/3$. Then, for $|x-y| < \delta$ we have that \begin{align*}
        |f(x) - f(y)| &\leq |f(x) - f_N(x)| + |f_N(x) - f_N(y)| + |f_N(y) - f(y)| < 3\varepsilon/3 = \varepsilon
    \end{align*}
    so $f$ is indeed continuous at $x$, and hence on $S$.
\end{proof}


\begin{prop}
    If $f_n$ converges uniformly to $f$ on $S$ and each $f_n$ is uniformly continuous on $S$, then $f$ is uniformly continuous on $S$.
\end{prop}
\begin{proof}
    First, fix $\varepsilon > 0$. As $f_n\rightarrow_uf$, there exists $N \in \N$ such that for $k \geq N$, $||f_k - f||_S < \varepsilon/3$. Then, as $f_N$ is uniformly continuous there exists $\delta > 0$ such that if $|x-y| < \delta$, $|f_N(x) - f_N(y)| < \varepsilon/3$ for all $x,y \in S$. Thus, if $|x-y| < \delta$ and $x,y \in S$, it follows that \begin{align*}
        |f(x) - f(y)| &\leq |f(x) - f_N(x)| + |f_N(x) - f_N(y)| + |f_N(y) - f(y)| \\
        &< 3\varepsilon/3 = \varepsilon
    \end{align*}
    so $f$ is uniformly continuous on $S$, as desired.
\end{proof}

\begin{eg}
    Let $S = [0,1]$ and $f_n(x) = \frac{1}{1+x^n}$. We have seen that $f_n(x) \rightarrow 1$ for $x \in [0,1)$, and $f_n(1)\rightarrow 1/2$, pointwise. Evidently, the convergence is not uniform since each $f_n$ is continuous on $[0,1]$, but the limit function is not.
\end{eg}

\begin{lem}
    Suppose $f_n$ converges uniformly to $f$ on $S$. If each $f_n$ are bounded on $S$, then $f$ is bounded on $S$ as well. Moreover, the sequence is \Emph{uniformly bounded}: $$\sup_{n \in \N}||f_n||_S < \infty$$
\end{lem}
\begin{proof}
    First, there exists $N \in \N$ such that if $k \geq N$, $||f_k - f||_S < 1$, by uniform convergence. Then, for all $x \in S$, $|f(x)| \leq |f(x)-f_N(x)| + |f_N(x)| < ||f_N||_S + 1$. But $f_N$ is assumed bounded on $S$, so $||f_N||_S < \infty$, and hence $||f||_S \leq ||f_N||_S + 1 < \infty$, so $f$ is also bounded on $S$. Now, for all $k \geq N$ we have that $||f_k||_S \leq ||f||_S + 1$. Let $M = \max\{||f_1||_S,...,||f_{N-1}||_S,||f_N||_S + 1\}$. Then it follows that $||f_n||_S \leq M$ for all $n \in \N$, and $M < \infty$ is finite, so $\sup_{n \in \N}||f_n||_S \leq M < \infty$, as desired.
\end{proof}


\begin{thm}
    Suppose that $f_n$ are bounded and integrable functions on $[a,b]$, $f_n \in \mathcal{R}([a,b])$, converging uniformly to $f$ on $[a,b]$. Then $f$ is integrable on $[a,b]$ and $$\lim\limits_{n\rightarrow \infty}\int_a^bf_n(x)dx = \int_a^bf(x)dx$$
\end{thm}
\begin{proof}
    By the previous lemma we have that $f$ is bounded on $[a,b]$. First, if $f$ is integrable on $[a,b]$, then \begin{align*}
        \left|\int_a^bf_n(x)dx - \int_a^bf(x)dx\right| &\leq \int_a^b|f_n(x)-f(x)|dx \\
        &\leq \int_a^b||f_n-f||_{\infty}dx \\
        &= (b-a)||f_n-f||_{\infty}
    \end{align*}
    which goes to $0$ as $n\rightarrow \infty$ by uniform convergence. Thus, all we need to show is $f$ is integrable. Fix $\varepsilon > 0$. Then there exists $N \in \N$ such that $||f_n - f|| < \frac{\varepsilon}{3(b-a)}$. Since each $f_n$ is integrable, there exists a partition $P_n \in \prod([a,b])$ with $$U(f_n,P_n) - L(f_n,P_n) < \frac{\varepsilon}{3}$$ Write $P_n = \{a=x_{0,n},x_{1,n},...,x_{M_n,n} = b\}$. Let $J_{k,n} = [x_{k,n},x_{k+1,n}]$ for $0 \leq k \leq N_n$. Then $||f-f_n||_{J_k} < \frac{\varepsilon}{3(b-a)}$ for $n \geq N$. Then $$||f||_{J_k} \leq \frac{\varepsilon}{3(b-a)} + ||f_n||_{J_k}$$ and $$||f_n||_{J_k} \leq \frac{\varepsilon}{3(b-a)}$$, so $$|\sup_{J_k}(f) - \sup_{J_k}(f_n)| \leq \frac{\varepsilon}{3(b-a)}$$ for all $n \geq N$. It follows that \begin{align*}
        |U(f,P_n) - U(f_n,P_n)| &= \left|\sum_{k=0}^{M_n-1}(\sup_{J_k}(f) - \sup_{J_k}(f_n))\ell(J_k)\right| \\
        &\leq \sum_{k=0}^{M_n - 1}|\sup_{J_k}(f) - \sup_{J_k}(f_n)|\ell(J_k) \\
        &< \sum_{k=0}^{M_n-1}\frac{\varepsilon}{3(b-a)}\ell(J_k) \\
        &= \frac{\varepsilon}{3(b-a)}\ell([a,b]) = \frac{\varepsilon}{3}
    \end{align*}
    Similarly, $|L(f,P_n) - L(f_n,P_n)| < \varepsilon/3$. Finally, \begin{align*}
        U(f,P_n) - L(f,P_n) &= U(f,P_n) - U(f_n,P_n) + U(f_n,P_n) - L(f_n,P_n) + L(f_n,P_n) - L(f,P_n) \\
        &\leq |U(f,P_n) - U(f_n,P_n)| + |U(f_n,P_n) - L(f_n,P_n)| + |L(f_n,P_n) - L(f,P_n)|\\
        &< 3\cdot \frac{\varepsilon}{3} = \varepsilon
    \end{align*}
    Thus, $f$ is Riemann integrable on $[a,b]$.
\end{proof}




\begin{rmk}
    Although these last two theorems are great successes, differentiability sadly fails. Even if each $f_n$ is differentiable and $\{f_n\}$ converges uniformly to $f$, it need not be the case that $f$ is differentiable. Moreover, even if $f$ is itself differentiable, it need not be the case that \begin{equation*}
        f'(x) = \lim\limits_{n\rightarrow \infty}f_n'(x)
    \end{equation*}
\end{rmk}

\begin{eg}[Counter Example 3]
    Consider the family of functions \begin{equation*}
        f_n(x) = \frac{1}{n}\sin(n^2x)
    \end{equation*}
    then $\{f_n\}$ converges uniformly to the function $f(x) = 0$, but \begin{equation*}
        f_n'(x) = n\cos(n^2x)
    \end{equation*}
    and $\lim\limits_{n\rightarrow\infty}n\cos(n^2x)$ does not even always exist (for example if $x =0$).
\end{eg}


\begin{thm}
    Suppose that $\{f_n\}$ is a sequence of functions which are differentiable on $[a,b]$, with integrable derivatives $f'_n$, and that $\{f_n\}$ converges (pointwise) to $f$. Suppose, moreover, that $\{f_n'\}$ converges uniformly on $[a,b]$ to some continuous function $g$. Then $f$ is differentiable and \begin{equation*}
        f'(x) = \lim\limits_{n\rightarrow \infty}f_n'(x)
    \end{equation*}
\end{thm}
\begin{proof}
    Applying Theorem $1$ to the interval $[a,x]$, we see that for each $x$ we have \begin{align*}
        \int_a^xg &= \lim\limits_{n\rightarrow \infty}\int_a^xf'_n \\
        &= \lim\limits_{n\rightarrow \infty}[f_n(x) - f_n(a)] \tag{by \ref{thmname:FTC2}} \\
        &= f(x) - f(a)
    \end{align*}
    Since $g$ is continuous, it follows that $f'(x) = g(x) = \lim\limits_{n\rightarrow \infty}f_n'(x)$ for all $x$ in the interval $[a,b]$, by \ref{thmname:FTC}.
\end{proof}

\begin{eg}
    Consider $\int_0^{\pi}\frac{n+\sin x}{3n+\sin^2(nx)}dx$. Observe that \begin{align*}
        \left|\frac{n+\sin x}{3n+\sin^2(nx)} - \frac{1}{3}\right| &= \left|\frac{3n+3\sin x-3n-\sin^2(nx)}{3(3n+\sin^2(nx))}\right| \\
        &\leq \frac{|3\sin x - \sin^2(nx)|}{9n} \leq \frac{3+1}{9n} = \frac{4}{9n}
    \end{align*}
    for all $x \in [0,\pi]$. Thus, $||f_n-\frac{1}{3}||_{[0,\pi]} \leq \frac{4}{9n}\rightarrow 0$, so it converges uniformly. It follows that \begin{align*}
        \lim\limits_{n\rightarrow \infty}\int_0^{\pi}\frac{n+\sin x}{3n+\sin^2(nx)}dx &= \int_0^{\pi}\lim\limits_{n\rightarrow \infty}\frac{n+\sin x}{3n+\sin^2(nx)}dx \\
        &= \int_0^{\pi}\frac{1}{3}dx \\
        &= \frac{\pi}{3}
    \end{align*}
\end{eg}

\begin{defn}
    The series $\sum\limits_{n=1}^{\infty}f_n$ \Emph{converges uniformly} (more formally, the sequence $\{f_n\}$ is \Emph{uniformly summable}) \Emph{to $f$ on $A$}, if the sequence \begin{equation*}
        f_1, f_1+f_2,f_1+f_2+f_3,...
    \end{equation*}
    converges uniformly to $f$ on $A$.
\end{defn}

\begin{defn}[Series of Functions (alt.)]
    Suppose $S$ is a subset of $\R$ and $f_n:S\rightarrow \R$. We say that the series $\sum_{n=1}^{\infty}f_n(x)$ \Emph{converges pointwise} if the sequence of partial sums $s_k(x) = \sum_{n=1}^kf_n(x)$ converges pointwise to a function $f:S\rightarrow \R$. In this case, write $f(x) = \sum_{n=1}^{\infty}f_n(x)$. We say that the series $\sum_{n=1}^{\infty}f_n(x)$ \Emph{converges uniformly} if $s_k$ converges uniformly to $f$ on $S$.
\end{defn}



\begin{cor}
    Let $\sum\limits_{n=1}^{\infty}f_n$ converge uniformly to $f$ on $[a,b]$. \begin{enumerate}
        \item If each $f_n$ is continuous on $[a,b]$, then $f$ is continuous on $[a,b]$.
        \item If $f$ and each $f_n$ is integrable on $[a,b]$, then \begin{equation*}
                \int_a^bf = \int_a^b\sum_{n=1}^{\infty}f_n = \sum\limits_{n=1}^{\infty}\int_a^bf_n
        \end{equation*}
    \end{enumerate}
    Moreover, if $\sum\limits_{n=1}^{\infty}f_n$ converges (pointwise) to $f$ on $[a,b]$, each $f_n$ has an integrable derivative $f_n'$ and $\sum\limits_{n=1}^{\infty}f'_n$ converges uniformly on $[a,b]$ to some continuous function, then \begin{enumerate}
        \item[3.] $f'(x) = \sum\limits_{n=1}^{\infty}f_n'(x)$   for all $x \in [a,b]$
    \end{enumerate}
\end{cor}
\begin{proof}
    Let $\{s_n\}$ be the sequence of partial sums of the $\{f_n\}$. Then since each $f_n$ is continuous, so is each $s_n$. Then as $\{s_n\}$ converges uniformly to $f$ we have by a previous theorem that $f$ is also continuous on $[a,b]$. Next, since each $f_n$ is integrable on $[a,b]$, so is each $s_n$. Then as $\{s_n\}$ converges uniformly to $f$ we have that \begin{align*}
        \int_a^bf &= \lim\limits_{n\rightarrow \infty}\int_a^bs_n \\
        &= \lim\limits_{n\rightarrow\infty}t_n \\
        &= \sum\limits_{n=1}^{\infty}\int_a^bf_n
    \end{align*}
    where $\{t_n\}$ is the sequence such that \begin{equation*}
        t_n = \sum\limits_{i=1}^n\int_a^bf_n = \int_a^bs_n
    \end{equation*}

    Finally, suppose $s_n$ converges (pointwise) to $f$ on $[a,b]$, and each $f_n$ has an integrable derivative $f_n'$. Then each $s_n$ has an integrable derivative $s_n'$ on $[a,b]$ by the linearity of the derivative and integral operators. Moreover, suppose $s_n'$ converges uniformly on $[a,b]$ to some continuous function $g$. Then it follows that for all $x \in [a,b]$ \begin{equation*}
        f'(x) = \lim\limits_{n\rightarrow \infty}s_n'(x) = \sum\limits_{n=1}^{\infty}f_n'(x)
    \end{equation*}
\end{proof}

\begin{eg}
    Let $S = [-r,r]$ with $0 < r < 1$. Let $f(t) = \sum_{n=0}^{infty}(-r)^n$. Then as $t \in [-r,r] \subset ( -1,1)$ we have $f(t) = \frac{1}{t}$. Convergence is uniform on $[-r,r]$ since $|t| \leq r$, so $|(-t)^n| \leq r^n$, and $\sum_{n=0}^{\infty}r^n$ converges. By the last corollary, we have that for $x \in [-r,r]$, \begin{align*}
        \log(1+x) &= \int_0^x\frac{1}{1+t}dt \tag{by the FTC} \\
        &= \int_0^x\sum_{n=0}^{\infty}(-t)^ndt \\
        &= \sum_{n=0}^{\infty}\int_0^x(-1)^nt^ndt \\
        &= \sum_{n=0}^{\infty}(-1)^n\frac{x^{n+1}}{n+1} \tag{by the FTC}
    \end{align*}
    Thus, $\log(1+x) = \sum_{n=0}^{\infty}(-1)^n\frac{x^{n+1}}{n+1}$ for all $x \in (-1,1)$
\end{eg}



\begin{namthm}[The Weierstrass M-Test]\label{thmname:mtest}
    Let $\{f_n\}$ be a sequence of functions defined on $A$, and suppose that $\{M_n\}$ is a sequence of numbers such that \begin{equation*}
        |f_n(x)|\leq M_n,\forall x \in A
    \end{equation*}
    Suppose moreover that $\sum\limits_{n=1}^{\infty}M_n$ converges. Then for each $x$ in $A$ the series $\sum\limits_{n=1}^{\infty}f_n(x)$ converges absolutely, and $\sum\limits_{n=1}^{\infty}f_n$ converges uniformly on $A$ to the function \begin{equation*}
        f(x) = \sum\limits_{n=1}^{\infty}f_n(x)
    \end{equation*}
\end{namthm}
\begin{proof}
    For each $x \in A$, the series $\sum\limits_{n=1}^{\infty}|f_n(x)|$ converges by \ref{thmname:comptest}; consequently $\sum\limits_{n=1}^{\infty}f_n(x)$ converges absolutely. Moreover, for all $x \in A$ we have \begin{align*}
        \left|f(x) - \sum\limits_{i=1}^{N}f(x)\right| &= \left|\sum\limits_{n=N+1}^{\infty}f_n(x)\right| \\
        &\leq \sum\limits_{n=N+1}^{\infty}|f_n(x)| \\
        &\leq \sum\limits_{n=N+1}^{\infty}M_n
    \end{align*}
    Since $\sum\limits_{n=1}^{\infty}M_n$ converges, the number $\sum\limits_{n=N+1}^{\infty}M_n$ can be made as small as desired (by \ref{thmname:cauchcrit}), by choosing $N$ sufficiently large.
\end{proof}


\begin{eg}
    The series $\sum_{n=0}^{\infty}\frac{x^n}{n!}$ converges uniformly on $[-R,R]$ for any $R > 0$, but not on $\R$. If $-R \leq x \leq R$, then $\left|\frac{x^n}{n!}\right| \leq \frac{R^n}{n!} =: M_n$. Note that $\sum_{n=0}^{\infty}M_n$ converges by the ratio test. This implies by the Weierstrass M-test that $\sum_{n=0}^{\infty}\frac{x^n}{n!}$ converges uniformly on $[-R,R]$. Note if $\sum_nf_n$ converges uniformly on $S$ to some function $s$, then $f_n\rightarrow_u 0$. In particular, $||f_n||_{\infty}\rightarrow 0$. But, $||x^n/n!||_{\R} = \infty$, so the series cannot be uniformly convergent on all of $\R$.
\end{eg}

The converse to the uniform limit of the terms needing to go to zero for the series to converge is false. For example, if $f_n(x) = 1/n, x \in \R$, then $||f_n||_{\R} = 1/n\rightarrow 0$, but $\sum_{n=1}^{\infty}\frac{1}{n}$ diverges (the harmonic series).

\begin{eg}
    Recall we saw that $\sum_{n=0}^{\infty}\frac{x^n}{n!}$ converges uniformly on $[-R,R]$ for any $R > 0$ by Weierstrass M-test. Since $\sum_{n=0}^N\frac{x^n}{n!}$ are continuous, $\sum_{n=0}^{\infty}\frac{x^n}{n!}$ is continuous on $[-R,R]$ for all $R$ so it is continuous on $\R$.
\end{eg}


\begin{thm}[Riemann 1861; Weierstrass 1872; du Bois-Reymond 1875]
    There is a continuous function on $\R$ that is nowhere differentiable.
\end{thm}
\begin{proof}
    Let $g(x) = |x|$ on $[-1,1]$, and continuously and periodically extend to $\R$ so $g(x+2) =g(x)$ for all $x$. Let $r,s \in \R$. If $|r-s| > 1$, then $|r-s| > 1 \geq |g(r) - g(s)|$, as $-1 = 0-1\geq g(r)-g(s) \leq 1 - 0 = 1$. Otherwise, suppose $|r-s| \leq 1$. Without loss of generality suppose $r \geq s$. If there does not exist $s \leq n \leq r$ for some $n \in \Z$, then there exists $n \in \Z$ such that $n \leq s \leq r \leq n+1$. It follows that either $g(r) - g(s) = r-n-(s-n) = r-s$, or $g(r) - g(s) = -(r-n-1)+(s-n-1) = s-r$. In either case $|g(r) - g(s)| = |r-s|$. Otherwise, there exists $n \in \Z$ such that $n-1 \leq s \leq n \leq r \leq n+1$. Then $$g(r) - g(s) = (r-n)-|s-n| = r-n-n+s=r+s-2n\leq r+s-2s = r-s = |r-s|$$ or $$g(r)-g(s) = |r-(n+1)| -(s-(n-1)) = n+1-r-s+n-1 = 2n-r-s \leq 2r-r-s = r-s = |r-s|$$ Thus, in any case we have $|g(r) - g(s)| \leq |r-s|$ for all $r,s \in \R$. This says that $g$ is Lipshitz continuous, and in particular is uniformly continuous on $\R$. Now, for all $n \in \N$, let $g_n(x) = \frac{3^n}{4^n}g(4^nx)$. Notice that $|g_n(x)| \leq \left(\frac{3}{4}\right)^n$, so the sum $\sum_n g_n$ converges uniformly on $\R$ by the Weierstrass M-test. Let $f(x) = \sum_{n=0}^{\infty}g_n(x)$, where $g_0(x) = g(x)$. Then $f$ is continuous on $\R$ as the $g_n$ are continuous and they converge uniformly (in particular, $f$ is uniformly continuous on $\R$). Fix $x \in \R$ and $m \in \N$. Define $\delta_m = \pm\frac{1}{2}\cdot\frac{1}{4^m}$, where $+$ or $-$ is selected so that there is no integer between $4^mx$ and $4^m(x+\delta)$, which has distance $\frac{1}{2}$. We have $|4^mx - 4^m(x+\delta_m)| = \frac{1}{2}$. We will show $\left|\frac{f(x+\delta_m) - f(x)}{\delta_m}\right|\rightarrow \infty$ as $m\rightarrow \infty$, so $\delta_m\rightarrow 0$. We compute \begin{align*}
        f(x+\delta_m) - f(x) &= \sum_{n=0}^{\infty}g_n(x+\delta_m) - g_n(x) \\
        &= \sum_{n=0}^{\infty}\frac{3^n}{4^n}\left[g(4^nx+4^n\delta_m) - g(4^nx)\right] 
    \end{align*}
    We have three cases on each term: \begin{itemize}
        \item[(i)] $n > m$: Then $4^n\delta_m = \frac{\pm 4^{n-m}}{2}$ is an even integer. Since $g(t+2)=g(t)$ for all $t$, $g(4^nx+4^n\delta_m) = g(4^nx)$. So $$f(x+\delta_m) - f(x) = \sum_{n=0}^m\frac{3^n}{4^n}[g(4^nx+4^n\delta_m) - g(4^nx)]$$
        \item[(ii)] $n < m$: We have $|g(r) - g(s)| \leq |r-s|$ for all $r,s \in \R$. Thus $$|g(4^nx+4^n\delta_m) - g(4^nx)| \leq 4^n|\delta_m|$$
        \item[(iii)] $n=m$: There is no integer between $4^nx+4^n\delta_n$ and $4^nx$ which means $g$ maps these two points to the same line segment. Then $|g(4^nx+4^n\delta_n) - g(4^nx)| = |4^n\delta_n| = \frac{1}{2}$
    \end{itemize}
    Now we compute \begin{align*}
        \left|\frac{f(x+\delta_m)-f(x)}{\delta_m}\right| &= \left|\sum_{n=0}^m{3^n}{4^n}\frac{g(4^nx+4^n\delta_m) - g(4^nx)}{\delta_m}\right| \\
        &\geq \left|\frac{3^m}{4^m}\frac{1/2}{\delta_m}\right| -\sum_{n=0}^{m-1}\left|\frac{3^n(g(4^nx+4^n\delta_m) - g(4^nx))}{4^n\delta_m}\right| \\
        &\geq 3^m - \sum_{n=0}^{m-1}\frac{4^n|\delta_m|}{|\delta_m|}\frac{3^n}{4^n} \\
        &= 3^m - \sum_{n=0}^{m-1}3^n \\
        &= 3^m - \frac{1-3^m}{1-3} \\
        &= \frac{2\cdot3^m+1-3^m}{2} \\
        &= \frac{1}{2}(3^m+1)\rightarrow \infty
    \end{align*}
    as $m\rightarrow \infty$, as desired. Thus, as if $f$ was differentiable this limit would be its derivative, so $f$ is not differentiable at any $x \in \R$.
\end{proof}

\begin{eg}
    Let $S = (0,1)$ and $f(x) = \sum_{n=0}^{\infty}\frac{1}{n^2x+1}$. Determine where $f$ is continuous and where the convergence of the series is uniform. Note that $f_n(1/n^2) = \frac{1}{1+1} = \frac{1}{2}\cancel{\rightarrow} 0$, so $f_n$ does not converge uniformly to $0$ on $S$, and hence the sum does not converge uniformly. Let $0 < a < 1$, and consider $f$ on $[a,1)$. Then $|f_n(x)| = \frac{1}{n^2x+1} \leq \frac{1}{n^2a+1}$ for $x \in [a,1)$. But $\sum_{n=0}^{\infty}\frac{1}{n^2a+1} < \infty$, so by the Weierstrass M-test, $\sum_{n=0}^{\infty}f_n$ converges uniformly on $[a,1)$. Since each $f_n$ is continuous on $[a,1)$, this implies $f$ is continuous on $[a,1)$. Finally, $0 < a < 1$ is arbitrary, so $f(x) = \sum_{n=0}^{\infty}f_n(x)$ is continuous on $S = (0,1)$.
\end{eg}


\subsection{Power Series}


\begin{defn}
    An infinite sum of functions of the form \begin{equation*}
        f(x) = \sum\limits_{n=0}^{\infty}a_n(x-a)^n
    \end{equation*}
    is called a \Emph{power series centered at $a$}. One especially important family of power series are those of the form \begin{equation*}
        \sum\limits_{n=0}^{\infty}\frac{f^{(n)}(a)}{n!}(x-a)^n
    \end{equation*}
    where $f$ is some infinitely differentiable function at $a$; this series is called the \Emph{Taylor series for $f$ at $a$}.
\end{defn}

A power series is a formal notion. It may not converge for many values of $x$. It always converges at $x = 0$ ($a$, its center).


\begin{rmk}
    Given a function $f$ infinitely differentiable at $a$, we have for $x \in \R$ that \begin{equation*}
        f(x) = \sum\limits_{n=0}^{\infty}\frac{f^{(n)}(a)}{n!}(x-a)^n
    \end{equation*}
    if and only if the remainder terms satisfy $\lim\limits_{n\rightarrow \infty}R_{n,a}(x) = 0$.
\end{rmk}


\begin{thm}
    Suppose that the series \begin{equation*}
        f(x_0) = \sum\limits_{n=0}^{\infty}a_nx_0^n
    \end{equation*}
    converges, and let $a$ be any number with $0 < a < |x_0|$. Then on $[-a,a]$ the series \begin{equation*}
        f(x) = \sum\limits_{n=0}^{\infty}a_nx^n
    \end{equation*}
    converges uniformly (and absolutely). Moreover, the same is true for the series \begin{equation*}
        g(x) = \sum\limits_{n=1}^{\infty}na_nx^{n-1}
    \end{equation*}
    Finally, $f$ is differentiable and \begin{equation*}
        f'(x) = \sum\limits_{n=1}^{\infty}na_nx^{n-1}
    \end{equation*}
    for all $x$ with $|x| < |x_0|$.
\end{thm}
\begin{proof}
    First, since $\sum\limits_{n=0}^{\infty}a_nx_0^n$ converges, $\lim\limits_{n\rightarrow \infty}a_nx_0^n = 0$. Hence, the sequence $\{a_nx_0^n\}$ is surely bounded: there is some number $M$ such that \begin{equation*}
        |a_nx_0|^n = |a_n|\cdot|x_0|^n \leq M
    \end{equation*}
    for all $n$. Now if $x$ is in $[-a,a]$, then $|x| \leq |a|$, so \begin{align*}
        |a_nx^n| &= |a_n|\cdot |x|^n \\
        &\leq |a_n|\cdot|a|^n \\
        &= |a_n|\cdot|x_0|^n\cdot\left|\frac{a}{x_0}\right|^n \\
        &\leq M\left|\frac{a}{x_0}\right|^n
    \end{align*}
    But $|a/x_0| < 1$, so the (geometric) series \begin{equation*}
        \sum\limits_{n=0}^{\infty}M\left|\frac{a}{x_0}\right|^n = M\sum\limits_{n=0}^{\infty}\left|\frac{a}{x_0}\right|^n
    \end{equation*}
    converges. Choosing $M\cdot|a/x_0|^n$ as the number $M_n$ in \ref{thmname:mtest}, it follows that $\sum\limits_{n=0}^{\infty}a_nx^n$ converges uniformly on $[-a,a]$.


    To prove the same assertion for $g(x) = \sum\limits_{n=1}^{\infty}na_nx^{n-1}$ notice that \begin{align*}
        |na_nx^{n-1}| &= n|a_n|\cdot |x^{n-1}| \\
        &\leq n|a_n|\cdot|a^{n-1}| \\
        &= \frac{|a_n|}{|a|}\cdot|x_0|^nn\left|\frac{a}{x_0}\right|^n \\
        &\leq \frac{M}{|a|}n\left|\frac{a}{x_0}\right|^n
    \end{align*}
    Since $|a/x_0| < 1$, the series \begin{equation*}
        \sum\limits_{n=1}^{\infty} \frac{M}{|a|}n\left|\frac{a}{x_0}\right|^n = \frac{M}{|a|}\sum\limits_{n=1}^{\infty}n\left|\frac{a}{x_0}\right|^n
    \end{equation*}
    converges (by an application of the Ratio Tes). Another appeal to \ref{thmname:mtest} proves that $\sum\limits_{n=1}^{\infty}na_nx^{n-1}$ converges uniformly on $[-a,a]$.

    Finally, our corollary proves, first that $g$ is continuous, and then that \begin{equation*}
        f'(x) = g(x) = \sum\limits_{n=1}^{\infty}na_nx^{n-1}
    \end{equation*}
    for all $x \in [-a,a]$. Since we could have chosen any $a$ with $0 < a < |x_0|$, this result holds for all $x$ with $|x| < |x_0|$.
\end{proof}

\begin{thm}[Ratio and Root Test]
    If $\sum_{n=0}^{\infty}a_n$ for $a_n \in \C$ non-zero, then we have \begin{itemize}
        \item If $\lim\sup\frac{|a_{n+1}|}{|a_n|} < 1$, then $\sum_na_n$ converges absolutely
        \item If $\lim\sup\frac{|a_{n+1}|}{|a_n|} > 1$, then $\sum_na_n$ diverges
        \item If $\lim\sup|a_n|^{\frac{1}{n}} < 1$, then $\sum_na_n$ converges absolutely
        \item If $\lim\sup|a_n|^{\frac{1}{n}} > 1$, then $\sum_na_n$ diverges
    \end{itemize}
\end{thm}

\begin{eg}
    If $a_n = 1$ for all $n$, we get the geometric series $\sum_{n=0}^{\infty}x^n = \frac{1}{1-x}$ provided $|x| < 1$.
\end{eg}
The natural domain of a power series $f(x) = \sum_{n=0}^{\infty}a_nx^n$ is all $x$ for which the sum converges.

\begin{prop}[Radius of Convergence]
    Let $\sum_na_nx^n$ be a power series and define $\alpha:= \lim\sup|a_n|^{1/n}$. Define $$R = \left\{\begin{array}{cc}\frac{1}{\alpha} & 0 < \alpha < \infty \\ \infty & \alpha = 0 \\ 0 & \alpha = \infty \end{array}\right.$$
    which we call the \Emph{radius of convergence} of the power series. Then $\sum_na_nx^n$ converges absolutely for $|x| < R$. If $R = 0$, it converges only when $x = 0$. If $R = \infty$, it converges absolutely on all of $\R$. The series may or may not converge when $|x| = R$.
\end{prop}
\begin{proof}
    Let $b_n = a_nx^n$, and \begin{align*}
        \beta = \lim\sup|b_n|^{1/n} = \lim\sup|a_n|^{1/n}|x^n|^{1/n} = |x|\alpha
    \end{align*}
    Apply the root test to $\sum_{n=0}^{\infty}b_n$: \begin{itemize}
        \item[(i)] If $\alpha = 0$ then $\beta = 0 <1$, so the series converges absolutely for all $x \in \R$
        \item[(ii)] If $\alpha = \infty$, $\beta = 0$ if $x = 0$ and $\infty$ otherwise, so the series converges absolutely only at $x  =0$
        \item[(iii)] If $0 < \alpha < \infty$, $\beta = |x| \alpha < 1$ if and only if $|x| < \frac{1}{\alpha} = R$, so the series converges absolutely for $|x| < R$ and diverges for $|x| > R$.
    \end{itemize}
\end{proof}

\begin{cor}
    If $\lim\limits_{n\rightarrow \infty}\frac{|a_n|}{|a_{n+1}|}$ exists, it equals $R$ above.
\end{cor}

\begin{eg}
    Consider $\sum_{n=0}^{\infty}\frac{n!x^n}{n^n}$, so $a_n = \frac{n!}{n^n}$. Observe that \begin{align*}
        \lim\limits_{n\rightarrow \infty}\frac{a_n}{a_{n+1}} = \lim\limits_{n\rightarrow \infty}\frac{(n+1)^n}{n^n} = \lim\limits_{n\rightarrow \infty}\left(1+\frac{1}{n}\right)^n = e = R
    \end{align*}
    For $x = \pm e$ we must use stirling's approximation $n! \approx \sqrt{2\pi n}\frac{n^n}{e^n}$, which becomes exact in the limit.
\end{eg}


\begin{prop}
    Suppose $\sum_{n}a_nx^n$ has radius of convergence $R > 0$. Then $\sum_na_nx^n$ converges uniformly on any set of the form $[-K,K]$ for $0 \leq K < R$.
\end{prop}
\begin{proof}
    Suppose $x \in [-K,K]$. Then $|a_nx^n| \leq |a_n|K^n$. But $\sum_{n=0}^{\infty}|a_n|K^n$ converges since $K \in (-R,R)$. Thus, by the Weierstrass M test $\sum_{n=0}^{\infty}a_nx^n$ converges uniformly on $[-K,K]$.
\end{proof}
In particular, this tells us that $\sum_{n=0}^{\infty}a_nx^n$ is continuous on $[-K,K]$ for any $0 < K < R$, so it is continuous on $(-R,R)$.

\begin{prop}
    Suppose $f(x) = \sum_na_nx^n$ has radius of convergence $R > 0$. Then $f$ is differentiable on $(-R,R)$ and $f'(x) = \sum_nna_nx^{n-1}$. Moreover, $f'$ also has radius of convergence $R$.
\end{prop}
\begin{proof}
    We have shown previously that if $f_n\rightarrow f$ pointwise, $f'_n$ are continuous, and $f'_n\rightarrow_ug$ for some $g$, then $f$ is differentiable and $f' = g$. Let $s_m(x) = \sum_{n=0}^ma_nx^n$, which converges uniformly to $\sum_{n=0}^{\infty}a_nx^n=f(x)$ on $[-K,K]$ for any $0 \leq K < R$. Note $s_m$ is differentiable with $s_m'(x) = \sum_{n=1}^mna_nx^{n-1}$, which converges uniformly to $\sum_{n=1}^{\infty}na_nx^{n-1} =: g(x)$ on $[-K,K]$ for all $0 \leq K < R$ as $\frac{1}{R'} = \lim\sup\frac{|(n+1)a_{n+1}|}{|na_n|} = \lim\sup\frac{|a_{n+1}|}{|a_n|} = \frac{1}{R}$, so $g$'s radius of convergence is also $R$. This implies that $f$ is differentiable on $(-K,K)$ and $f'(x) = g(x)$ for all $x \in (-K,K)$. As this holds for all $0 \leq K < R$, this implies $f'(x) = g(x)$ on $(-R,R)$
\end{proof}

\begin{cor}
    The power series $f(x) = \sum_na_nx^n$ with radius of convergence $R > 0$ has derivatives of all orders on $(-R,R)$ and $f^{(k)}(x) = \sum_{n\geq k}\frac{n!}{(n-k)!}a_nx^{n-k}$ on $(-R,R)$.
\end{cor}
\begin{proof}
    From the previous proposition we have $f'(x) = \sum_{n=1}^{\infty}na_nx^{n-1}$ with radius of convergence $R$. Applying this result repeatedly we have $f^{(k)}(x) = \sum_{n=k}^{\infty}\frac{n!}{(n-k)!}a_nx^{n-k}$ has radius of convergence $R$.
\end{proof}

Note that in the formula above, it follows that $f^{(k)}(0) = k!a_k$, so $a_k = \frac{f^{(k)}(0)}{k!}$.

\begin{cor}
    If $\sum_na_nx^n = \sum_nb_nx^n$ for all $x \in (-R,R)$, where $R$ is the radius of convergence of both series, then $a_n = b_n$ for all $n$.
\end{cor}
Using the remark above, letting $f(x) = \sum_na_nx^n = \sum_nb_nx^n$, we have $a_n = \frac{f^{(n)}(0)}{n!} = b_n$ for all $n$.

\begin{defn}[Power Series Representation]
    A function $f:S\rightarrow \R$ has a \Emph{power series representation on $S$} if there exists a sequence $(a_n)$ such that $$f(x) = \sum_{n=0}^{\infty}a_nx^n$$ for all $x \in S$. In particular, the series on the right hand side above converges for all $x \in S$.
\end{defn}

We recall Taylor's Theorem:

\begin{thm}[Taylor's Theorem]
    Suppose $f$ is $(n+1)$ times differentiable on $(a,b)$ containing $0$, then $$f(x) = \underbrace{\sum_{k=0}^n\frac{f^{(k)}(0)}{k!}x^k}_{\text{Taylor Polynomial } P_n(x)} + \underbrace{\frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}}_{\text{Taylor remainder }R_n(x)}$$ for somce $c \in (0,x)$ (or $(x,0)$). We get $|f(x) - P_n(x)| = |R_n(x)|$. If $R_n(x)\rightarrow 0$ as $n\rightarrow \infty$, then $f(x) = \lim\limits_{n\rightarrow \infty}P_n(x) = \sum_{k=0}^{\infty}\frac{f^{(k)}(0)}{k!}x^k$.
\end{thm}

This result gives a sufficient condition for a function admitting a power series representation. Note that if $f$ admits a power series representation, it automatically has derivatives of all orders on the radius of convergence.

\begin{eg}
    Consider $$f(x) = \left\{\begin{array}{cc} e^{-1/x^2} & x \neq 0 \\ 0 & x =0\end{array}\right.$$ $f$ has derivatives of all orders at $x = 0$: $$f'(0) = \lim\limits_{h\rightarrow 0}\frac{e^{-1/h^2}}{h} = \lim\limits_{h\rightarrow 0}\frac{2h}{e^{1/h^2}} = 0$$ Similarly, $f^{(k)}(0) = 0$ for all $k$. So, $\sum_{n=0}^{\infty}\frac{f^{(n)}(0)}{n!}x^n = 0$, which converges for all $x$ but only agrees with $f(x)$ at $x = 0$. So $f$ has only a power series representation at $x = 0$.
\end{eg}

\begin{thm}[Abel]
    Suppose the power series $\sum_na_nx^n$ has radius of convergence $R \in (0,\infty)$. If $\sum_na_nx^n$ converges at $x = R$, then $\sum_na_nx^n$ converges uniformly on $[0,R]$. Similarly, if $\sum_na_nx^n$ converges at $x = -R$, then $\sum_na_nx^n$ converges uniformly on $[-R,0]$.
\end{thm}
\begin{proof}
    By rescaling if necessary, we can assume $R = 1$ (i.e. replace $f(x)$ with $f_R(x) = f(x/R)$) We use the Cauchy criterion for uniform convergence. Fix $\varepsilon > 0$. We have convergence at $x = 1$, so there exists $N \in \N$ such that $m \geq N$ implies $\left|\sum_{n=m}^{m+k}a_n\right| < \varepsilon$ for all $k \in \N$. For $m \geq N$ and for each $j \in \N\cup\{0\}$, define $s_j = \sum_{n=m}^{m+j}a_n$, so $|s_j| < \varepsilon$ for all $j$. Then write \begin{align*}
        \sum_{n=m}^{m+k}a_nx^n &= s_0x^m + \sum_{n=1}^k(s_n-s_{n-1})x^{m+n} \\
        &= \sum_{n=0}^{k-1}s_n(x^{m+n}-x^{m+n+1}) + s_kx^{m+k} \\
        &= x^m\left[ \sum_{n=0}^{k-1}s_n(x^{n}-x^{n+1}) + s_kx^{k}\right] \\
        &=x^m\left[ \sum_{n=0}^{k-1}s_nx^n(1-x) + s_kx^{k}\right] \\
        &= x^m(1-x)\sum_{n=0}^{k-1}s_nx^n + s_kx^{m+k}
    \end{align*}
    For $x \in [0,1]$ \begin{align*}
        \left|\sum_{n=m}^{m+k}a_nx^n\right| &\leq |x|^m|1-x|\sum_{n=0}^{k-1}|s_n||x|^n + |s_k||x|^{m+k} \\
        &< x^m(1-x)[\varepsilon+\varepsilon x+...+\varepsilon x^{k-1}] + \varepsilon x^{m+k} \\
        &= \varepsilon x^m(1-x^k) + \varepsilon x^{m+k} \\
        &= \varepsilon x^m < \varepsilon
    \end{align*}
    as desired. The proof for $[-1,0]$ is similar.
\end{proof}


We now investigate power series in the complex numbers. First, recall that if $r \in \C$, $|r| < 1$, then we have the geometric series \begin{equation*}
    \sum_{k=0}^{\infty}r^k = \frac{1}{1-r}
\end{equation*}

\begin{defn}
    A \Emph{power series} (in $\C$) is a series of functions of the form \begin{equation*}
        f(z) = \sum_{k=0}^{\infty}a_kz^k,\;\;\;a_k \in \C, z \in \C
    \end{equation*}
\end{defn}
Note by definition $f(z) = \lim\limits_{n\rightarrow \infty}\sum_{k=0}^na_kz^k$.

\begin{prop}\label{prop:3.3.1}
    If $f(z) = \sum_{k=0}^{\infty}a_kz^k$ is defined for some $z_0 \in \C, z_0 \neq 0$, then $f(z)$ converges absolutely and uniformly for all $|z| < |z_0|$.
\end{prop}
\begin{proof}
    Note that if $|z| < |z_0|$, $|a_kz^k| < |a_k||z_0|^k$ for all $k \geq 0$. Further, as $\sum_{k=0}^na_kz_0^k$ is convergent, it is Cauchy so $|a_kz^k|\rightarrow 0$, and in particular $|a_kz_0^k| \leq C$ for all $ k\geq 0$ for some $C \in \R$. Then $0 \leq \frac{|z|}{|z_0|} = r < 1$. So $$\left|\sum_{k=0}^na_kz^k\right| \leq \sum_{k=0}^n|a_kz_0^k|r^k \leq C\sum_{k=0}^nr^k \leq \frac{C}{1-r}$$ Thus $\sum_{k=0}^n|a_kz^k|$ is a bounded increasing sequence, and so converges. In particular, letting $M_k = Cr^k = C\frac{|z_1|^k}{|z_0|^k}$, we have that for all $|z| < |z_1| < |z_0|$, $|a_kz^k| \leq M_k$ and $\sum_{k=0}^{\infty}M_k < \infty$, so $\sum_{k=0}^na_kz^k$ is uniformly convergent to $f(z)$ on $|z| < |z_1|$ for all $|z_1| < |z_0|$, so in particular it is uniformly convergent on $|z| < |z_0|$.
\end{proof}

From this theorem we have a few cases for $f(z)$: \begin{itemize}
    \item $f(z)$ converges for all $z \in \C$
    \item There exists $R  > 0$ such that the series converges absolutely for $|z| < R$, but diverges for $|z| > R$.
    \item The series does not converge for any $z \neq 0$ ($R = 0$)
\end{itemize}

\begin{prop}\label{prop:3.3.2}
    If $f(z) = \sum_{k=0}^{\infty}a_kz^k$ converges in $D_R = \{z \in \C:|z| < R\}$, then it converges uniformly in any disk $D_S$ with $0 < S < R$. In particular, this yields $f(z)$ is continuous on $D_R$.
\end{prop}
\begin{proof}
    Let $0 < S < R$ and pick $T$ such that $0 < S < T<R$. Then the series converges for any $|z| = T$. In particular, there exists $C > 0$ such that $|a_kT^k| < C$ for all $k \geq 0$. If $z \in D_S$, then $|a_kz^k| \leq |a_kT^k|\frac{|z|^k}{|T|^k} \leq Cr^k$, where $0 \leq r = \frac{|z|}{|T|} < 1$. As $|r| < 1$, $C\sum_{k=0}^nr^k$ converges, and so is Cauchy. Thus, for $\varepsilon > 0$, there exists $N \in \N$ such that if $m > n \geq N$, \begin{equation*}
        \sum_{k=n}^m||a_kz^k||_{D_S} \leq \sum_{k=n}^mCr^k < \varepsilon
    \end{equation*}
    so $\sum_{k=0}^na_kz^k$ is uniformly Cauchy on $D_S$. So it converges uniformly to $f(z)$ in $D_S$ for all $S < R$. So $f(z)$ is continuous in $D_S$ for all $S < R$, and in particular $f$ is continuous on $D_R$.
\end{proof}

\begin{defn}
    For $f(z) = \sum_{k=0}^{\infty}(z-z_0)^k$, let the \Emph{radius of convergence} $R$ be defined by \begin{equation*}
        \frac{1}{R} = \lim\sup_{n\rightarrow \infty}|a_n|^{1/n}
    \end{equation*}
    If the right hand side is $0$, $R = \infty$, and if the right hand side is $\infty$, $R = 0$.
\end{defn}

\begin{prop}\label{prop:3.3.3}
    The series $f(z) = \sum_{k=0}^{\infty}a_k(z-z_0)^k$ converges absolutely for $|z-z_0| < R$, and diverges when $|z-z_0| > R$. Then when $R > 0$, $f$ is a continuous function $f:D_R(z_0) \rightarrow \C$.
\end{prop}
\begin{proof}
    If $R' < R$, and $R' \neq 0$, $\frac{1}{R} < \frac{1}{R'}$. Note $\frac{1}{R} = \lim\sup_{n\rightarrow \infty}|a_n|^{1/n}$, and let $\varepsilon = \left(\frac{1}{R'} - \frac{1}{R}\right)$. Then there exists $N \in \N$ such that for $n \geq N$, $\sup_{k\geq n}|a_k|^{1/k} < \frac{1}{R}+\varepsilon$, so $\sup_{k\geq n}|a_k|^{1/k} < \frac{1}{R'}$, so $|a_n|^{1/n}R' < 1$ and $|a_n|{R'}^n < 1$ for all $n \geq N$. If $|z-z_0| < R'$, then $\frac{|z-z_0|}{R'} = r < 1$, so $|a_n(z-z_0)^n| = |a_n|{R'}^nr^n < r^n$ for all $n \geq N$. Then $\sum_{n=m}^{'infty}||a_n(z-z_0)||_{D_{R'}} \leq \sum_{n=m}^{\infty}r^n\rightarrow 0$ for all $m \geq N$. So we have absolute uniform convergence in $D_{R'}(z_0)$, for all $0 < R' < R$.

    Divergence: Let $R'' > R$, so $\frac{1}{R''} < \frac{1}{R} = \lim\sup_{n\rightarrow \infty}|a_n|^{1/n}$. Then for all $n$, $\sup_{k\geq n}|a_n|^{1/n} > \frac{1}{R''}$, and in particular there are infinitely many $k \geq n$, for all $n$, such that $|a_k|^{1/k} > \frac{1}{R''}$, so $|a_k|{R''}^k > 1$. If $|z-z_0| \geq R''$, then $|a_n(z-z_0)^n| \geq |a_n|{R''}^n > 1$ for infinitely many $n \geq 0$. Thus, $|a_n(z-z_0)^n|$ does not converge to $0$, so the series diverges.
\end{proof}

\begin{defn}
    A function defined by $\sum_{k=0}^{\infty}a_kz^k$ with radius of convergence $R > 0$ is said to be \Emph{analytic} on $D_R$.
\end{defn}

\begin{defn}
    In $\R^n$ with $x = (x_1,...,x_n)$, for any multi-index $\alpha = (\alpha_1,...,\alpha_n)$ with $\alpha_j \in \N_0$, we define \begin{equation*}
        x^{\alpha} = x_1^{\alpha_1}\cdots x_n^{\alpha_n}
    \end{equation*}
    and we define a power series in $\R^n$ by \begin{equation*}
        f(x) = \sum_{|\alpha| \geq 0}a_{\alpha}x^{\alpha}
    \end{equation*}
\end{defn}


\subsection{Products of Series}

We investigate when we can distribute multiplication of two series:

\begin{prop}\label{prop:3.3.4}
    If $A = \sum_{k=0}^{\infty}a_k$ and $B = \sum_{k=0}^{\infty}b_k$ are absolutely convergent, then \begin{equation*}
        AB = \sum_{k=0}^{\infty}c_k,\;\;c_k = \sum_{j=0}^ka_{k-j}b_j
    \end{equation*}
\end{prop}
\begin{proof}
    Let $A_k = \sum_{n=0}^ka_n$ and $B_k = \sum_{n=0}^kb_n$. Then \begin{align*}
        A_kB_k &= \sum_{n=0}^k\sum_{m=0}^ka_nb_m \\
        &= \sum_{l=0}^k\sum_{j=0}^la_jb_{l-j} + R_l = \sum_{l=0}^kc_l+R_k
    \end{align*}
    where $R_k = \sum_{(n,m) \in \sigma(k)}a_nb_m$ where $\sigma(k) := \{(n,m) \in \N_0\times \N_0:n,m\leq k,n+m > k\}$. Then \begin{align*}
        |R_k| &\leq \sum_{(n,m) \in \sigma(k)}|a_nb_m| \\
        &\leq \sum_{k\geq n\geq k/2}\sum_{m\leq k}|a_n||b_m| + \sum_{n\leq k}\sum_{k/2\leq m\leq k}|a_n||b_m| \\
        &\leq \sum_{k/2\leq n \leq k}\sum_{m=0}^{\infty}|a_n||b_m| + \sum_{k/2\leq m\leq k}\sum_{n=0}^{\infty}|b_m||a_n| \\
        &\leq A \sum_{k/2\leq m}|b_m| + B\sum_{k/2\leq n}|a_n|
    \end{align*}
    where both sums on the right go to zero as $k$ goes to infinity. So for all $\varepsilon > 0$, there exists $N \in \N$ such that $k \geq N$ implies $|R_k| < \varepsilon$. Then $$A_kB_k = \sum_{l=0}^kc_l+R_k$$ so $$\lim\limits_{k\rightarrow \infty}\sum_{l=0}^kc_l = \lim\limits_{k\rightarrow \infty}(A_kB_k-R_k) = AB+0 + AB$$ and we conclude $$AB = \sum_{l=0}^{\infty}c_l$$ as claimed.
\end{proof}

\begin{cor}
    If the series $$f(z) = \sum_{n=0}^{\infty}a_n(z-z_0)^n,\;\;g(z) = \sum_{n=0}^{\infty}b_n(z-z_0)^n$$ converge for some $|z-z_0| < R$, then $$f(z)g(z) = \sum_{n=0}^{\infty}c_n(z-z_0)^n$$ converges for $|z-z_0| < R$ with $$c_n = \sum_{j=0}^na_jb_{n-j}$$
\end{cor}

It follows that the set of analytic functions on some disk is an algebra over $\C$.

\begin{prop}\label{prop:3.3.6}
    If $a_{jk} \in \C$ and $\sum_{j,k=0}^{\infty}|a_{j,k}| < \infty$, then for each $k$, $\alpha_k = \sum_{j=0}^{\infty}a_{j,k}$ and for each $j$ $\beta_j = \sum_{k=0}^{\infty}a_{j,k}$ are absolutely convergent, where \begin{equation*}
        \lim\limits_{n\rightarrow \infty}\sum_{j=0}^n\sum_{k=0}^n|a_{j,k}| =: \sum_{j,k=0}^{\infty}|a_{j,k}|
    \end{equation*}
    Then \begin{equation*}
        \sum_{j=0}^{\infty}\beta_j = \sum_{k=0}^{\infty}\alpha_k = \sum_{j,k=0}^{\infty}a_{j,k}
    \end{equation*}
    or equivalently \begin{equation*}
        \sum_{j=0}^{\infty}\left(\sum_{k=0}^{\infty}a_{j,k}\right)= \sum_{k=0}^{\infty}\left(\sum_{j=0}^{\infty}a_{j,k}\right) = \sum_{j,k=0}^{\infty}a_{j,k}
    \end{equation*}
\end{prop}
\begin{proof}
    For all $\varepsilon > 0$ there exists $N \in \N$ such that $$\sum_{j,k\geq n}|a_{j,k}| < \varepsilon$$ Thus for all $j \in \N$ and all $k \in \N$, $$\sum_{k=0}^{\infty}|a_{j,k}| < \sum_{j,k}|a_{j,k}| < \infty$$ and $$\sum_{j=0}^{\infty}|a_{j,k}| < \sum_{j,k}|a_{j,k}| <\infty$$ Then for all $M,K \geq N$, $$\left|\sum_{k=0}^M\sum_{j=0}^Ka_{j,k} - \sum_{j,k}^Na_{j,k}\right| < \sum_{j,k \geq N}|a_{j,k}| < \varepsilon$$ In particular, taking the limit as $K$ goes to infinity, $$\left|\sum_{k=0}^M\sum_{j=0}^{\infty}a_{j,k} - \sum_{j,k}^Na_{j,k}\right| \leq \sum_{j,k \geq N}|a_{j,k}| < \varepsilon$$
    and $$\left|\sum_{k=0}^{\infty}\sum_{j=0}^{\infty}a_{j,k} - \sum_{j,k}^Na_{j,k}\right| \leq \sum_{j,k \geq N}|a_{j,k}| < \varepsilon$$
    Reversing the sums before taking the limits, we obtain the other direction, so \begin{equation*}
        \sum_{j=0}^{\infty}\left(\sum_{k=0}^{\infty}a_{j,k}\right) = \sum_{j,k=0}^{\infty}a_{j,k} = \sum_{k=0}^{\infty}\left(\sum_{j=0}^{\infty}a_{j,k}\right)
    \end{equation*}
    as desired.
\end{proof}

